{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Oiu5Fabu3ue"
   },
   "source": [
    "# Text Generation\n",
    "## Character-Level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "In this assignment, you will train a Reccurent Neural Network to generate a text one character at the time. The task is divided into steps for simpler navigation.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aC79_wIgWfGw",
    "outputId": "a3bb3375-734e-453b-f772-9e44b773f326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 14 18:04:09 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JY1C-COvVRb"
   },
   "source": [
    "## Step 0. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHfCDyzceEl"
   },
   "source": [
    "As training data, we will use \"War and Peace\" by Leo Tolstoy. The book in plain text format can be downloaded from [Project Gutenberg website ](https://www.gutenberg.org/ebooks/2600).\n",
    "\n",
    "Download and load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0NB7CbGW23l",
    "outputId": "47d7ccd6-1beb-4205-c3a6-3068ec7a6a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-14 18:04:16--  https://www.gutenberg.org/files/2600/2600-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3359408 (3.2M) [text/plain]\n",
      "Saving to: ‘2600-0.txt.4’\n",
      "\n",
      "2600-0.txt.4        100%[===================>]   3.20M  2.14MB/s    in 1.5s    \n",
      "\n",
      "2021-11-14 18:04:21 (2.14 MB/s) - ‘2600-0.txt.4’ saved [3359408/3359408]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.gutenberg.org/files/2600/2600-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CnskyXUl4K3I"
   },
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('2600-0.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp1Ljc4mceEl"
   },
   "source": [
    "Let's view the first 100 symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NUb_mCJS5W5i",
    "outputId": "7e081079-72be-4573-a278-a26c80847485"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of War and Peace, by Leo Tolstoy\\n\\nThis eBook is for the use of anyone a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4TwJP8f5eOt"
   },
   "source": [
    "The book starts at `7477` (the range was corrected due to a different version of the book):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "XH6JIIPDYOFL",
    "outputId": "91abfb06-6c53-4f62-b9f8-a840180c6b81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'CHAPTER I\\n\\n“Well, Prince, so Genoa and Lucca are now just family estates of the\\nBuonapartes. But I warn you, if you don’t tell me that this means war,\\nif you still try to defend the infamies and horrors perpetrated by that\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[7477:7700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iG2vAfmwhWN"
   },
   "source": [
    "## Step 1. Tokenization (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC21bopceEl"
   },
   "source": [
    "Now let's create dictionaries for converting characters to integers and vice versa. This makes it easier to use symbols as input on the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tYVlmnxLceEl"
   },
   "outputs": [],
   "source": [
    "# Create two dictionaries:\n",
    "# int2char -- maps integers to characters\n",
    "# char2int -- maps characters to unique integers\n",
    "\n",
    "## YOUR CODE HERE\n",
    "int2char = dict(enumerate(set(text)))   # maps integers to characters\n",
    "char2int = {value: key for key, value in int2char.items()}   # maps characters to unique integers\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJIzwzSwceEl"
   },
   "source": [
    "Let's see how the encoding of characters into integers looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJzbwKT0Zwwr",
    "outputId": "80b703d6-6446-4bde-95f8-c714fd5e0d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 74,  59,  65,  42,  31, 100,  12,  38,   3,  42,  79,  32,  31,\n",
       "         4,  56,  32,  42,  50,  27,  42,  12,   8,  31,  42, 106,  38,\n",
       "        38,  73,  31,  38,   1,  31, 110, 109,  12,  31, 109,  50,  85,\n",
       "        31, 100,  42, 109,  79,  42,  24,  31,  27,  58,  31,  69,  42,\n",
       "        38,  31,  59,  38,  18,  98,  32,  38,  58, 104, 104,  59,  65,\n",
       "         5,  98,  31,  42, 106,  38,  38,  73,  31,   5,  98,  31,   1,\n",
       "        38,  12,  31,  32,  65,  42,  31,  56,  98,  42,  31,  38,   1,\n",
       "        31, 109,  50,  58,  38,  50,  42,  31, 109])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEzrUe5Qyjz8"
   },
   "source": [
    "## Step 2. One-hot encoding (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azltQy-gceEl"
   },
   "source": [
    "LSTM expects one-hot encoded input, which means that each character is converted to an integer (via our created dictionary) and then converted to a vector, where a value 1 will be only on a corresponding position and the rest of the vector will be filled with zeros.\n",
    "\n",
    "Implement a function that does this kind of coding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OnahALhiceEl"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # arr - array of integers\n",
    "    # n_labels - number of labels (the size of a one-hot-encoded vector)\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    one_hot = np.eye(n_labels)[arr]\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3lTdLKfceEl",
    "outputId": "9d92eaba-2091-44b6-e0a4-4258d9a6ee5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works correctly\n",
    "test_indx = np.array([[7, 2, 5]])\n",
    "one_hot = one_hot_encode(test_indx, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7qAGC8R0lF1"
   },
   "source": [
    "The output must be\n",
    "\n",
    "```\n",
    "[[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
    "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
    "  [0. 0. 0. 0. 0. 1. 0. 0.]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Qxw5Dg41HAO"
   },
   "source": [
    "## Step 3. Mini-batches (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50zKx6pM43RB"
   },
   "source": [
    "To train the neural network, we will organize mini-batches as follows: divide the entire input sequence 'arr' by the desired number of subsequences (parameter `batch_size`), and send a sequence of length `seq_length` to the input of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YyL91CuceEl"
   },
   "source": [
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzTSxOr37JyR"
   },
   "source": [
    "### How to create mini-batches\n",
    "\n",
    "\n",
    "1.   Find the total number of batches `n_batches` that fit the text.\n",
    "2.   Discard all unnecessary text that does not fit into full batches.\n",
    "3.   Split text into `n_batches` batches'.\n",
    "4.   Get `x's` and `y`'s for a batch. Therefore, `y` is a version of `x` shifted by 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8vmDKLiOceEl"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    # Create a generator that returns batches of size batch_size x seq_length\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    ## Get the number of batches we can make\n",
    "    n_batches = len(arr) // batch_size_total ## YOUR CODE HERE\n",
    "    \n",
    "    ## Keep only enough characters to make full batches\n",
    "    arr = arr[: n_batches * batch_size_total] ## YOUR CODE HERE\n",
    "    \n",
    "    ## Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1)) ## YOUR CODE HERE\n",
    "    \n",
    "    ## Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:(n+seq_length)]  ## YOUR CODE HERE\n",
    "        \n",
    "        # The target is a version of x shifted by one (do not forget border conditions)\n",
    "        ## YOUR CODE HERE\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1] = x[:, 1:]\n",
    "        y[:, -1] = arr[:, (n+seq_length)] if (n+seq_length) < arr.shape[1] else arr[:, 0]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9uKOvbqceEl"
   },
   "source": [
    "Let's check how our function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qtKlLXi1ceEl"
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 4, 30)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cUu_ftpdq4W",
    "outputId": "5ef66657-5f60-45fa-be89-e622be82caf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 74  59  65  42  31 100  12  38   3  42]\n",
      " [ 12 103  98 104  65  42 109  18  32  65]\n",
      " [ 27  42  12   5  50   8  31  65   5  98]\n",
      " [109  50  85  18  42  31  16   5  32  65]]\n",
      "\n",
      "y\n",
      " [[ 59  65  42  31 100  12  38   3  42  79]\n",
      " [103  98 104  65  42 109  18  32  65  31]\n",
      " [ 42  12   5  50   8  31  65   5  98  31]\n",
      " [ 50  85  18  42  31  16   5  32  65  31]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_qHIAEIceEl"
   },
   "source": [
    "If you've implemented get_batches correctly, the result should look something like this (exact numbers may vary):\n",
    "```\n",
    "x\n",
    " [[ 48  94  12 110  32  96 107  34  18 106]\n",
    " [ 96  78   1  69  32  34  18  34  27   3]\n",
    " [  3  96   0  18  96  86  18 100  34  96]\n",
    " [ 32  96 110  42 101  96   0  18  19  96]]\n",
    "\n",
    "y\n",
    " [[ 94  12 110  32  96 107  34  18 106  32]\n",
    " [ 78   1  69  32  34  18  34  27   3  94]\n",
    " [ 96   0  18  96  86  18 100  34  96   1]\n",
    " [ 96 110  42 101  96   0  18  19  96   3]]\n",
    " ```\n",
    "Make sure the data is shifted one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tholjONVAJUa"
   },
   "source": [
    "## Step 4. Defining the network (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7s5eRaoceEl"
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "The proposed architecture is as follows:\n",
    "\n",
    "* Define an [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) with `dropout=drop_prob` and `batch_first=True` (since we use batches)\n",
    "* Define a [Dropout layer](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout) with `drop_prob`.\n",
    "* Define a [Linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) with `in_features=n_hidden` and `out_features` equals to number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlTnDntHceEl",
    "outputId": "b5d0ee27-a966-44ba-ed38-aeaefdfefba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lg-SvaGhceEl"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=self.n_hidden, \n",
    "                            num_layers=self.n_layers, batch_first=True, dropout=self.drop_prob)\n",
    "\n",
    "        # Define a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "\n",
    "        # Define the final, fully-connected output layer\n",
    "        ## YOUR CODE HERE\n",
    "        self.linear = nn.Linear(in_features=self.n_hidden, out_features=len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        # Get the outputs and the new hidden state from the lstm\n",
    "        ## YOUR CODE HERE\n",
    "        out, hidden = self.lstm(x.float(), hidden)\n",
    "\n",
    "        # Pass through a dropout layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        ## YOUR CODE HERE\n",
    "        out = out.reshape(out.size()[0] * out.size()[1], self.n_hidden)\n",
    "\n",
    "        # Put x through the fully-connected layer\n",
    "        ## YOUR CODE HERE\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPX7bf08EipB"
   },
   "source": [
    "## Step 5. Train the model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBgcPj1mGOZN"
   },
   "source": [
    "We use Cross Entropy as a loss function, Adam as optimizer, and [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to get rid of the gradient explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lv8VkRI0ceEl"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJmlzeyfmNZq"
   },
   "source": [
    "### Model_1: Two LSTM-layers\n",
    "n_hidden = 256  \n",
    "n_layers = 2  \n",
    "drop_prob=0.5  \n",
    "batch_size = 64  \n",
    "seq_length = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gt0q4KGEceEm"
   },
   "source": [
    "Initialize the model and set hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OOgs59nDceEm",
    "outputId": "cfc52771-3406-4a6c-a4fe-27cb67e9bc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(112, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=112, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set your model hyperparameters\n",
    "\n",
    "## YOUR CODE HERE\n",
    "n_hidden = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = CharRNN(tuple(set(text)), n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHy6mECuceEm"
   },
   "source": [
    "Set training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABqi9klKceEm",
    "outputId": "c2f5ad4a-8627-4815-d589-b32a85a34656",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.3538... Val Loss: 3.1813\n",
      "Epoch: 1/20... Step: 20... Loss: 3.2587... Val Loss: 3.1136\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1573... Val Loss: 3.1075\n",
      "Epoch: 1/20... Step: 40... Loss: 3.2028... Val Loss: 3.1137\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1946... Val Loss: 3.1056\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1751... Val Loss: 3.1033\n",
      "Epoch: 1/20... Step: 70... Loss: 3.2426... Val Loss: 3.1071\n",
      "Epoch: 1/20... Step: 80... Loss: 3.2324... Val Loss: 3.1032\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1475... Val Loss: 3.1008\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1589... Val Loss: 3.1007\n",
      "Epoch: 1/20... Step: 110... Loss: 3.2132... Val Loss: 3.1000\n",
      "Epoch: 1/20... Step: 120... Loss: 3.1791... Val Loss: 3.0991\n",
      "Epoch: 1/20... Step: 130... Loss: 3.2001... Val Loss: 3.0992\n",
      "Epoch: 1/20... Step: 140... Loss: 3.1391... Val Loss: 3.0955\n",
      "Epoch: 1/20... Step: 150... Loss: 3.1542... Val Loss: 3.0933\n",
      "Epoch: 1/20... Step: 160... Loss: 3.1142... Val Loss: 3.0891\n",
      "Epoch: 1/20... Step: 170... Loss: 3.0824... Val Loss: 3.0839\n",
      "Epoch: 1/20... Step: 180... Loss: 3.1291... Val Loss: 3.0735\n",
      "Epoch: 1/20... Step: 190... Loss: 3.1487... Val Loss: 3.0619\n",
      "Epoch: 1/20... Step: 200... Loss: 3.1344... Val Loss: 3.0379\n",
      "Epoch: 1/20... Step: 210... Loss: 3.0547... Val Loss: 3.0041\n",
      "Epoch: 1/20... Step: 220... Loss: 3.0090... Val Loss: 2.9606\n",
      "Epoch: 1/20... Step: 230... Loss: 3.0222... Val Loss: 2.9141\n",
      "Epoch: 1/20... Step: 240... Loss: 2.9330... Val Loss: 2.8583\n",
      "Epoch: 1/20... Step: 250... Loss: 2.9166... Val Loss: 2.7826\n",
      "Epoch: 1/20... Step: 260... Loss: 2.8224... Val Loss: 2.7301\n",
      "Epoch: 1/20... Step: 270... Loss: 2.7454... Val Loss: 2.6859\n",
      "Epoch: 1/20... Step: 280... Loss: 2.6976... Val Loss: 2.6827\n",
      "Epoch: 1/20... Step: 290... Loss: 2.6720... Val Loss: 2.6269\n",
      "Epoch: 1/20... Step: 300... Loss: 2.6618... Val Loss: 2.6015\n",
      "Epoch: 1/20... Step: 310... Loss: 2.6204... Val Loss: 2.5780\n",
      "Epoch: 1/20... Step: 320... Loss: 2.6489... Val Loss: 2.5635\n",
      "Epoch: 1/20... Step: 330... Loss: 2.6117... Val Loss: 2.5408\n",
      "Epoch: 1/20... Step: 340... Loss: 2.6056... Val Loss: 2.5272\n",
      "Epoch: 1/20... Step: 350... Loss: 2.6225... Val Loss: 2.5255\n",
      "Epoch: 1/20... Step: 360... Loss: 2.6204... Val Loss: 2.5028\n",
      "Epoch: 1/20... Step: 370... Loss: 2.5550... Val Loss: 2.4881\n",
      "Epoch: 1/20... Step: 380... Loss: 2.5463... Val Loss: 2.4804\n",
      "Epoch: 1/20... Step: 390... Loss: 2.5504... Val Loss: 2.4618\n",
      "Epoch: 1/20... Step: 400... Loss: 2.5323... Val Loss: 2.4524\n",
      "Epoch: 1/20... Step: 410... Loss: 2.4884... Val Loss: 2.4423\n",
      "Epoch: 1/20... Step: 420... Loss: 2.5255... Val Loss: 2.4339\n",
      "Epoch: 1/20... Step: 430... Loss: 2.5679... Val Loss: 2.4245\n",
      "Epoch: 1/20... Step: 440... Loss: 2.4772... Val Loss: 2.4160\n",
      "Epoch: 1/20... Step: 450... Loss: 2.4857... Val Loss: 2.4047\n",
      "Epoch: 1/20... Step: 460... Loss: 2.4985... Val Loss: 2.4142\n",
      "Epoch: 1/20... Step: 470... Loss: 2.4697... Val Loss: 2.3949\n",
      "Epoch: 1/20... Step: 480... Loss: 2.5508... Val Loss: 2.3814\n",
      "Epoch: 1/20... Step: 490... Loss: 2.4240... Val Loss: 2.3835\n",
      "Epoch: 1/20... Step: 500... Loss: 2.4206... Val Loss: 2.3666\n",
      "Epoch: 1/20... Step: 510... Loss: 2.4438... Val Loss: 2.3619\n",
      "Epoch: 1/20... Step: 520... Loss: 2.4244... Val Loss: 2.3533\n",
      "Epoch: 1/20... Step: 530... Loss: 2.4348... Val Loss: 2.3477\n",
      "Epoch: 1/20... Step: 540... Loss: 2.4310... Val Loss: 2.3386\n",
      "Epoch: 1/20... Step: 550... Loss: 2.4236... Val Loss: 2.3338\n",
      "Epoch: 1/20... Step: 560... Loss: 2.3991... Val Loss: 2.3243\n",
      "Epoch: 1/20... Step: 570... Loss: 2.4001... Val Loss: 2.3404\n",
      "Epoch: 1/20... Step: 580... Loss: 2.3936... Val Loss: 2.3258\n",
      "Epoch: 1/20... Step: 590... Loss: 2.3584... Val Loss: 2.3158\n",
      "Epoch: 1/20... Step: 600... Loss: 2.3860... Val Loss: 2.3048\n",
      "Epoch: 1/20... Step: 610... Loss: 2.3744... Val Loss: 2.2964\n",
      "Epoch: 1/20... Step: 620... Loss: 2.4208... Val Loss: 2.2900\n",
      "Epoch: 1/20... Step: 630... Loss: 2.2808... Val Loss: 2.2811\n",
      "Epoch: 1/20... Step: 640... Loss: 2.3946... Val Loss: 2.2816\n",
      "Epoch: 1/20... Step: 650... Loss: 2.3596... Val Loss: 2.2728\n",
      "Epoch: 1/20... Step: 660... Loss: 2.3025... Val Loss: 2.2671\n",
      "Epoch: 1/20... Step: 670... Loss: 2.3222... Val Loss: 2.2637\n",
      "Epoch: 1/20... Step: 680... Loss: 2.2656... Val Loss: 2.2524\n",
      "Epoch: 1/20... Step: 690... Loss: 2.3285... Val Loss: 2.2512\n",
      "Epoch: 1/20... Step: 700... Loss: 2.2974... Val Loss: 2.2473\n",
      "Epoch: 1/20... Step: 710... Loss: 2.3204... Val Loss: 2.2338\n",
      "Epoch: 1/20... Step: 720... Loss: 2.3160... Val Loss: 2.2352\n",
      "Epoch: 1/20... Step: 730... Loss: 2.3437... Val Loss: 2.2330\n",
      "Epoch: 1/20... Step: 740... Loss: 2.3418... Val Loss: 2.2219\n",
      "Epoch: 1/20... Step: 750... Loss: 2.3595... Val Loss: 2.2227\n",
      "Epoch: 1/20... Step: 760... Loss: 2.2721... Val Loss: 2.2148\n",
      "Epoch: 1/20... Step: 770... Loss: 2.3854... Val Loss: 2.2165\n",
      "Epoch: 1/20... Step: 780... Loss: 2.2083... Val Loss: 2.2020\n",
      "Epoch: 1/20... Step: 790... Loss: 2.3745... Val Loss: 2.2027\n",
      "Epoch: 1/20... Step: 800... Loss: 2.3397... Val Loss: 2.1978\n",
      "Epoch: 1/20... Step: 810... Loss: 2.2766... Val Loss: 2.1917\n",
      "Epoch: 1/20... Step: 820... Loss: 2.2383... Val Loss: 2.1858\n",
      "Epoch: 1/20... Step: 830... Loss: 2.2648... Val Loss: 2.1839\n",
      "Epoch: 1/20... Step: 840... Loss: 2.2835... Val Loss: 2.1780\n",
      "Epoch: 1/20... Step: 850... Loss: 2.2185... Val Loss: 2.1772\n",
      "Epoch: 1/20... Step: 860... Loss: 2.2146... Val Loss: 2.1643\n",
      "Epoch: 1/20... Step: 870... Loss: 2.2202... Val Loss: 2.1602\n",
      "Epoch: 1/20... Step: 880... Loss: 2.2071... Val Loss: 2.1561\n",
      "Epoch: 1/20... Step: 890... Loss: 2.2538... Val Loss: 2.1565\n",
      "Epoch: 1/20... Step: 900... Loss: 2.2172... Val Loss: 2.1523\n",
      "Epoch: 1/20... Step: 910... Loss: 2.1948... Val Loss: 2.1476\n",
      "Epoch: 1/20... Step: 920... Loss: 2.1820... Val Loss: 2.1435\n",
      "Epoch: 1/20... Step: 930... Loss: 2.2008... Val Loss: 2.1371\n",
      "Epoch: 1/20... Step: 940... Loss: 2.2638... Val Loss: 2.1356\n",
      "Epoch: 1/20... Step: 950... Loss: 2.2349... Val Loss: 2.1338\n",
      "Epoch: 1/20... Step: 960... Loss: 2.2236... Val Loss: 2.1467\n",
      "Epoch: 1/20... Step: 970... Loss: 2.2019... Val Loss: 2.1260\n",
      "Epoch: 1/20... Step: 980... Loss: 2.1571... Val Loss: 2.1197\n",
      "Epoch: 1/20... Step: 990... Loss: 2.1609... Val Loss: 2.1164\n",
      "Epoch: 1/20... Step: 1000... Loss: 2.2060... Val Loss: 2.1149\n",
      "Epoch: 1/20... Step: 1010... Loss: 2.1703... Val Loss: 2.1067\n",
      "Epoch: 1/20... Step: 1020... Loss: 2.1844... Val Loss: 2.1036\n",
      "Epoch: 1/20... Step: 1030... Loss: 2.1717... Val Loss: 2.1003\n",
      "Epoch: 1/20... Step: 1040... Loss: 2.1773... Val Loss: 2.0966\n",
      "Epoch: 1/20... Step: 1050... Loss: 2.1872... Val Loss: 2.0956\n",
      "Epoch: 1/20... Step: 1060... Loss: 2.2017... Val Loss: 2.0924\n",
      "Epoch: 1/20... Step: 1070... Loss: 2.1445... Val Loss: 2.0870\n",
      "Epoch: 1/20... Step: 1080... Loss: 2.1797... Val Loss: 2.0806\n",
      "Epoch: 1/20... Step: 1090... Loss: 2.1765... Val Loss: 2.0814\n",
      "Epoch: 1/20... Step: 1100... Loss: 2.0643... Val Loss: 2.0761\n",
      "Epoch: 1/20... Step: 1110... Loss: 2.1386... Val Loss: 2.0754\n",
      "Epoch: 1/20... Step: 1120... Loss: 2.1656... Val Loss: 2.0728\n",
      "Epoch: 1/20... Step: 1130... Loss: 2.1219... Val Loss: 2.0657\n",
      "Epoch: 1/20... Step: 1140... Loss: 2.0873... Val Loss: 2.0657\n",
      "Epoch: 1/20... Step: 1150... Loss: 2.2247... Val Loss: 2.0623\n",
      "Epoch: 1/20... Step: 1160... Loss: 2.1446... Val Loss: 2.0557\n",
      "Epoch: 1/20... Step: 1170... Loss: 2.0667... Val Loss: 2.0560\n",
      "Epoch: 1/20... Step: 1180... Loss: 2.0840... Val Loss: 2.0495\n",
      "Epoch: 1/20... Step: 1190... Loss: 2.1291... Val Loss: 2.0490\n",
      "Epoch: 1/20... Step: 1200... Loss: 2.1146... Val Loss: 2.0502\n",
      "Epoch: 1/20... Step: 1210... Loss: 2.0835... Val Loss: 2.0388\n",
      "Epoch: 1/20... Step: 1220... Loss: 2.1386... Val Loss: 2.0355\n",
      "Epoch: 1/20... Step: 1230... Loss: 2.0767... Val Loss: 2.0353\n",
      "Epoch: 1/20... Step: 1240... Loss: 2.1250... Val Loss: 2.0289\n",
      "Epoch: 1/20... Step: 1250... Loss: 2.1714... Val Loss: 2.0279\n",
      "Epoch: 1/20... Step: 1260... Loss: 2.1228... Val Loss: 2.0250\n",
      "Epoch: 1/20... Step: 1270... Loss: 2.1371... Val Loss: 2.0208\n",
      "Epoch: 1/20... Step: 1280... Loss: 2.1766... Val Loss: 2.0204\n",
      "Epoch: 1/20... Step: 1290... Loss: 2.1241... Val Loss: 2.0174\n",
      "Epoch: 1/20... Step: 1300... Loss: 2.1219... Val Loss: 2.0151\n",
      "Epoch: 1/20... Step: 1310... Loss: 2.1007... Val Loss: 2.0115\n",
      "Epoch: 1/20... Step: 1320... Loss: 2.1022... Val Loss: 2.0143\n",
      "Epoch: 1/20... Step: 1330... Loss: 2.0569... Val Loss: 2.0115\n",
      "Epoch: 1/20... Step: 1340... Loss: 2.1121... Val Loss: 2.0042\n",
      "Epoch: 1/20... Step: 1350... Loss: 2.0710... Val Loss: 2.0027\n",
      "Epoch: 1/20... Step: 1360... Loss: 2.1175... Val Loss: 1.9995\n",
      "Epoch: 1/20... Step: 1370... Loss: 2.1138... Val Loss: 1.9952\n",
      "Epoch: 1/20... Step: 1380... Loss: 2.0626... Val Loss: 1.9940\n",
      "Epoch: 1/20... Step: 1390... Loss: 2.0217... Val Loss: 1.9896\n",
      "Epoch: 1/20... Step: 1400... Loss: 2.0919... Val Loss: 1.9900\n",
      "Epoch: 1/20... Step: 1410... Loss: 2.0112... Val Loss: 1.9875\n",
      "Epoch: 2/20... Step: 1420... Loss: 2.0802... Val Loss: 1.9797\n",
      "Epoch: 2/20... Step: 1430... Loss: 2.1446... Val Loss: 1.9782\n",
      "Epoch: 2/20... Step: 1440... Loss: 2.0766... Val Loss: 1.9746\n",
      "Epoch: 2/20... Step: 1450... Loss: 2.0129... Val Loss: 1.9798\n",
      "Epoch: 2/20... Step: 1460... Loss: 1.9930... Val Loss: 1.9721\n",
      "Epoch: 2/20... Step: 1470... Loss: 2.0005... Val Loss: 1.9650\n",
      "Epoch: 2/20... Step: 1480... Loss: 2.0863... Val Loss: 1.9666\n",
      "Epoch: 2/20... Step: 1490... Loss: 1.9989... Val Loss: 1.9652\n",
      "Epoch: 2/20... Step: 1500... Loss: 2.1055... Val Loss: 1.9610\n",
      "Epoch: 2/20... Step: 1510... Loss: 2.0075... Val Loss: 1.9545\n",
      "Epoch: 2/20... Step: 1520... Loss: 2.0616... Val Loss: 1.9525\n",
      "Epoch: 2/20... Step: 1530... Loss: 1.9736... Val Loss: 1.9532\n",
      "Epoch: 2/20... Step: 1540... Loss: 2.0368... Val Loss: 1.9465\n",
      "Epoch: 2/20... Step: 1550... Loss: 1.9759... Val Loss: 1.9485\n",
      "Epoch: 2/20... Step: 1560... Loss: 2.0274... Val Loss: 1.9436\n",
      "Epoch: 2/20... Step: 1570... Loss: 2.0299... Val Loss: 1.9413\n",
      "Epoch: 2/20... Step: 1580... Loss: 2.0208... Val Loss: 1.9413\n",
      "Epoch: 2/20... Step: 1590... Loss: 2.0332... Val Loss: 1.9407\n",
      "Epoch: 2/20... Step: 1600... Loss: 2.0153... Val Loss: 1.9357\n",
      "Epoch: 2/20... Step: 1610... Loss: 1.9727... Val Loss: 1.9366\n",
      "Epoch: 2/20... Step: 1620... Loss: 2.0157... Val Loss: 1.9337\n",
      "Epoch: 2/20... Step: 1630... Loss: 2.0185... Val Loss: 1.9306\n",
      "Epoch: 2/20... Step: 1640... Loss: 1.9728... Val Loss: 1.9247\n",
      "Epoch: 2/20... Step: 1650... Loss: 1.9592... Val Loss: 1.9251\n",
      "Epoch: 2/20... Step: 1660... Loss: 2.0177... Val Loss: 1.9247\n",
      "Epoch: 2/20... Step: 1670... Loss: 2.0130... Val Loss: 1.9178\n",
      "Epoch: 2/20... Step: 1680... Loss: 1.9594... Val Loss: 1.9152\n",
      "Epoch: 2/20... Step: 1690... Loss: 1.9986... Val Loss: 1.9130\n",
      "Epoch: 2/20... Step: 1700... Loss: 1.9479... Val Loss: 1.9121\n",
      "Epoch: 2/20... Step: 1710... Loss: 1.9740... Val Loss: 1.9131\n",
      "Epoch: 2/20... Step: 1720... Loss: 1.9108... Val Loss: 1.9106\n",
      "Epoch: 2/20... Step: 1730... Loss: 1.9966... Val Loss: 1.9042\n",
      "Epoch: 2/20... Step: 1740... Loss: 1.9640... Val Loss: 1.9015\n",
      "Epoch: 2/20... Step: 1750... Loss: 2.0542... Val Loss: 1.9003\n",
      "Epoch: 2/20... Step: 1760... Loss: 2.0828... Val Loss: 1.8992\n",
      "Epoch: 2/20... Step: 1770... Loss: 2.0272... Val Loss: 1.8943\n",
      "Epoch: 2/20... Step: 1780... Loss: 2.0138... Val Loss: 1.8939\n",
      "Epoch: 2/20... Step: 1790... Loss: 2.0114... Val Loss: 1.8926\n",
      "Epoch: 2/20... Step: 1800... Loss: 2.0243... Val Loss: 1.8955\n",
      "Epoch: 2/20... Step: 1810... Loss: 1.9229... Val Loss: 1.8885\n",
      "Epoch: 2/20... Step: 1820... Loss: 1.8835... Val Loss: 1.8894\n",
      "Epoch: 2/20... Step: 1830... Loss: 1.9506... Val Loss: 1.8867\n",
      "Epoch: 2/20... Step: 1840... Loss: 2.0128... Val Loss: 1.8832\n",
      "Epoch: 2/20... Step: 1850... Loss: 2.0049... Val Loss: 1.8781\n",
      "Epoch: 2/20... Step: 1860... Loss: 1.9795... Val Loss: 1.8870\n",
      "Epoch: 2/20... Step: 1870... Loss: 1.9787... Val Loss: 1.8846\n",
      "Epoch: 2/20... Step: 1880... Loss: 1.9244... Val Loss: 1.8773\n",
      "Epoch: 2/20... Step: 1890... Loss: 1.9202... Val Loss: 1.8718\n",
      "Epoch: 2/20... Step: 1900... Loss: 1.9352... Val Loss: 1.8705\n",
      "Epoch: 2/20... Step: 1910... Loss: 1.9560... Val Loss: 1.8643\n",
      "Epoch: 2/20... Step: 1920... Loss: 1.9354... Val Loss: 1.8636\n",
      "Epoch: 2/20... Step: 1930... Loss: 1.9484... Val Loss: 1.8622\n",
      "Epoch: 2/20... Step: 1940... Loss: 1.9766... Val Loss: 1.8640\n",
      "Epoch: 2/20... Step: 1950... Loss: 1.9691... Val Loss: 1.8675\n",
      "Epoch: 2/20... Step: 1960... Loss: 2.0233... Val Loss: 1.8603\n",
      "Epoch: 2/20... Step: 1970... Loss: 1.9515... Val Loss: 1.8519\n",
      "Epoch: 2/20... Step: 1980... Loss: 1.9016... Val Loss: 1.8528\n",
      "Epoch: 2/20... Step: 1990... Loss: 1.9315... Val Loss: 1.8511\n",
      "Epoch: 2/20... Step: 2000... Loss: 2.0080... Val Loss: 1.8473\n",
      "Epoch: 2/20... Step: 2010... Loss: 1.8810... Val Loss: 1.8478\n",
      "Epoch: 2/20... Step: 2020... Loss: 1.8947... Val Loss: 1.8463\n",
      "Epoch: 2/20... Step: 2030... Loss: 1.9274... Val Loss: 1.8420\n",
      "Epoch: 2/20... Step: 2040... Loss: 1.8057... Val Loss: 1.8364\n",
      "Epoch: 2/20... Step: 2050... Loss: 1.8975... Val Loss: 1.8340\n",
      "Epoch: 2/20... Step: 2060... Loss: 1.8663... Val Loss: 1.8357\n",
      "Epoch: 2/20... Step: 2070... Loss: 1.9227... Val Loss: 1.8354\n",
      "Epoch: 2/20... Step: 2080... Loss: 1.9497... Val Loss: 1.8398\n",
      "Epoch: 2/20... Step: 2090... Loss: 1.8657... Val Loss: 1.8343\n",
      "Epoch: 2/20... Step: 2100... Loss: 1.7873... Val Loss: 1.8297\n",
      "Epoch: 2/20... Step: 2110... Loss: 1.8383... Val Loss: 1.8267\n",
      "Epoch: 2/20... Step: 2120... Loss: 1.9001... Val Loss: 1.8247\n",
      "Epoch: 2/20... Step: 2130... Loss: 1.8506... Val Loss: 1.8204\n",
      "Epoch: 2/20... Step: 2140... Loss: 2.0012... Val Loss: 1.8184\n",
      "Epoch: 2/20... Step: 2150... Loss: 1.8286... Val Loss: 1.8179\n",
      "Epoch: 2/20... Step: 2160... Loss: 1.9587... Val Loss: 1.8135\n",
      "Epoch: 2/20... Step: 2170... Loss: 1.9227... Val Loss: 1.8146\n",
      "Epoch: 2/20... Step: 2180... Loss: 1.9369... Val Loss: 1.8120\n",
      "Epoch: 2/20... Step: 2190... Loss: 1.9084... Val Loss: 1.8148\n",
      "Epoch: 2/20... Step: 2200... Loss: 1.8995... Val Loss: 1.8139\n",
      "Epoch: 2/20... Step: 2210... Loss: 1.9080... Val Loss: 1.8080\n",
      "Epoch: 2/20... Step: 2220... Loss: 1.9907... Val Loss: 1.8029\n",
      "Epoch: 2/20... Step: 2230... Loss: 1.8528... Val Loss: 1.8035\n",
      "Epoch: 2/20... Step: 2240... Loss: 1.8564... Val Loss: 1.8027\n",
      "Epoch: 2/20... Step: 2250... Loss: 1.8401... Val Loss: 1.8008\n",
      "Epoch: 2/20... Step: 2260... Loss: 1.9100... Val Loss: 1.7965\n",
      "Epoch: 2/20... Step: 2270... Loss: 1.8103... Val Loss: 1.7971\n",
      "Epoch: 2/20... Step: 2280... Loss: 1.8637... Val Loss: 1.7962\n",
      "Epoch: 2/20... Step: 2290... Loss: 1.8294... Val Loss: 1.7919\n",
      "Epoch: 2/20... Step: 2300... Loss: 1.8841... Val Loss: 1.7911\n",
      "Epoch: 2/20... Step: 2310... Loss: 1.8922... Val Loss: 1.7906\n",
      "Epoch: 2/20... Step: 2320... Loss: 1.9061... Val Loss: 1.7890\n",
      "Epoch: 2/20... Step: 2330... Loss: 1.8964... Val Loss: 1.7906\n",
      "Epoch: 2/20... Step: 2340... Loss: 1.8219... Val Loss: 1.7902\n",
      "Epoch: 2/20... Step: 2350... Loss: 1.7835... Val Loss: 1.7863\n",
      "Epoch: 2/20... Step: 2360... Loss: 1.8489... Val Loss: 1.7816\n",
      "Epoch: 2/20... Step: 2370... Loss: 1.8307... Val Loss: 1.7797\n",
      "Epoch: 2/20... Step: 2380... Loss: 1.8569... Val Loss: 1.7782\n",
      "Epoch: 2/20... Step: 2390... Loss: 1.8522... Val Loss: 1.7805\n",
      "Epoch: 2/20... Step: 2400... Loss: 1.9488... Val Loss: 1.7759\n",
      "Epoch: 2/20... Step: 2410... Loss: 1.8221... Val Loss: 1.7765\n",
      "Epoch: 2/20... Step: 2420... Loss: 1.8205... Val Loss: 1.7745\n",
      "Epoch: 2/20... Step: 2430... Loss: 1.8257... Val Loss: 1.7723\n",
      "Epoch: 2/20... Step: 2440... Loss: 1.8102... Val Loss: 1.7705\n",
      "Epoch: 2/20... Step: 2450... Loss: 1.7512... Val Loss: 1.7680\n",
      "Epoch: 2/20... Step: 2460... Loss: 1.8670... Val Loss: 1.7642\n",
      "Epoch: 2/20... Step: 2470... Loss: 1.8320... Val Loss: 1.7636\n",
      "Epoch: 2/20... Step: 2480... Loss: 1.8354... Val Loss: 1.7648\n",
      "Epoch: 2/20... Step: 2490... Loss: 1.8620... Val Loss: 1.7583\n",
      "Epoch: 2/20... Step: 2500... Loss: 1.8185... Val Loss: 1.7575\n",
      "Epoch: 2/20... Step: 2510... Loss: 1.7978... Val Loss: 1.7541\n",
      "Epoch: 2/20... Step: 2520... Loss: 1.8575... Val Loss: 1.7566\n",
      "Epoch: 2/20... Step: 2530... Loss: 1.8865... Val Loss: 1.7539\n",
      "Epoch: 2/20... Step: 2540... Loss: 1.7907... Val Loss: 1.7554\n",
      "Epoch: 2/20... Step: 2550... Loss: 1.8774... Val Loss: 1.7471\n",
      "Epoch: 2/20... Step: 2560... Loss: 1.8184... Val Loss: 1.7471\n",
      "Epoch: 2/20... Step: 2570... Loss: 1.8289... Val Loss: 1.7491\n",
      "Epoch: 2/20... Step: 2580... Loss: 1.8571... Val Loss: 1.7456\n",
      "Epoch: 2/20... Step: 2590... Loss: 1.7658... Val Loss: 1.7425\n",
      "Epoch: 2/20... Step: 2600... Loss: 1.8171... Val Loss: 1.7417\n",
      "Epoch: 2/20... Step: 2610... Loss: 1.7795... Val Loss: 1.7406\n",
      "Epoch: 2/20... Step: 2620... Loss: 1.7687... Val Loss: 1.7361\n",
      "Epoch: 2/20... Step: 2630... Loss: 1.7654... Val Loss: 1.7347\n",
      "Epoch: 2/20... Step: 2640... Loss: 1.8684... Val Loss: 1.7341\n",
      "Epoch: 2/20... Step: 2650... Loss: 1.7644... Val Loss: 1.7330\n",
      "Epoch: 2/20... Step: 2660... Loss: 1.8042... Val Loss: 1.7328\n",
      "Epoch: 2/20... Step: 2670... Loss: 1.7740... Val Loss: 1.7297\n",
      "Epoch: 2/20... Step: 2680... Loss: 1.7750... Val Loss: 1.7258\n",
      "Epoch: 2/20... Step: 2690... Loss: 1.7755... Val Loss: 1.7313\n",
      "Epoch: 2/20... Step: 2700... Loss: 1.7704... Val Loss: 1.7271\n",
      "Epoch: 2/20... Step: 2710... Loss: 1.7826... Val Loss: 1.7329\n",
      "Epoch: 2/20... Step: 2720... Loss: 1.8556... Val Loss: 1.7264\n",
      "Epoch: 2/20... Step: 2730... Loss: 1.9168... Val Loss: 1.7277\n",
      "Epoch: 2/20... Step: 2740... Loss: 1.7526... Val Loss: 1.7265\n",
      "Epoch: 2/20... Step: 2750... Loss: 1.7693... Val Loss: 1.7241\n",
      "Epoch: 2/20... Step: 2760... Loss: 1.7972... Val Loss: 1.7209\n",
      "Epoch: 2/20... Step: 2770... Loss: 1.9223... Val Loss: 1.7195\n",
      "Epoch: 2/20... Step: 2780... Loss: 1.7914... Val Loss: 1.7146\n",
      "Epoch: 2/20... Step: 2790... Loss: 1.7885... Val Loss: 1.7184\n",
      "Epoch: 2/20... Step: 2800... Loss: 1.8077... Val Loss: 1.7207\n",
      "Epoch: 2/20... Step: 2810... Loss: 1.8048... Val Loss: 1.7160\n",
      "Epoch: 2/20... Step: 2820... Loss: 1.8502... Val Loss: 1.7076\n",
      "Epoch: 2/20... Step: 2830... Loss: 1.7869... Val Loss: 1.7124\n",
      "Epoch: 3/20... Step: 2840... Loss: 1.8221... Val Loss: 1.7085\n",
      "Epoch: 3/20... Step: 2850... Loss: 1.7665... Val Loss: 1.7038\n",
      "Epoch: 3/20... Step: 2860... Loss: 1.7245... Val Loss: 1.6994\n",
      "Epoch: 3/20... Step: 2870... Loss: 1.7537... Val Loss: 1.7052\n",
      "Epoch: 3/20... Step: 2880... Loss: 1.8276... Val Loss: 1.6983\n",
      "Epoch: 3/20... Step: 2890... Loss: 1.7602... Val Loss: 1.6975\n",
      "Epoch: 3/20... Step: 2900... Loss: 1.7187... Val Loss: 1.6997\n",
      "Epoch: 3/20... Step: 2910... Loss: 1.7174... Val Loss: 1.6942\n",
      "Epoch: 3/20... Step: 2920... Loss: 1.7840... Val Loss: 1.6944\n",
      "Epoch: 3/20... Step: 2930... Loss: 1.7298... Val Loss: 1.6955\n",
      "Epoch: 3/20... Step: 2940... Loss: 1.7090... Val Loss: 1.6970\n",
      "Epoch: 3/20... Step: 2950... Loss: 1.7867... Val Loss: 1.6917\n",
      "Epoch: 3/20... Step: 2960... Loss: 1.7927... Val Loss: 1.6895\n",
      "Epoch: 3/20... Step: 2970... Loss: 1.7681... Val Loss: 1.6889\n",
      "Epoch: 3/20... Step: 2980... Loss: 1.7453... Val Loss: 1.6913\n",
      "Epoch: 3/20... Step: 2990... Loss: 1.7194... Val Loss: 1.6857\n",
      "Epoch: 3/20... Step: 3000... Loss: 1.8126... Val Loss: 1.6893\n",
      "Epoch: 3/20... Step: 3010... Loss: 1.7065... Val Loss: 1.6872\n",
      "Epoch: 3/20... Step: 3020... Loss: 1.7202... Val Loss: 1.6893\n",
      "Epoch: 3/20... Step: 3030... Loss: 1.7212... Val Loss: 1.6875\n",
      "Epoch: 3/20... Step: 3040... Loss: 1.7128... Val Loss: 1.6846\n",
      "Epoch: 3/20... Step: 3050... Loss: 1.7857... Val Loss: 1.6845\n",
      "Epoch: 3/20... Step: 3060... Loss: 1.7486... Val Loss: 1.6814\n",
      "Epoch: 3/20... Step: 3070... Loss: 1.7441... Val Loss: 1.6820\n",
      "Epoch: 3/20... Step: 3080... Loss: 1.6866... Val Loss: 1.6790\n",
      "Epoch: 3/20... Step: 3090... Loss: 1.7564... Val Loss: 1.6786\n",
      "Epoch: 3/20... Step: 3100... Loss: 1.7764... Val Loss: 1.6800\n",
      "Epoch: 3/20... Step: 3110... Loss: 1.6424... Val Loss: 1.6739\n",
      "Epoch: 3/20... Step: 3120... Loss: 1.7243... Val Loss: 1.6718\n",
      "Epoch: 3/20... Step: 3130... Loss: 1.7456... Val Loss: 1.6730\n",
      "Epoch: 3/20... Step: 3140... Loss: 1.7286... Val Loss: 1.6709\n",
      "Epoch: 3/20... Step: 3150... Loss: 1.8264... Val Loss: 1.6666\n",
      "Epoch: 3/20... Step: 3160... Loss: 1.7525... Val Loss: 1.6696\n",
      "Epoch: 3/20... Step: 3170... Loss: 1.7636... Val Loss: 1.6673\n",
      "Epoch: 3/20... Step: 3180... Loss: 1.7922... Val Loss: 1.6631\n",
      "Epoch: 3/20... Step: 3190... Loss: 1.7609... Val Loss: 1.6634\n",
      "Epoch: 3/20... Step: 3200... Loss: 1.7271... Val Loss: 1.6649\n",
      "Epoch: 3/20... Step: 3210... Loss: 1.7865... Val Loss: 1.6591\n",
      "Epoch: 3/20... Step: 3220... Loss: 1.7490... Val Loss: 1.6622\n",
      "Epoch: 3/20... Step: 3230... Loss: 1.7420... Val Loss: 1.6597\n",
      "Epoch: 3/20... Step: 3240... Loss: 1.6892... Val Loss: 1.6586\n",
      "Epoch: 3/20... Step: 3250... Loss: 1.7243... Val Loss: 1.6599\n",
      "Epoch: 3/20... Step: 3260... Loss: 1.6573... Val Loss: 1.6576\n",
      "Epoch: 3/20... Step: 3270... Loss: 1.7971... Val Loss: 1.6550\n",
      "Epoch: 3/20... Step: 3280... Loss: 1.6983... Val Loss: 1.6597\n",
      "Epoch: 3/20... Step: 3290... Loss: 1.6816... Val Loss: 1.6600\n",
      "Epoch: 3/20... Step: 3300... Loss: 1.7732... Val Loss: 1.6548\n",
      "Epoch: 3/20... Step: 3310... Loss: 1.7126... Val Loss: 1.6542\n",
      "Epoch: 3/20... Step: 3320... Loss: 1.6818... Val Loss: 1.6543\n",
      "Epoch: 3/20... Step: 3330... Loss: 1.6574... Val Loss: 1.6477\n",
      "Epoch: 3/20... Step: 3340... Loss: 1.6743... Val Loss: 1.6477\n",
      "Epoch: 3/20... Step: 3350... Loss: 1.7729... Val Loss: 1.6463\n",
      "Epoch: 3/20... Step: 3360... Loss: 1.8733... Val Loss: 1.6574\n",
      "Epoch: 3/20... Step: 3370... Loss: 1.7161... Val Loss: 1.6502\n",
      "Epoch: 3/20... Step: 3380... Loss: 1.6971... Val Loss: 1.6499\n",
      "Epoch: 3/20... Step: 3390... Loss: 1.6991... Val Loss: 1.6470\n",
      "Epoch: 3/20... Step: 3400... Loss: 1.7880... Val Loss: 1.6432\n",
      "Epoch: 3/20... Step: 3410... Loss: 1.7569... Val Loss: 1.6432\n",
      "Epoch: 3/20... Step: 3420... Loss: 1.7811... Val Loss: 1.6416\n",
      "Epoch: 3/20... Step: 3430... Loss: 1.7766... Val Loss: 1.6400\n",
      "Epoch: 3/20... Step: 3440... Loss: 1.7688... Val Loss: 1.6435\n",
      "Epoch: 3/20... Step: 3450... Loss: 1.7293... Val Loss: 1.6378\n",
      "Epoch: 3/20... Step: 3460... Loss: 1.7570... Val Loss: 1.6368\n",
      "Epoch: 3/20... Step: 3470... Loss: 1.6866... Val Loss: 1.6365\n",
      "Epoch: 3/20... Step: 3480... Loss: 1.7446... Val Loss: 1.6378\n",
      "Epoch: 3/20... Step: 3490... Loss: 1.6849... Val Loss: 1.6388\n",
      "Epoch: 3/20... Step: 3500... Loss: 1.7260... Val Loss: 1.6367\n",
      "Epoch: 3/20... Step: 3510... Loss: 1.7175... Val Loss: 1.6326\n",
      "Epoch: 3/20... Step: 3520... Loss: 1.6219... Val Loss: 1.6329\n",
      "Epoch: 3/20... Step: 3530... Loss: 1.6716... Val Loss: 1.6285\n",
      "Epoch: 3/20... Step: 3540... Loss: 1.6412... Val Loss: 1.6280\n",
      "Epoch: 3/20... Step: 3550... Loss: 1.6756... Val Loss: 1.6254\n",
      "Epoch: 3/20... Step: 3560... Loss: 1.7501... Val Loss: 1.6239\n",
      "Epoch: 3/20... Step: 3570... Loss: 1.6310... Val Loss: 1.6244\n",
      "Epoch: 3/20... Step: 3580... Loss: 1.7368... Val Loss: 1.6231\n",
      "Epoch: 3/20... Step: 3590... Loss: 1.6734... Val Loss: 1.6237\n",
      "Epoch: 3/20... Step: 3600... Loss: 1.6419... Val Loss: 1.6241\n",
      "Epoch: 3/20... Step: 3610... Loss: 1.7829... Val Loss: 1.6252\n",
      "Epoch: 3/20... Step: 3620... Loss: 1.7358... Val Loss: 1.6253\n",
      "Epoch: 3/20... Step: 3630... Loss: 1.7182... Val Loss: 1.6215\n",
      "Epoch: 3/20... Step: 3640... Loss: 1.8042... Val Loss: 1.6195\n",
      "Epoch: 3/20... Step: 3650... Loss: 1.7510... Val Loss: 1.6204\n",
      "Epoch: 3/20... Step: 3660... Loss: 1.6289... Val Loss: 1.6194\n",
      "Epoch: 3/20... Step: 3670... Loss: 1.7279... Val Loss: 1.6207\n",
      "Epoch: 3/20... Step: 3680... Loss: 1.6620... Val Loss: 1.6141\n",
      "Epoch: 3/20... Step: 3690... Loss: 1.7254... Val Loss: 1.6186\n",
      "Epoch: 3/20... Step: 3700... Loss: 1.6850... Val Loss: 1.6162\n",
      "Epoch: 3/20... Step: 3710... Loss: 1.7273... Val Loss: 1.6195\n",
      "Epoch: 3/20... Step: 3720... Loss: 1.6555... Val Loss: 1.6130\n",
      "Epoch: 3/20... Step: 3730... Loss: 1.6822... Val Loss: 1.6149\n",
      "Epoch: 3/20... Step: 3740... Loss: 1.7109... Val Loss: 1.6142\n",
      "Epoch: 3/20... Step: 3750... Loss: 1.6139... Val Loss: 1.6156\n",
      "Epoch: 3/20... Step: 3760... Loss: 1.5847... Val Loss: 1.6141\n",
      "Epoch: 3/20... Step: 3770... Loss: 1.6665... Val Loss: 1.6154\n",
      "Epoch: 3/20... Step: 3780... Loss: 1.7151... Val Loss: 1.6115\n",
      "Epoch: 3/20... Step: 3790... Loss: 1.7466... Val Loss: 1.6090\n",
      "Epoch: 3/20... Step: 3800... Loss: 1.6628... Val Loss: 1.6078\n",
      "Epoch: 3/20... Step: 3810... Loss: 1.7308... Val Loss: 1.6081\n",
      "Epoch: 3/20... Step: 3820... Loss: 1.6526... Val Loss: 1.6053\n",
      "Epoch: 3/20... Step: 3830... Loss: 1.6418... Val Loss: 1.6028\n",
      "Epoch: 3/20... Step: 3840... Loss: 1.6854... Val Loss: 1.6069\n",
      "Epoch: 3/20... Step: 3850... Loss: 1.6111... Val Loss: 1.6024\n",
      "Epoch: 3/20... Step: 3860... Loss: 1.6807... Val Loss: 1.6073\n",
      "Epoch: 3/20... Step: 3870... Loss: 1.6466... Val Loss: 1.6073\n",
      "Epoch: 3/20... Step: 3880... Loss: 1.6691... Val Loss: 1.6064\n",
      "Epoch: 3/20... Step: 3890... Loss: 1.5953... Val Loss: 1.6001\n",
      "Epoch: 3/20... Step: 3900... Loss: 1.6592... Val Loss: 1.6009\n",
      "Epoch: 3/20... Step: 3910... Loss: 1.6844... Val Loss: 1.5967\n",
      "Epoch: 3/20... Step: 3920... Loss: 1.6634... Val Loss: 1.5954\n",
      "Epoch: 3/20... Step: 3930... Loss: 1.6250... Val Loss: 1.5932\n",
      "Epoch: 3/20... Step: 3940... Loss: 1.6991... Val Loss: 1.5941\n",
      "Epoch: 3/20... Step: 3950... Loss: 1.6345... Val Loss: 1.5956\n",
      "Epoch: 3/20... Step: 3960... Loss: 1.6626... Val Loss: 1.5930\n",
      "Epoch: 3/20... Step: 3970... Loss: 1.6477... Val Loss: 1.5869\n",
      "Epoch: 3/20... Step: 3980... Loss: 1.6430... Val Loss: 1.5888\n",
      "Epoch: 3/20... Step: 3990... Loss: 1.7026... Val Loss: 1.5917\n",
      "Epoch: 3/20... Step: 4000... Loss: 1.6015... Val Loss: 1.5886\n",
      "Epoch: 3/20... Step: 4010... Loss: 1.6498... Val Loss: 1.5892\n",
      "Epoch: 3/20... Step: 4020... Loss: 1.6321... Val Loss: 1.5923\n",
      "Epoch: 3/20... Step: 4030... Loss: 1.7258... Val Loss: 1.5852\n",
      "Epoch: 3/20... Step: 4040... Loss: 1.6287... Val Loss: 1.5843\n",
      "Epoch: 3/20... Step: 4050... Loss: 1.6091... Val Loss: 1.5874\n",
      "Epoch: 3/20... Step: 4060... Loss: 1.5705... Val Loss: 1.5841\n",
      "Epoch: 3/20... Step: 4070... Loss: 1.7381... Val Loss: 1.5853\n",
      "Epoch: 3/20... Step: 4080... Loss: 1.6176... Val Loss: 1.5820\n",
      "Epoch: 3/20... Step: 4090... Loss: 1.6612... Val Loss: 1.5806\n",
      "Epoch: 3/20... Step: 4100... Loss: 1.6736... Val Loss: 1.5772\n",
      "Epoch: 3/20... Step: 4110... Loss: 1.7038... Val Loss: 1.5763\n",
      "Epoch: 3/20... Step: 4120... Loss: 1.6870... Val Loss: 1.5793\n",
      "Epoch: 3/20... Step: 4130... Loss: 1.6805... Val Loss: 1.5811\n",
      "Epoch: 3/20... Step: 4140... Loss: 1.6750... Val Loss: 1.5785\n",
      "Epoch: 3/20... Step: 4150... Loss: 1.6690... Val Loss: 1.5764\n",
      "Epoch: 3/20... Step: 4160... Loss: 1.5872... Val Loss: 1.5794\n",
      "Epoch: 3/20... Step: 4170... Loss: 1.6828... Val Loss: 1.5758\n",
      "Epoch: 3/20... Step: 4180... Loss: 1.6938... Val Loss: 1.5740\n",
      "Epoch: 3/20... Step: 4190... Loss: 1.6579... Val Loss: 1.5771\n",
      "Epoch: 3/20... Step: 4200... Loss: 1.6280... Val Loss: 1.5736\n",
      "Epoch: 3/20... Step: 4210... Loss: 1.7381... Val Loss: 1.5751\n",
      "Epoch: 3/20... Step: 4220... Loss: 1.6354... Val Loss: 1.5775\n",
      "Epoch: 3/20... Step: 4230... Loss: 1.5977... Val Loss: 1.5747\n",
      "Epoch: 3/20... Step: 4240... Loss: 1.6379... Val Loss: 1.5723\n",
      "Epoch: 3/20... Step: 4250... Loss: 1.5634... Val Loss: 1.5734\n",
      "Epoch: 4/20... Step: 4260... Loss: 1.6563... Val Loss: 1.5773\n",
      "Epoch: 4/20... Step: 4270... Loss: 1.6084... Val Loss: 1.5640\n",
      "Epoch: 4/20... Step: 4280... Loss: 1.6648... Val Loss: 1.5644\n",
      "Epoch: 4/20... Step: 4290... Loss: 1.5499... Val Loss: 1.5652\n",
      "Epoch: 4/20... Step: 4300... Loss: 1.5914... Val Loss: 1.5644\n",
      "Epoch: 4/20... Step: 4310... Loss: 1.6424... Val Loss: 1.5633\n",
      "Epoch: 4/20... Step: 4320... Loss: 1.6066... Val Loss: 1.5631\n",
      "Epoch: 4/20... Step: 4330... Loss: 1.6857... Val Loss: 1.5593\n",
      "Epoch: 4/20... Step: 4340... Loss: 1.5464... Val Loss: 1.5591\n",
      "Epoch: 4/20... Step: 4350... Loss: 1.7131... Val Loss: 1.5621\n",
      "Epoch: 4/20... Step: 4360... Loss: 1.6081... Val Loss: 1.5625\n",
      "Epoch: 4/20... Step: 4370... Loss: 1.5953... Val Loss: 1.5570\n",
      "Epoch: 4/20... Step: 4380... Loss: 1.6781... Val Loss: 1.5559\n",
      "Epoch: 4/20... Step: 4390... Loss: 1.6018... Val Loss: 1.5542\n",
      "Epoch: 4/20... Step: 4400... Loss: 1.6378... Val Loss: 1.5601\n",
      "Epoch: 4/20... Step: 4410... Loss: 1.5743... Val Loss: 1.5577\n",
      "Epoch: 4/20... Step: 4420... Loss: 1.5714... Val Loss: 1.5571\n",
      "Epoch: 4/20... Step: 4430... Loss: 1.6253... Val Loss: 1.5579\n",
      "Epoch: 4/20... Step: 4440... Loss: 1.6166... Val Loss: 1.5600\n",
      "Epoch: 4/20... Step: 4450... Loss: 1.6012... Val Loss: 1.5567\n",
      "Epoch: 4/20... Step: 4460... Loss: 1.6045... Val Loss: 1.5549\n",
      "Epoch: 4/20... Step: 4470... Loss: 1.6125... Val Loss: 1.5559\n",
      "Epoch: 4/20... Step: 4480... Loss: 1.7058... Val Loss: 1.5550\n",
      "Epoch: 4/20... Step: 4490... Loss: 1.5637... Val Loss: 1.5623\n",
      "Epoch: 4/20... Step: 4500... Loss: 1.6131... Val Loss: 1.5564\n",
      "Epoch: 4/20... Step: 4510... Loss: 1.5947... Val Loss: 1.5546\n",
      "Epoch: 4/20... Step: 4520... Loss: 1.6364... Val Loss: 1.5510\n",
      "Epoch: 4/20... Step: 4530... Loss: 1.5981... Val Loss: 1.5529\n",
      "Epoch: 4/20... Step: 4540... Loss: 1.6692... Val Loss: 1.5546\n",
      "Epoch: 4/20... Step: 4550... Loss: 1.6613... Val Loss: 1.5553\n",
      "Epoch: 4/20... Step: 4560... Loss: 1.6031... Val Loss: 1.5545\n",
      "Epoch: 4/20... Step: 4570... Loss: 1.5644... Val Loss: 1.5471\n",
      "Epoch: 4/20... Step: 4580... Loss: 1.5880... Val Loss: 1.5485\n",
      "Epoch: 4/20... Step: 4590... Loss: 1.5675... Val Loss: 1.5540\n",
      "Epoch: 4/20... Step: 4600... Loss: 1.5588... Val Loss: 1.5491\n",
      "Epoch: 4/20... Step: 4610... Loss: 1.6149... Val Loss: 1.5469\n",
      "Epoch: 4/20... Step: 4620... Loss: 1.6238... Val Loss: 1.5499\n",
      "Epoch: 4/20... Step: 4630... Loss: 1.6568... Val Loss: 1.5432\n",
      "Epoch: 4/20... Step: 4640... Loss: 1.6639... Val Loss: 1.5442\n",
      "Epoch: 4/20... Step: 4650... Loss: 1.6535... Val Loss: 1.5404\n",
      "Epoch: 4/20... Step: 4660... Loss: 1.4974... Val Loss: 1.5419\n",
      "Epoch: 4/20... Step: 4670... Loss: 1.5295... Val Loss: 1.5438\n",
      "Epoch: 4/20... Step: 4680... Loss: 1.6378... Val Loss: 1.5422\n",
      "Epoch: 4/20... Step: 4690... Loss: 1.6246... Val Loss: 1.5416\n",
      "Epoch: 4/20... Step: 4700... Loss: 1.6008... Val Loss: 1.5458\n",
      "Epoch: 4/20... Step: 4710... Loss: 1.5816... Val Loss: 1.5414\n",
      "Epoch: 4/20... Step: 4720... Loss: 1.6423... Val Loss: 1.5400\n",
      "Epoch: 4/20... Step: 4730... Loss: 1.6068... Val Loss: 1.5448\n",
      "Epoch: 4/20... Step: 4740... Loss: 1.6349... Val Loss: 1.5419\n",
      "Epoch: 4/20... Step: 4750... Loss: 1.6317... Val Loss: 1.5418\n",
      "Epoch: 4/20... Step: 4760... Loss: 1.6100... Val Loss: 1.5389\n",
      "Epoch: 4/20... Step: 4770... Loss: 1.5889... Val Loss: 1.5375\n",
      "Epoch: 4/20... Step: 4780... Loss: 1.6456... Val Loss: 1.5390\n",
      "Epoch: 4/20... Step: 4790... Loss: 1.5771... Val Loss: 1.5346\n",
      "Epoch: 4/20... Step: 4800... Loss: 1.5883... Val Loss: 1.5402\n",
      "Epoch: 4/20... Step: 4810... Loss: 1.5920... Val Loss: 1.5379\n",
      "Epoch: 4/20... Step: 4820... Loss: 1.5726... Val Loss: 1.5346\n",
      "Epoch: 4/20... Step: 4830... Loss: 1.5658... Val Loss: 1.5344\n",
      "Epoch: 4/20... Step: 4840... Loss: 1.5980... Val Loss: 1.5338\n",
      "Epoch: 4/20... Step: 4850... Loss: 1.5373... Val Loss: 1.5337\n",
      "Epoch: 4/20... Step: 4860... Loss: 1.6193... Val Loss: 1.5356\n",
      "Epoch: 4/20... Step: 4870... Loss: 1.5937... Val Loss: 1.5327\n",
      "Epoch: 4/20... Step: 4880... Loss: 1.5977... Val Loss: 1.5334\n",
      "Epoch: 4/20... Step: 4890... Loss: 1.6024... Val Loss: 1.5313\n",
      "Epoch: 4/20... Step: 4900... Loss: 1.6753... Val Loss: 1.5308\n",
      "Epoch: 4/20... Step: 4910... Loss: 1.5133... Val Loss: 1.5359\n",
      "Epoch: 4/20... Step: 4920... Loss: 1.6302... Val Loss: 1.5329\n",
      "Epoch: 4/20... Step: 4930... Loss: 1.6019... Val Loss: 1.5293\n",
      "Epoch: 4/20... Step: 4940... Loss: 1.6068... Val Loss: 1.5300\n",
      "Epoch: 4/20... Step: 4950... Loss: 1.5481... Val Loss: 1.5305\n",
      "Epoch: 4/20... Step: 4960... Loss: 1.5433... Val Loss: 1.5300\n",
      "Epoch: 4/20... Step: 4970... Loss: 1.6287... Val Loss: 1.5282\n",
      "Epoch: 4/20... Step: 4980... Loss: 1.6118... Val Loss: 1.5230\n",
      "Epoch: 4/20... Step: 4990... Loss: 1.5629... Val Loss: 1.5287\n",
      "Epoch: 4/20... Step: 5000... Loss: 1.6590... Val Loss: 1.5248\n",
      "Epoch: 4/20... Step: 5010... Loss: 1.6148... Val Loss: 1.5219\n",
      "Epoch: 4/20... Step: 5020... Loss: 1.5716... Val Loss: 1.5242\n",
      "Epoch: 4/20... Step: 5030... Loss: 1.6046... Val Loss: 1.5236\n",
      "Epoch: 4/20... Step: 5040... Loss: 1.5879... Val Loss: 1.5269\n",
      "Epoch: 4/20... Step: 5050... Loss: 1.5884... Val Loss: 1.5198\n",
      "Epoch: 4/20... Step: 5060... Loss: 1.5970... Val Loss: 1.5215\n",
      "Epoch: 4/20... Step: 5070... Loss: 1.5731... Val Loss: 1.5178\n",
      "Epoch: 4/20... Step: 5080... Loss: 1.5257... Val Loss: 1.5219\n",
      "Epoch: 4/20... Step: 5090... Loss: 1.5978... Val Loss: 1.5233\n",
      "Epoch: 4/20... Step: 5100... Loss: 1.5683... Val Loss: 1.5214\n",
      "Epoch: 4/20... Step: 5110... Loss: 1.5691... Val Loss: 1.5213\n",
      "Epoch: 4/20... Step: 5120... Loss: 1.7159... Val Loss: 1.5213\n",
      "Epoch: 4/20... Step: 5130... Loss: 1.5079... Val Loss: 1.5221\n",
      "Epoch: 4/20... Step: 5140... Loss: 1.5881... Val Loss: 1.5219\n",
      "Epoch: 4/20... Step: 5150... Loss: 1.5646... Val Loss: 1.5226\n",
      "Epoch: 4/20... Step: 5160... Loss: 1.6413... Val Loss: 1.5198\n",
      "Epoch: 4/20... Step: 5170... Loss: 1.6199... Val Loss: 1.5207\n",
      "Epoch: 4/20... Step: 5180... Loss: 1.5941... Val Loss: 1.5222\n",
      "Epoch: 4/20... Step: 5190... Loss: 1.5137... Val Loss: 1.5181\n",
      "Epoch: 4/20... Step: 5200... Loss: 1.5648... Val Loss: 1.5165\n",
      "Epoch: 4/20... Step: 5210... Loss: 1.5501... Val Loss: 1.5201\n",
      "Epoch: 4/20... Step: 5220... Loss: 1.5654... Val Loss: 1.5145\n",
      "Epoch: 4/20... Step: 5230... Loss: 1.5359... Val Loss: 1.5129\n",
      "Epoch: 4/20... Step: 5240... Loss: 1.5525... Val Loss: 1.5166\n",
      "Epoch: 4/20... Step: 5250... Loss: 1.5449... Val Loss: 1.5171\n",
      "Epoch: 4/20... Step: 5260... Loss: 1.5061... Val Loss: 1.5148\n",
      "Epoch: 4/20... Step: 5270... Loss: 1.5588... Val Loss: 1.5131\n",
      "Epoch: 4/20... Step: 5280... Loss: 1.5489... Val Loss: 1.5201\n",
      "Epoch: 4/20... Step: 5290... Loss: 1.5703... Val Loss: 1.5181\n",
      "Epoch: 4/20... Step: 5300... Loss: 1.5640... Val Loss: 1.5158\n",
      "Epoch: 4/20... Step: 5310... Loss: 1.4983... Val Loss: 1.5179\n",
      "Epoch: 4/20... Step: 5320... Loss: 1.5746... Val Loss: 1.5166\n",
      "Epoch: 4/20... Step: 5330... Loss: 1.6186... Val Loss: 1.5094\n",
      "Epoch: 4/20... Step: 5340... Loss: 1.5790... Val Loss: 1.5104\n",
      "Epoch: 4/20... Step: 5350... Loss: 1.6175... Val Loss: 1.5097\n",
      "Epoch: 4/20... Step: 5360... Loss: 1.5758... Val Loss: 1.5090\n",
      "Epoch: 4/20... Step: 5370... Loss: 1.5508... Val Loss: 1.5088\n",
      "Epoch: 4/20... Step: 5380... Loss: 1.5052... Val Loss: 1.5081\n",
      "Epoch: 4/20... Step: 5390... Loss: 1.5734... Val Loss: 1.5035\n",
      "Epoch: 4/20... Step: 5400... Loss: 1.5976... Val Loss: 1.5045\n",
      "Epoch: 4/20... Step: 5410... Loss: 1.5082... Val Loss: 1.5081\n",
      "Epoch: 4/20... Step: 5420... Loss: 1.5210... Val Loss: 1.5066\n",
      "Epoch: 4/20... Step: 5430... Loss: 1.4964... Val Loss: 1.5062\n",
      "Epoch: 4/20... Step: 5440... Loss: 1.5035... Val Loss: 1.5093\n",
      "Epoch: 4/20... Step: 5450... Loss: 1.5652... Val Loss: 1.5019\n",
      "Epoch: 4/20... Step: 5460... Loss: 1.5813... Val Loss: 1.5017\n",
      "Epoch: 4/20... Step: 5470... Loss: 1.5086... Val Loss: 1.5076\n",
      "Epoch: 4/20... Step: 5480... Loss: 1.5330... Val Loss: 1.4990\n",
      "Epoch: 4/20... Step: 5490... Loss: 1.5095... Val Loss: 1.4991\n",
      "Epoch: 4/20... Step: 5500... Loss: 1.5440... Val Loss: 1.5006\n",
      "Epoch: 4/20... Step: 5510... Loss: 1.5905... Val Loss: 1.5002\n",
      "Epoch: 4/20... Step: 5520... Loss: 1.5907... Val Loss: 1.4970\n",
      "Epoch: 4/20... Step: 5530... Loss: 1.5600... Val Loss: 1.5095\n",
      "Epoch: 4/20... Step: 5540... Loss: 1.5456... Val Loss: 1.4984\n",
      "Epoch: 4/20... Step: 5550... Loss: 1.4598... Val Loss: 1.5009\n",
      "Epoch: 4/20... Step: 5560... Loss: 1.5195... Val Loss: 1.4998\n",
      "Epoch: 4/20... Step: 5570... Loss: 1.5903... Val Loss: 1.4959\n",
      "Epoch: 4/20... Step: 5580... Loss: 1.6193... Val Loss: 1.5003\n",
      "Epoch: 4/20... Step: 5590... Loss: 1.5905... Val Loss: 1.4967\n",
      "Epoch: 4/20... Step: 5600... Loss: 1.5441... Val Loss: 1.4977\n",
      "Epoch: 4/20... Step: 5610... Loss: 1.6000... Val Loss: 1.4958\n",
      "Epoch: 4/20... Step: 5620... Loss: 1.5922... Val Loss: 1.4974\n",
      "Epoch: 4/20... Step: 5630... Loss: 1.5528... Val Loss: 1.4935\n",
      "Epoch: 4/20... Step: 5640... Loss: 1.6379... Val Loss: 1.4975\n",
      "Epoch: 4/20... Step: 5650... Loss: 1.5459... Val Loss: 1.4980\n",
      "Epoch: 4/20... Step: 5660... Loss: 1.5537... Val Loss: 1.4964\n",
      "Epoch: 4/20... Step: 5670... Loss: 1.5959... Val Loss: 1.4965\n",
      "Epoch: 5/20... Step: 5680... Loss: 1.5862... Val Loss: 1.4974\n",
      "Epoch: 5/20... Step: 5690... Loss: 1.5924... Val Loss: 1.4898\n",
      "Epoch: 5/20... Step: 5700... Loss: 1.5128... Val Loss: 1.4889\n",
      "Epoch: 5/20... Step: 5710... Loss: 1.6113... Val Loss: 1.4905\n",
      "Epoch: 5/20... Step: 5720... Loss: 1.5095... Val Loss: 1.4874\n",
      "Epoch: 5/20... Step: 5730... Loss: 1.6078... Val Loss: 1.4892\n",
      "Epoch: 5/20... Step: 5740... Loss: 1.4299... Val Loss: 1.4901\n",
      "Epoch: 5/20... Step: 5750... Loss: 1.5408... Val Loss: 1.4862\n",
      "Epoch: 5/20... Step: 5760... Loss: 1.5331... Val Loss: 1.4873\n",
      "Epoch: 5/20... Step: 5770... Loss: 1.5924... Val Loss: 1.4898\n",
      "Epoch: 5/20... Step: 5780... Loss: 1.5248... Val Loss: 1.4903\n",
      "Epoch: 5/20... Step: 5790... Loss: 1.5262... Val Loss: 1.4868\n",
      "Epoch: 5/20... Step: 5800... Loss: 1.5690... Val Loss: 1.4857\n",
      "Epoch: 5/20... Step: 5810... Loss: 1.5231... Val Loss: 1.4884\n",
      "Epoch: 5/20... Step: 5820... Loss: 1.5861... Val Loss: 1.4882\n",
      "Epoch: 5/20... Step: 5830... Loss: 1.4959... Val Loss: 1.4871\n",
      "Epoch: 5/20... Step: 5840... Loss: 1.5234... Val Loss: 1.4857\n",
      "Epoch: 5/20... Step: 5850... Loss: 1.5740... Val Loss: 1.4864\n",
      "Epoch: 5/20... Step: 5860... Loss: 1.5661... Val Loss: 1.4898\n",
      "Epoch: 5/20... Step: 5870... Loss: 1.5683... Val Loss: 1.4874\n",
      "Epoch: 5/20... Step: 5880... Loss: 1.4739... Val Loss: 1.4906\n",
      "Epoch: 5/20... Step: 5890... Loss: 1.5651... Val Loss: 1.4869\n",
      "Epoch: 5/20... Step: 5900... Loss: 1.4879... Val Loss: 1.4859\n",
      "Epoch: 5/20... Step: 5910... Loss: 1.4787... Val Loss: 1.4870\n",
      "Epoch: 5/20... Step: 5920... Loss: 1.5510... Val Loss: 1.4859\n",
      "Epoch: 5/20... Step: 5930... Loss: 1.5727... Val Loss: 1.4853\n",
      "Epoch: 5/20... Step: 5940... Loss: 1.5261... Val Loss: 1.4870\n",
      "Epoch: 5/20... Step: 5950... Loss: 1.5991... Val Loss: 1.4851\n",
      "Epoch: 5/20... Step: 5960... Loss: 1.5639... Val Loss: 1.4881\n",
      "Epoch: 5/20... Step: 5970... Loss: 1.5597... Val Loss: 1.4824\n",
      "Epoch: 5/20... Step: 5980... Loss: 1.4821... Val Loss: 1.4868\n",
      "Epoch: 5/20... Step: 5990... Loss: 1.5645... Val Loss: 1.4836\n",
      "Epoch: 5/20... Step: 6000... Loss: 1.5777... Val Loss: 1.4809\n",
      "Epoch: 5/20... Step: 6010... Loss: 1.6203... Val Loss: 1.4872\n",
      "Epoch: 5/20... Step: 6020... Loss: 1.5359... Val Loss: 1.4800\n",
      "Epoch: 5/20... Step: 6030... Loss: 1.6209... Val Loss: 1.4805\n",
      "Epoch: 5/20... Step: 6040... Loss: 1.5794... Val Loss: 1.4815\n",
      "Epoch: 5/20... Step: 6050... Loss: 1.5358... Val Loss: 1.4753\n",
      "Epoch: 5/20... Step: 6060... Loss: 1.5449... Val Loss: 1.4814\n",
      "Epoch: 5/20... Step: 6070... Loss: 1.5770... Val Loss: 1.4803\n",
      "Epoch: 5/20... Step: 6080... Loss: 1.5323... Val Loss: 1.4774\n",
      "Epoch: 5/20... Step: 6090... Loss: 1.4530... Val Loss: 1.4847\n",
      "Epoch: 5/20... Step: 6100... Loss: 1.5220... Val Loss: 1.4808\n",
      "Epoch: 5/20... Step: 6110... Loss: 1.5473... Val Loss: 1.4792\n",
      "Epoch: 5/20... Step: 6120... Loss: 1.5128... Val Loss: 1.4841\n",
      "Epoch: 5/20... Step: 6130... Loss: 1.5707... Val Loss: 1.4793\n",
      "Epoch: 5/20... Step: 6140... Loss: 1.5649... Val Loss: 1.4775\n",
      "Epoch: 5/20... Step: 6150... Loss: 1.4319... Val Loss: 1.4764\n",
      "Epoch: 5/20... Step: 6160... Loss: 1.5540... Val Loss: 1.4810\n",
      "Epoch: 5/20... Step: 6170... Loss: 1.5801... Val Loss: 1.4776\n",
      "Epoch: 5/20... Step: 6180... Loss: 1.5440... Val Loss: 1.4752\n",
      "Epoch: 5/20... Step: 6190... Loss: 1.5351... Val Loss: 1.4808\n",
      "Epoch: 5/20... Step: 6200... Loss: 1.5031... Val Loss: 1.4729\n",
      "Epoch: 5/20... Step: 6210... Loss: 1.6090... Val Loss: 1.4730\n",
      "Epoch: 5/20... Step: 6220... Loss: 1.4994... Val Loss: 1.4754\n",
      "Epoch: 5/20... Step: 6230... Loss: 1.4966... Val Loss: 1.4731\n",
      "Epoch: 5/20... Step: 6240... Loss: 1.5372... Val Loss: 1.4702\n",
      "Epoch: 5/20... Step: 6250... Loss: 1.4680... Val Loss: 1.4766\n",
      "Epoch: 5/20... Step: 6260... Loss: 1.5244... Val Loss: 1.4728\n",
      "Epoch: 5/20... Step: 6270... Loss: 1.5092... Val Loss: 1.4730\n",
      "Epoch: 5/20... Step: 6280... Loss: 1.5025... Val Loss: 1.4727\n",
      "Epoch: 5/20... Step: 6290... Loss: 1.5726... Val Loss: 1.4687\n",
      "Epoch: 5/20... Step: 6300... Loss: 1.5431... Val Loss: 1.4696\n",
      "Epoch: 5/20... Step: 6310... Loss: 1.5565... Val Loss: 1.4729\n",
      "Epoch: 5/20... Step: 6320... Loss: 1.5654... Val Loss: 1.4705\n",
      "Epoch: 5/20... Step: 6330... Loss: 1.5276... Val Loss: 1.4766\n",
      "Epoch: 5/20... Step: 6340... Loss: 1.5834... Val Loss: 1.4732\n",
      "Epoch: 5/20... Step: 6350... Loss: 1.5169... Val Loss: 1.4729\n",
      "Epoch: 5/20... Step: 6360... Loss: 1.4922... Val Loss: 1.4736\n",
      "Epoch: 5/20... Step: 6370... Loss: 1.5075... Val Loss: 1.4744\n",
      "Epoch: 5/20... Step: 6380... Loss: 1.5335... Val Loss: 1.4718\n",
      "Epoch: 5/20... Step: 6390... Loss: 1.5177... Val Loss: 1.4700\n",
      "Epoch: 5/20... Step: 6400... Loss: 1.5607... Val Loss: 1.4671\n",
      "Epoch: 5/20... Step: 6410... Loss: 1.4890... Val Loss: 1.4706\n",
      "Epoch: 5/20... Step: 6420... Loss: 1.5470... Val Loss: 1.4655\n",
      "Epoch: 5/20... Step: 6430... Loss: 1.5572... Val Loss: 1.4674\n",
      "Epoch: 5/20... Step: 6440... Loss: 1.5911... Val Loss: 1.4693\n",
      "Epoch: 5/20... Step: 6450... Loss: 1.5863... Val Loss: 1.4691\n",
      "Epoch: 5/20... Step: 6460... Loss: 1.5823... Val Loss: 1.4714\n",
      "Epoch: 5/20... Step: 6470... Loss: 1.5381... Val Loss: 1.4699\n",
      "Epoch: 5/20... Step: 6480... Loss: 1.5104... Val Loss: 1.4661\n",
      "Epoch: 5/20... Step: 6490... Loss: 1.5009... Val Loss: 1.4661\n",
      "Epoch: 5/20... Step: 6500... Loss: 1.4562... Val Loss: 1.4695\n",
      "Epoch: 5/20... Step: 6510... Loss: 1.5819... Val Loss: 1.4702\n",
      "Epoch: 5/20... Step: 6520... Loss: 1.5161... Val Loss: 1.4662\n",
      "Epoch: 5/20... Step: 6530... Loss: 1.5082... Val Loss: 1.4641\n",
      "Epoch: 5/20... Step: 6540... Loss: 1.5418... Val Loss: 1.4662\n",
      "Epoch: 5/20... Step: 6550... Loss: 1.4861... Val Loss: 1.4654\n",
      "Epoch: 5/20... Step: 6560... Loss: 1.4656... Val Loss: 1.4649\n",
      "Epoch: 5/20... Step: 6570... Loss: 1.5061... Val Loss: 1.4640\n",
      "Epoch: 5/20... Step: 6580... Loss: 1.5090... Val Loss: 1.4685\n",
      "Epoch: 5/20... Step: 6590... Loss: 1.4941... Val Loss: 1.4647\n",
      "Epoch: 5/20... Step: 6600... Loss: 1.4814... Val Loss: 1.4716\n",
      "Epoch: 5/20... Step: 6610... Loss: 1.4983... Val Loss: 1.4638\n",
      "Epoch: 5/20... Step: 6620... Loss: 1.5568... Val Loss: 1.4646\n",
      "Epoch: 5/20... Step: 6630... Loss: 1.5217... Val Loss: 1.4656\n",
      "Epoch: 5/20... Step: 6640... Loss: 1.5369... Val Loss: 1.4624\n",
      "Epoch: 5/20... Step: 6650... Loss: 1.5351... Val Loss: 1.4607\n",
      "Epoch: 5/20... Step: 6660... Loss: 1.5305... Val Loss: 1.4650\n",
      "Epoch: 5/20... Step: 6670... Loss: 1.4531... Val Loss: 1.4627\n",
      "Epoch: 5/20... Step: 6680... Loss: 1.5040... Val Loss: 1.4646\n",
      "Epoch: 5/20... Step: 6690... Loss: 1.5683... Val Loss: 1.4611\n",
      "Epoch: 5/20... Step: 6700... Loss: 1.5987... Val Loss: 1.4654\n",
      "Epoch: 5/20... Step: 6710... Loss: 1.4366... Val Loss: 1.4689\n",
      "Epoch: 5/20... Step: 6720... Loss: 1.5163... Val Loss: 1.4645\n",
      "Epoch: 5/20... Step: 6730... Loss: 1.4833... Val Loss: 1.4638\n",
      "Epoch: 5/20... Step: 6740... Loss: 1.4328... Val Loss: 1.4667\n",
      "Epoch: 5/20... Step: 6750... Loss: 1.5931... Val Loss: 1.4594\n",
      "Epoch: 5/20... Step: 6760... Loss: 1.4984... Val Loss: 1.4630\n",
      "Epoch: 5/20... Step: 6770... Loss: 1.5245... Val Loss: 1.4587\n",
      "Epoch: 5/20... Step: 6780... Loss: 1.4991... Val Loss: 1.4578\n",
      "Epoch: 5/20... Step: 6790... Loss: 1.5268... Val Loss: 1.4596\n",
      "Epoch: 5/20... Step: 6800... Loss: 1.5230... Val Loss: 1.4589\n",
      "Epoch: 5/20... Step: 6810... Loss: 1.4766... Val Loss: 1.4559\n",
      "Epoch: 5/20... Step: 6820... Loss: 1.5801... Val Loss: 1.4575\n",
      "Epoch: 5/20... Step: 6830... Loss: 1.4826... Val Loss: 1.4572\n",
      "Epoch: 5/20... Step: 6840... Loss: 1.4527... Val Loss: 1.4566\n",
      "Epoch: 5/20... Step: 6850... Loss: 1.5175... Val Loss: 1.4551\n",
      "Epoch: 5/20... Step: 6860... Loss: 1.5591... Val Loss: 1.4602\n",
      "Epoch: 5/20... Step: 6870... Loss: 1.4996... Val Loss: 1.4527\n",
      "Epoch: 5/20... Step: 6880... Loss: 1.5993... Val Loss: 1.4565\n",
      "Epoch: 5/20... Step: 6890... Loss: 1.5255... Val Loss: 1.4568\n",
      "Epoch: 5/20... Step: 6900... Loss: 1.5455... Val Loss: 1.4540\n",
      "Epoch: 5/20... Step: 6910... Loss: 1.4908... Val Loss: 1.4519\n",
      "Epoch: 5/20... Step: 6920... Loss: 1.5400... Val Loss: 1.4532\n",
      "Epoch: 5/20... Step: 6930... Loss: 1.5629... Val Loss: 1.4539\n",
      "Epoch: 5/20... Step: 6940... Loss: 1.5204... Val Loss: 1.4489\n",
      "Epoch: 5/20... Step: 6950... Loss: 1.5121... Val Loss: 1.4594\n",
      "Epoch: 5/20... Step: 6960... Loss: 1.5028... Val Loss: 1.4505\n",
      "Epoch: 5/20... Step: 6970... Loss: 1.4671... Val Loss: 1.4547\n",
      "Epoch: 5/20... Step: 6980... Loss: 1.4858... Val Loss: 1.4547\n",
      "Epoch: 5/20... Step: 6990... Loss: 1.5822... Val Loss: 1.4495\n",
      "Epoch: 5/20... Step: 7000... Loss: 1.5715... Val Loss: 1.4556\n",
      "Epoch: 5/20... Step: 7010... Loss: 1.4968... Val Loss: 1.4498\n",
      "Epoch: 5/20... Step: 7020... Loss: 1.4393... Val Loss: 1.4516\n",
      "Epoch: 5/20... Step: 7030... Loss: 1.4961... Val Loss: 1.4514\n",
      "Epoch: 5/20... Step: 7040... Loss: 1.5252... Val Loss: 1.4524\n",
      "Epoch: 5/20... Step: 7050... Loss: 1.4755... Val Loss: 1.4490\n",
      "Epoch: 5/20... Step: 7060... Loss: 1.5015... Val Loss: 1.4531\n",
      "Epoch: 5/20... Step: 7070... Loss: 1.5733... Val Loss: 1.4496\n",
      "Epoch: 5/20... Step: 7080... Loss: 1.5680... Val Loss: 1.4517\n",
      "Epoch: 5/20... Step: 7090... Loss: 1.6923... Val Loss: 1.4513\n",
      "Epoch: 6/20... Step: 7100... Loss: 1.5545... Val Loss: 1.4494\n",
      "Epoch: 6/20... Step: 7110... Loss: 1.5246... Val Loss: 1.4454\n",
      "Epoch: 6/20... Step: 7120... Loss: 1.4504... Val Loss: 1.4453\n",
      "Epoch: 6/20... Step: 7130... Loss: 1.4282... Val Loss: 1.4468\n",
      "Epoch: 6/20... Step: 7140... Loss: 1.5112... Val Loss: 1.4421\n",
      "Epoch: 6/20... Step: 7150... Loss: 1.5278... Val Loss: 1.4430\n",
      "Epoch: 6/20... Step: 7160... Loss: 1.4965... Val Loss: 1.4479\n",
      "Epoch: 6/20... Step: 7170... Loss: 1.4293... Val Loss: 1.4438\n",
      "Epoch: 6/20... Step: 7180... Loss: 1.4579... Val Loss: 1.4462\n",
      "Epoch: 6/20... Step: 7190... Loss: 1.5217... Val Loss: 1.4442\n",
      "Epoch: 6/20... Step: 7200... Loss: 1.6000... Val Loss: 1.4476\n",
      "Epoch: 6/20... Step: 7210... Loss: 1.4577... Val Loss: 1.4436\n",
      "Epoch: 6/20... Step: 7220... Loss: 1.5924... Val Loss: 1.4427\n",
      "Epoch: 6/20... Step: 7230... Loss: 1.5155... Val Loss: 1.4446\n",
      "Epoch: 6/20... Step: 7240... Loss: 1.4255... Val Loss: 1.4457\n",
      "Epoch: 6/20... Step: 7250... Loss: 1.4714... Val Loss: 1.4436\n",
      "Epoch: 6/20... Step: 7260... Loss: 1.4072... Val Loss: 1.4460\n",
      "Epoch: 6/20... Step: 7270... Loss: 1.4625... Val Loss: 1.4444\n",
      "Epoch: 6/20... Step: 7280... Loss: 1.4991... Val Loss: 1.4453\n",
      "Epoch: 6/20... Step: 7290... Loss: 1.4594... Val Loss: 1.4451\n",
      "Epoch: 6/20... Step: 7300... Loss: 1.4602... Val Loss: 1.4440\n",
      "Epoch: 6/20... Step: 7310... Loss: 1.4957... Val Loss: 1.4467\n",
      "Epoch: 6/20... Step: 7320... Loss: 1.5421... Val Loss: 1.4438\n",
      "Epoch: 6/20... Step: 7330... Loss: 1.5083... Val Loss: 1.4436\n",
      "Epoch: 6/20... Step: 7340... Loss: 1.5145... Val Loss: 1.4454\n",
      "Epoch: 6/20... Step: 7350... Loss: 1.4927... Val Loss: 1.4417\n",
      "Epoch: 6/20... Step: 7360... Loss: 1.4927... Val Loss: 1.4432\n",
      "Epoch: 6/20... Step: 7370... Loss: 1.4714... Val Loss: 1.4420\n",
      "Epoch: 6/20... Step: 7380... Loss: 1.4669... Val Loss: 1.4437\n",
      "Epoch: 6/20... Step: 7390... Loss: 1.4802... Val Loss: 1.4454\n",
      "Epoch: 6/20... Step: 7400... Loss: 1.5145... Val Loss: 1.4427\n",
      "Epoch: 6/20... Step: 7410... Loss: 1.5023... Val Loss: 1.4417\n",
      "Epoch: 6/20... Step: 7420... Loss: 1.5303... Val Loss: 1.4399\n",
      "Epoch: 6/20... Step: 7430... Loss: 1.5079... Val Loss: 1.4424\n",
      "Epoch: 6/20... Step: 7440... Loss: 1.5331... Val Loss: 1.4411\n",
      "Epoch: 6/20... Step: 7450... Loss: 1.5133... Val Loss: 1.4403\n",
      "Epoch: 6/20... Step: 7460... Loss: 1.4773... Val Loss: 1.4402\n",
      "Epoch: 6/20... Step: 7470... Loss: 1.5432... Val Loss: 1.4374\n",
      "Epoch: 6/20... Step: 7480... Loss: 1.5258... Val Loss: 1.4400\n",
      "Epoch: 6/20... Step: 7490... Loss: 1.4950... Val Loss: 1.4381\n",
      "Epoch: 6/20... Step: 7500... Loss: 1.4597... Val Loss: 1.4374\n",
      "Epoch: 6/20... Step: 7510... Loss: 1.4519... Val Loss: 1.4415\n",
      "Epoch: 6/20... Step: 7520... Loss: 1.5900... Val Loss: 1.4385\n",
      "Epoch: 6/20... Step: 7530... Loss: 1.4968... Val Loss: 1.4374\n",
      "Epoch: 6/20... Step: 7540... Loss: 1.5068... Val Loss: 1.4392\n",
      "Epoch: 6/20... Step: 7550... Loss: 1.4893... Val Loss: 1.4387\n",
      "Epoch: 6/20... Step: 7560... Loss: 1.5276... Val Loss: 1.4380\n",
      "Epoch: 6/20... Step: 7570... Loss: 1.5191... Val Loss: 1.4382\n",
      "Epoch: 6/20... Step: 7580... Loss: 1.5110... Val Loss: 1.4403\n",
      "Epoch: 6/20... Step: 7590... Loss: 1.5199... Val Loss: 1.4390\n",
      "Epoch: 6/20... Step: 7600... Loss: 1.4497... Val Loss: 1.4362\n",
      "Epoch: 6/20... Step: 7610... Loss: 1.5034... Val Loss: 1.4411\n",
      "Epoch: 6/20... Step: 7620... Loss: 1.4956... Val Loss: 1.4347\n",
      "Epoch: 6/20... Step: 7630... Loss: 1.5160... Val Loss: 1.4338\n",
      "Epoch: 6/20... Step: 7640... Loss: 1.5383... Val Loss: 1.4400\n",
      "Epoch: 6/20... Step: 7650... Loss: 1.4903... Val Loss: 1.4357\n",
      "Epoch: 6/20... Step: 7660... Loss: 1.4825... Val Loss: 1.4366\n",
      "Epoch: 6/20... Step: 7670... Loss: 1.5072... Val Loss: 1.4371\n",
      "Epoch: 6/20... Step: 7680... Loss: 1.4326... Val Loss: 1.4360\n",
      "Epoch: 6/20... Step: 7690... Loss: 1.5149... Val Loss: 1.4382\n",
      "Epoch: 6/20... Step: 7700... Loss: 1.4433... Val Loss: 1.4375\n",
      "Epoch: 6/20... Step: 7710... Loss: 1.5923... Val Loss: 1.4321\n",
      "Epoch: 6/20... Step: 7720... Loss: 1.4561... Val Loss: 1.4347\n",
      "Epoch: 6/20... Step: 7730... Loss: 1.5234... Val Loss: 1.4357\n",
      "Epoch: 6/20... Step: 7740... Loss: 1.5442... Val Loss: 1.4345\n",
      "Epoch: 6/20... Step: 7750... Loss: 1.4688... Val Loss: 1.4378\n",
      "Epoch: 6/20... Step: 7760... Loss: 1.4884... Val Loss: 1.4338\n",
      "Epoch: 6/20... Step: 7770... Loss: 1.3612... Val Loss: 1.4368\n",
      "Epoch: 6/20... Step: 7780... Loss: 1.5408... Val Loss: 1.4342\n",
      "Epoch: 6/20... Step: 7790... Loss: 1.5159... Val Loss: 1.4357\n",
      "Epoch: 6/20... Step: 7800... Loss: 1.4402... Val Loss: 1.4323\n",
      "Epoch: 6/20... Step: 7810... Loss: 1.5078... Val Loss: 1.4306\n",
      "Epoch: 6/20... Step: 7820... Loss: 1.5001... Val Loss: 1.4310\n",
      "Epoch: 6/20... Step: 7830... Loss: 1.5534... Val Loss: 1.4308\n",
      "Epoch: 6/20... Step: 7840... Loss: 1.5054... Val Loss: 1.4311\n",
      "Epoch: 6/20... Step: 7850... Loss: 1.5699... Val Loss: 1.4332\n",
      "Epoch: 6/20... Step: 7860... Loss: 1.5153... Val Loss: 1.4315\n",
      "Epoch: 6/20... Step: 7870... Loss: 1.4192... Val Loss: 1.4330\n",
      "Epoch: 6/20... Step: 7880... Loss: 1.6008... Val Loss: 1.4323\n",
      "Epoch: 6/20... Step: 7890... Loss: 1.5475... Val Loss: 1.4348\n",
      "Epoch: 6/20... Step: 7900... Loss: 1.4632... Val Loss: 1.4332\n",
      "Epoch: 6/20... Step: 7910... Loss: 1.4978... Val Loss: 1.4334\n",
      "Epoch: 6/20... Step: 7920... Loss: 1.5507... Val Loss: 1.4350\n",
      "Epoch: 6/20... Step: 7930... Loss: 1.5137... Val Loss: 1.4333\n",
      "Epoch: 6/20... Step: 7940... Loss: 1.4261... Val Loss: 1.4280\n",
      "Epoch: 6/20... Step: 7950... Loss: 1.4708... Val Loss: 1.4291\n",
      "Epoch: 6/20... Step: 7960... Loss: 1.4820... Val Loss: 1.4312\n",
      "Epoch: 6/20... Step: 7970... Loss: 1.4129... Val Loss: 1.4318\n",
      "Epoch: 6/20... Step: 7980... Loss: 1.4111... Val Loss: 1.4283\n",
      "Epoch: 6/20... Step: 7990... Loss: 1.4227... Val Loss: 1.4310\n",
      "Epoch: 6/20... Step: 8000... Loss: 1.4731... Val Loss: 1.4306\n",
      "Epoch: 6/20... Step: 8010... Loss: 1.4415... Val Loss: 1.4281\n",
      "Epoch: 6/20... Step: 8020... Loss: 1.4173... Val Loss: 1.4366\n",
      "Epoch: 6/20... Step: 8030... Loss: 1.5244... Val Loss: 1.4314\n",
      "Epoch: 6/20... Step: 8040... Loss: 1.4614... Val Loss: 1.4303\n",
      "Epoch: 6/20... Step: 8050... Loss: 1.4623... Val Loss: 1.4283\n",
      "Epoch: 6/20... Step: 8060... Loss: 1.5317... Val Loss: 1.4284\n",
      "Epoch: 6/20... Step: 8070... Loss: 1.4544... Val Loss: 1.4280\n",
      "Epoch: 6/20... Step: 8080... Loss: 1.4366... Val Loss: 1.4280\n",
      "Epoch: 6/20... Step: 8090... Loss: 1.4770... Val Loss: 1.4313\n",
      "Epoch: 6/20... Step: 8100... Loss: 1.4696... Val Loss: 1.4280\n",
      "Epoch: 6/20... Step: 8110... Loss: 1.4603... Val Loss: 1.4288\n",
      "Epoch: 6/20... Step: 8120... Loss: 1.5377... Val Loss: 1.4282\n",
      "Epoch: 6/20... Step: 8130... Loss: 1.4591... Val Loss: 1.4323\n",
      "Epoch: 6/20... Step: 8140... Loss: 1.4791... Val Loss: 1.4266\n",
      "Epoch: 6/20... Step: 8150... Loss: 1.5556... Val Loss: 1.4257\n",
      "Epoch: 6/20... Step: 8160... Loss: 1.4530... Val Loss: 1.4308\n",
      "Epoch: 6/20... Step: 8170... Loss: 1.4928... Val Loss: 1.4254\n",
      "Epoch: 6/20... Step: 8180... Loss: 1.4893... Val Loss: 1.4252\n",
      "Epoch: 6/20... Step: 8190... Loss: 1.4025... Val Loss: 1.4288\n",
      "Epoch: 6/20... Step: 8200... Loss: 1.4910... Val Loss: 1.4227\n",
      "Epoch: 6/20... Step: 8210... Loss: 1.5018... Val Loss: 1.4233\n",
      "Epoch: 6/20... Step: 8220... Loss: 1.5058... Val Loss: 1.4280\n",
      "Epoch: 6/20... Step: 8230... Loss: 1.4296... Val Loss: 1.4239\n",
      "Epoch: 6/20... Step: 8240... Loss: 1.5338... Val Loss: 1.4228\n",
      "Epoch: 6/20... Step: 8250... Loss: 1.4718... Val Loss: 1.4265\n",
      "Epoch: 6/20... Step: 8260... Loss: 1.4464... Val Loss: 1.4226\n",
      "Epoch: 6/20... Step: 8270... Loss: 1.4255... Val Loss: 1.4249\n",
      "Epoch: 6/20... Step: 8280... Loss: 1.4869... Val Loss: 1.4230\n",
      "Epoch: 6/20... Step: 8290... Loss: 1.4412... Val Loss: 1.4190\n",
      "Epoch: 6/20... Step: 8300... Loss: 1.4644... Val Loss: 1.4230\n",
      "Epoch: 6/20... Step: 8310... Loss: 1.4486... Val Loss: 1.4222\n",
      "Epoch: 6/20... Step: 8320... Loss: 1.4306... Val Loss: 1.4206\n",
      "Epoch: 6/20... Step: 8330... Loss: 1.4867... Val Loss: 1.4209\n",
      "Epoch: 6/20... Step: 8340... Loss: 1.4612... Val Loss: 1.4162\n",
      "Epoch: 6/20... Step: 8350... Loss: 1.4517... Val Loss: 1.4220\n",
      "Epoch: 6/20... Step: 8360... Loss: 1.5148... Val Loss: 1.4182\n",
      "Epoch: 6/20... Step: 8370... Loss: 1.5712... Val Loss: 1.4238\n",
      "Epoch: 6/20... Step: 8380... Loss: 1.5178... Val Loss: 1.4201\n",
      "Epoch: 6/20... Step: 8390... Loss: 1.5410... Val Loss: 1.4192\n",
      "Epoch: 6/20... Step: 8400... Loss: 1.4532... Val Loss: 1.4212\n",
      "Epoch: 6/20... Step: 8410... Loss: 1.5330... Val Loss: 1.4187\n",
      "Epoch: 6/20... Step: 8420... Loss: 1.4198... Val Loss: 1.4218\n",
      "Epoch: 6/20... Step: 8430... Loss: 1.5025... Val Loss: 1.4209\n",
      "Epoch: 6/20... Step: 8440... Loss: 1.4744... Val Loss: 1.4166\n",
      "Epoch: 6/20... Step: 8450... Loss: 1.4627... Val Loss: 1.4186\n",
      "Epoch: 6/20... Step: 8460... Loss: 1.5308... Val Loss: 1.4182\n",
      "Epoch: 6/20... Step: 8470... Loss: 1.3759... Val Loss: 1.4205\n",
      "Epoch: 6/20... Step: 8480... Loss: 1.4960... Val Loss: 1.4190\n",
      "Epoch: 6/20... Step: 8490... Loss: 1.5204... Val Loss: 1.4179\n",
      "Epoch: 6/20... Step: 8500... Loss: 1.4692... Val Loss: 1.4185\n",
      "Epoch: 7/20... Step: 8510... Loss: 1.5122... Val Loss: 1.4211\n",
      "Epoch: 7/20... Step: 8520... Loss: 1.5853... Val Loss: 1.4182\n",
      "Epoch: 7/20... Step: 8530... Loss: 1.4913... Val Loss: 1.4154\n",
      "Epoch: 7/20... Step: 8540... Loss: 1.4338... Val Loss: 1.4158\n",
      "Epoch: 7/20... Step: 8550... Loss: 1.4033... Val Loss: 1.4161\n",
      "Epoch: 7/20... Step: 8560... Loss: 1.4258... Val Loss: 1.4113\n",
      "Epoch: 7/20... Step: 8570... Loss: 1.4955... Val Loss: 1.4171\n",
      "Epoch: 7/20... Step: 8580... Loss: 1.4342... Val Loss: 1.4140\n",
      "Epoch: 7/20... Step: 8590... Loss: 1.5583... Val Loss: 1.4119\n",
      "Epoch: 7/20... Step: 8600... Loss: 1.4297... Val Loss: 1.4182\n",
      "Epoch: 7/20... Step: 8610... Loss: 1.4656... Val Loss: 1.4128\n",
      "Epoch: 7/20... Step: 8620... Loss: 1.4389... Val Loss: 1.4232\n",
      "Epoch: 7/20... Step: 8630... Loss: 1.4211... Val Loss: 1.4123\n",
      "Epoch: 7/20... Step: 8640... Loss: 1.3931... Val Loss: 1.4130\n",
      "Epoch: 7/20... Step: 8650... Loss: 1.4393... Val Loss: 1.4162\n",
      "Epoch: 7/20... Step: 8660... Loss: 1.3718... Val Loss: 1.4150\n",
      "Epoch: 7/20... Step: 8670... Loss: 1.5018... Val Loss: 1.4149\n",
      "Epoch: 7/20... Step: 8680... Loss: 1.4066... Val Loss: 1.4142\n",
      "Epoch: 7/20... Step: 8690... Loss: 1.4899... Val Loss: 1.4178\n",
      "Epoch: 7/20... Step: 8700... Loss: 1.4558... Val Loss: 1.4180\n",
      "Epoch: 7/20... Step: 8710... Loss: 1.4291... Val Loss: 1.4158\n",
      "Epoch: 7/20... Step: 8720... Loss: 1.4849... Val Loss: 1.4152\n",
      "Epoch: 7/20... Step: 8730... Loss: 1.3758... Val Loss: 1.4179\n",
      "Epoch: 7/20... Step: 8740... Loss: 1.4376... Val Loss: 1.4145\n",
      "Epoch: 7/20... Step: 8750... Loss: 1.4824... Val Loss: 1.4179\n",
      "Epoch: 7/20... Step: 8760... Loss: 1.4439... Val Loss: 1.4170\n",
      "Epoch: 7/20... Step: 8770... Loss: 1.4314... Val Loss: 1.4126\n",
      "Epoch: 7/20... Step: 8780... Loss: 1.4253... Val Loss: 1.4196\n",
      "Epoch: 7/20... Step: 8790... Loss: 1.4323... Val Loss: 1.4176\n",
      "Epoch: 7/20... Step: 8800... Loss: 1.4293... Val Loss: 1.4153\n",
      "Epoch: 7/20... Step: 8810... Loss: 1.3812... Val Loss: 1.4183\n",
      "Epoch: 7/20... Step: 8820... Loss: 1.5137... Val Loss: 1.4152\n",
      "Epoch: 7/20... Step: 8830... Loss: 1.4238... Val Loss: 1.4179\n",
      "Epoch: 7/20... Step: 8840... Loss: 1.5078... Val Loss: 1.4168\n",
      "Epoch: 7/20... Step: 8850... Loss: 1.5598... Val Loss: 1.4161\n",
      "Epoch: 7/20... Step: 8860... Loss: 1.4724... Val Loss: 1.4134\n",
      "Epoch: 7/20... Step: 8870... Loss: 1.5155... Val Loss: 1.4156\n",
      "Epoch: 7/20... Step: 8880... Loss: 1.5019... Val Loss: 1.4119\n",
      "Epoch: 7/20... Step: 8890... Loss: 1.4903... Val Loss: 1.4105\n",
      "Epoch: 7/20... Step: 8900... Loss: 1.4598... Val Loss: 1.4129\n",
      "Epoch: 7/20... Step: 8910... Loss: 1.3902... Val Loss: 1.4096\n",
      "Epoch: 7/20... Step: 8920... Loss: 1.4122... Val Loss: 1.4100\n",
      "Epoch: 7/20... Step: 8930... Loss: 1.5257... Val Loss: 1.4172\n",
      "Epoch: 7/20... Step: 8940... Loss: 1.4968... Val Loss: 1.4123\n",
      "Epoch: 7/20... Step: 8950... Loss: 1.4573... Val Loss: 1.4173\n",
      "Epoch: 7/20... Step: 8960... Loss: 1.4862... Val Loss: 1.4104\n",
      "Epoch: 7/20... Step: 8970... Loss: 1.4081... Val Loss: 1.4113\n",
      "Epoch: 7/20... Step: 8980... Loss: 1.4499... Val Loss: 1.4130\n",
      "Epoch: 7/20... Step: 8990... Loss: 1.4372... Val Loss: 1.4121\n",
      "Epoch: 7/20... Step: 9000... Loss: 1.5218... Val Loss: 1.4161\n",
      "Epoch: 7/20... Step: 9010... Loss: 1.4471... Val Loss: 1.4103\n",
      "Epoch: 7/20... Step: 9020... Loss: 1.4474... Val Loss: 1.4090\n",
      "Epoch: 7/20... Step: 9030... Loss: 1.4903... Val Loss: 1.4124\n",
      "Epoch: 7/20... Step: 9040... Loss: 1.5035... Val Loss: 1.4080\n",
      "Epoch: 7/20... Step: 9050... Loss: 1.5931... Val Loss: 1.4088\n",
      "Epoch: 7/20... Step: 9060... Loss: 1.4136... Val Loss: 1.4091\n",
      "Epoch: 7/20... Step: 9070... Loss: 1.4766... Val Loss: 1.4086\n",
      "Epoch: 7/20... Step: 9080... Loss: 1.4754... Val Loss: 1.4084\n",
      "Epoch: 7/20... Step: 9090... Loss: 1.5398... Val Loss: 1.4085\n",
      "Epoch: 7/20... Step: 9100... Loss: 1.4054... Val Loss: 1.4140\n",
      "Epoch: 7/20... Step: 9110... Loss: 1.3941... Val Loss: 1.4089\n",
      "Epoch: 7/20... Step: 9120... Loss: 1.4786... Val Loss: 1.4145\n",
      "Epoch: 7/20... Step: 9130... Loss: 1.3812... Val Loss: 1.4070\n",
      "Epoch: 7/20... Step: 9140... Loss: 1.4176... Val Loss: 1.4113\n",
      "Epoch: 7/20... Step: 9150... Loss: 1.4410... Val Loss: 1.4085\n",
      "Epoch: 7/20... Step: 9160... Loss: 1.5019... Val Loss: 1.4077\n",
      "Epoch: 7/20... Step: 9170... Loss: 1.4754... Val Loss: 1.4094\n",
      "Epoch: 7/20... Step: 9180... Loss: 1.4366... Val Loss: 1.4103\n",
      "Epoch: 7/20... Step: 9190... Loss: 1.3762... Val Loss: 1.4088\n",
      "Epoch: 7/20... Step: 9200... Loss: 1.3879... Val Loss: 1.4099\n",
      "Epoch: 7/20... Step: 9210... Loss: 1.4584... Val Loss: 1.4089\n",
      "Epoch: 7/20... Step: 9220... Loss: 1.4134... Val Loss: 1.4085\n",
      "Epoch: 7/20... Step: 9230... Loss: 1.5399... Val Loss: 1.4057\n",
      "Epoch: 7/20... Step: 9240... Loss: 1.4061... Val Loss: 1.4069\n",
      "Epoch: 7/20... Step: 9250... Loss: 1.5079... Val Loss: 1.4070\n",
      "Epoch: 7/20... Step: 9260... Loss: 1.5028... Val Loss: 1.4100\n",
      "Epoch: 7/20... Step: 9270... Loss: 1.4710... Val Loss: 1.4049\n",
      "Epoch: 7/20... Step: 9280... Loss: 1.4843... Val Loss: 1.4065\n",
      "Epoch: 7/20... Step: 9290... Loss: 1.4969... Val Loss: 1.4073\n",
      "Epoch: 7/20... Step: 9300... Loss: 1.4655... Val Loss: 1.4078\n",
      "Epoch: 7/20... Step: 9310... Loss: 1.5206... Val Loss: 1.4102\n",
      "Epoch: 7/20... Step: 9320... Loss: 1.4178... Val Loss: 1.4074\n",
      "Epoch: 7/20... Step: 9330... Loss: 1.4239... Val Loss: 1.4094\n",
      "Epoch: 7/20... Step: 9340... Loss: 1.4244... Val Loss: 1.4077\n",
      "Epoch: 7/20... Step: 9350... Loss: 1.4640... Val Loss: 1.4065\n",
      "Epoch: 7/20... Step: 9360... Loss: 1.4088... Val Loss: 1.4062\n",
      "Epoch: 7/20... Step: 9370... Loss: 1.4296... Val Loss: 1.4051\n",
      "Epoch: 7/20... Step: 9380... Loss: 1.4579... Val Loss: 1.4092\n",
      "Epoch: 7/20... Step: 9390... Loss: 1.4361... Val Loss: 1.4080\n",
      "Epoch: 7/20... Step: 9400... Loss: 1.4460... Val Loss: 1.4060\n",
      "Epoch: 7/20... Step: 9410... Loss: 1.4988... Val Loss: 1.4091\n",
      "Epoch: 7/20... Step: 9420... Loss: 1.4813... Val Loss: 1.4099\n",
      "Epoch: 7/20... Step: 9430... Loss: 1.4512... Val Loss: 1.4046\n",
      "Epoch: 7/20... Step: 9440... Loss: 1.4202... Val Loss: 1.4130\n",
      "Epoch: 7/20... Step: 9450... Loss: 1.4287... Val Loss: 1.4056\n",
      "Epoch: 7/20... Step: 9460... Loss: 1.4228... Val Loss: 1.4058\n",
      "Epoch: 7/20... Step: 9470... Loss: 1.4323... Val Loss: 1.4106\n",
      "Epoch: 7/20... Step: 9480... Loss: 1.4097... Val Loss: 1.4047\n",
      "Epoch: 7/20... Step: 9490... Loss: 1.4874... Val Loss: 1.4062\n",
      "Epoch: 7/20... Step: 9500... Loss: 1.4480... Val Loss: 1.4061\n",
      "Epoch: 7/20... Step: 9510... Loss: 1.4413... Val Loss: 1.4063\n",
      "Epoch: 7/20... Step: 9520... Loss: 1.4024... Val Loss: 1.4080\n",
      "Epoch: 7/20... Step: 9530... Loss: 1.4573... Val Loss: 1.4044\n",
      "Epoch: 7/20... Step: 9540... Loss: 1.3903... Val Loss: 1.4067\n",
      "Epoch: 7/20... Step: 9550... Loss: 1.4409... Val Loss: 1.4061\n",
      "Epoch: 7/20... Step: 9560... Loss: 1.4566... Val Loss: 1.4044\n",
      "Epoch: 7/20... Step: 9570... Loss: 1.4240... Val Loss: 1.4033\n",
      "Epoch: 7/20... Step: 9580... Loss: 1.4636... Val Loss: 1.4047\n",
      "Epoch: 7/20... Step: 9590... Loss: 1.4363... Val Loss: 1.4006\n",
      "Epoch: 7/20... Step: 9600... Loss: 1.4084... Val Loss: 1.4014\n",
      "Epoch: 7/20... Step: 9610... Loss: 1.4486... Val Loss: 1.4018\n",
      "Epoch: 7/20... Step: 9620... Loss: 1.4558... Val Loss: 1.4003\n",
      "Epoch: 7/20... Step: 9630... Loss: 1.4021... Val Loss: 1.3999\n",
      "Epoch: 7/20... Step: 9640... Loss: 1.4864... Val Loss: 1.4030\n",
      "Epoch: 7/20... Step: 9650... Loss: 1.4155... Val Loss: 1.3986\n",
      "Epoch: 7/20... Step: 9660... Loss: 1.4501... Val Loss: 1.3998\n",
      "Epoch: 7/20... Step: 9670... Loss: 1.5016... Val Loss: 1.4001\n",
      "Epoch: 7/20... Step: 9680... Loss: 1.4174... Val Loss: 1.4008\n",
      "Epoch: 7/20... Step: 9690... Loss: 1.4329... Val Loss: 1.4015\n",
      "Epoch: 7/20... Step: 9700... Loss: 1.3991... Val Loss: 1.3993\n",
      "Epoch: 7/20... Step: 9710... Loss: 1.4009... Val Loss: 1.4016\n",
      "Epoch: 7/20... Step: 9720... Loss: 1.4034... Val Loss: 1.4018\n",
      "Epoch: 7/20... Step: 9730... Loss: 1.4932... Val Loss: 1.4051\n",
      "Epoch: 7/20... Step: 9740... Loss: 1.4483... Val Loss: 1.3960\n",
      "Epoch: 7/20... Step: 9750... Loss: 1.4639... Val Loss: 1.4021\n",
      "Epoch: 7/20... Step: 9760... Loss: 1.4135... Val Loss: 1.3983\n",
      "Epoch: 7/20... Step: 9770... Loss: 1.4333... Val Loss: 1.3993\n",
      "Epoch: 7/20... Step: 9780... Loss: 1.4493... Val Loss: 1.3970\n",
      "Epoch: 7/20... Step: 9790... Loss: 1.4310... Val Loss: 1.4008\n",
      "Epoch: 7/20... Step: 9800... Loss: 1.4835... Val Loss: 1.3970\n",
      "Epoch: 7/20... Step: 9810... Loss: 1.4643... Val Loss: 1.3987\n",
      "Epoch: 7/20... Step: 9820... Loss: 1.5497... Val Loss: 1.3980\n",
      "Epoch: 7/20... Step: 9830... Loss: 1.3813... Val Loss: 1.3957\n",
      "Epoch: 7/20... Step: 9840... Loss: 1.4520... Val Loss: 1.4013\n",
      "Epoch: 7/20... Step: 9850... Loss: 1.4422... Val Loss: 1.3954\n",
      "Epoch: 7/20... Step: 9860... Loss: 1.5133... Val Loss: 1.3965\n",
      "Epoch: 7/20... Step: 9870... Loss: 1.4288... Val Loss: 1.3978\n",
      "Epoch: 7/20... Step: 9880... Loss: 1.4172... Val Loss: 1.3994\n",
      "Epoch: 7/20... Step: 9890... Loss: 1.4376... Val Loss: 1.3964\n",
      "Epoch: 7/20... Step: 9900... Loss: 1.4457... Val Loss: 1.4013\n",
      "Epoch: 7/20... Step: 9910... Loss: 1.4738... Val Loss: 1.3931\n",
      "Epoch: 7/20... Step: 9920... Loss: 1.4333... Val Loss: 1.4040\n",
      "Epoch: 8/20... Step: 9930... Loss: 1.5216... Val Loss: 1.3964\n",
      "Epoch: 8/20... Step: 9940... Loss: 1.4380... Val Loss: 1.3942\n",
      "Epoch: 8/20... Step: 9950... Loss: 1.4176... Val Loss: 1.3939\n",
      "Epoch: 8/20... Step: 9960... Loss: 1.3738... Val Loss: 1.3919\n",
      "Epoch: 8/20... Step: 9970... Loss: 1.5222... Val Loss: 1.3961\n",
      "Epoch: 8/20... Step: 9980... Loss: 1.4742... Val Loss: 1.3912\n",
      "Epoch: 8/20... Step: 9990... Loss: 1.3814... Val Loss: 1.3943\n",
      "Epoch: 8/20... Step: 10000... Loss: 1.3975... Val Loss: 1.3946\n",
      "Epoch: 8/20... Step: 10010... Loss: 1.4235... Val Loss: 1.3906\n",
      "Epoch: 8/20... Step: 10020... Loss: 1.4140... Val Loss: 1.3975\n",
      "Epoch: 8/20... Step: 10030... Loss: 1.3752... Val Loss: 1.3922\n",
      "Epoch: 8/20... Step: 10040... Loss: 1.4660... Val Loss: 1.3961\n",
      "Epoch: 8/20... Step: 10050... Loss: 1.4764... Val Loss: 1.3923\n",
      "Epoch: 8/20... Step: 10060... Loss: 1.4221... Val Loss: 1.3911\n",
      "Epoch: 8/20... Step: 10070... Loss: 1.3860... Val Loss: 1.3929\n",
      "Epoch: 8/20... Step: 10080... Loss: 1.4152... Val Loss: 1.3908\n",
      "Epoch: 8/20... Step: 10090... Loss: 1.4626... Val Loss: 1.3920\n",
      "Epoch: 8/20... Step: 10100... Loss: 1.4214... Val Loss: 1.3932\n",
      "Epoch: 8/20... Step: 10110... Loss: 1.4240... Val Loss: 1.3913\n",
      "Epoch: 8/20... Step: 10120... Loss: 1.4197... Val Loss: 1.3982\n",
      "Epoch: 8/20... Step: 10130... Loss: 1.3775... Val Loss: 1.3935\n",
      "Epoch: 8/20... Step: 10140... Loss: 1.4406... Val Loss: 1.3985\n",
      "Epoch: 8/20... Step: 10150... Loss: 1.4561... Val Loss: 1.3946\n",
      "Epoch: 8/20... Step: 10160... Loss: 1.4137... Val Loss: 1.3959\n",
      "Epoch: 8/20... Step: 10170... Loss: 1.3795... Val Loss: 1.3980\n",
      "Epoch: 8/20... Step: 10180... Loss: 1.4681... Val Loss: 1.3957\n",
      "Epoch: 8/20... Step: 10190... Loss: 1.3942... Val Loss: 1.3954\n",
      "Epoch: 8/20... Step: 10200... Loss: 1.3767... Val Loss: 1.3974\n",
      "Epoch: 8/20... Step: 10210... Loss: 1.4433... Val Loss: 1.3975\n",
      "Epoch: 8/20... Step: 10220... Loss: 1.3999... Val Loss: 1.3973\n",
      "Epoch: 8/20... Step: 10230... Loss: 1.4261... Val Loss: 1.3960\n",
      "Epoch: 8/20... Step: 10240... Loss: 1.4903... Val Loss: 1.3939\n",
      "Epoch: 8/20... Step: 10250... Loss: 1.4492... Val Loss: 1.3956\n",
      "Epoch: 8/20... Step: 10260... Loss: 1.4346... Val Loss: 1.3931\n",
      "Epoch: 8/20... Step: 10270... Loss: 1.5127... Val Loss: 1.3921\n",
      "Epoch: 8/20... Step: 10280... Loss: 1.4728... Val Loss: 1.3933\n",
      "Epoch: 8/20... Step: 10290... Loss: 1.4213... Val Loss: 1.3905\n",
      "Epoch: 8/20... Step: 10300... Loss: 1.4731... Val Loss: 1.3917\n",
      "Epoch: 8/20... Step: 10310... Loss: 1.4449... Val Loss: 1.3911\n",
      "Epoch: 8/20... Step: 10320... Loss: 1.4665... Val Loss: 1.3914\n",
      "Epoch: 8/20... Step: 10330... Loss: 1.3710... Val Loss: 1.3952\n",
      "Epoch: 8/20... Step: 10340... Loss: 1.4140... Val Loss: 1.3915\n",
      "Epoch: 8/20... Step: 10350... Loss: 1.4024... Val Loss: 1.4002\n",
      "Epoch: 8/20... Step: 10360... Loss: 1.5232... Val Loss: 1.3909\n",
      "Epoch: 8/20... Step: 10370... Loss: 1.4152... Val Loss: 1.4007\n",
      "Epoch: 8/20... Step: 10380... Loss: 1.3873... Val Loss: 1.3914\n",
      "Epoch: 8/20... Step: 10390... Loss: 1.4690... Val Loss: 1.3924\n",
      "Epoch: 8/20... Step: 10400... Loss: 1.4037... Val Loss: 1.3942\n",
      "Epoch: 8/20... Step: 10410... Loss: 1.4035... Val Loss: 1.3914\n",
      "Epoch: 8/20... Step: 10420... Loss: 1.3706... Val Loss: 1.3921\n",
      "Epoch: 8/20... Step: 10430... Loss: 1.4153... Val Loss: 1.3929\n",
      "Epoch: 8/20... Step: 10440... Loss: 1.4579... Val Loss: 1.3898\n",
      "Epoch: 8/20... Step: 10450... Loss: 1.5699... Val Loss: 1.3915\n",
      "Epoch: 8/20... Step: 10460... Loss: 1.4470... Val Loss: 1.3895\n",
      "Epoch: 8/20... Step: 10470... Loss: 1.4074... Val Loss: 1.3902\n",
      "Epoch: 8/20... Step: 10480... Loss: 1.4377... Val Loss: 1.3933\n",
      "Epoch: 8/20... Step: 10490... Loss: 1.5438... Val Loss: 1.3886\n",
      "Epoch: 8/20... Step: 10500... Loss: 1.4745... Val Loss: 1.3917\n",
      "Epoch: 8/20... Step: 10510... Loss: 1.5372... Val Loss: 1.3897\n",
      "Epoch: 8/20... Step: 10520... Loss: 1.5219... Val Loss: 1.3973\n",
      "Epoch: 8/20... Step: 10530... Loss: 1.4421... Val Loss: 1.3910\n",
      "Epoch: 8/20... Step: 10540... Loss: 1.4441... Val Loss: 1.3926\n",
      "Epoch: 8/20... Step: 10550... Loss: 1.5051... Val Loss: 1.3869\n",
      "Epoch: 8/20... Step: 10560... Loss: 1.4295... Val Loss: 1.3892\n",
      "Epoch: 8/20... Step: 10570... Loss: 1.4517... Val Loss: 1.3903\n",
      "Epoch: 8/20... Step: 10580... Loss: 1.4394... Val Loss: 1.3887\n",
      "Epoch: 8/20... Step: 10590... Loss: 1.4656... Val Loss: 1.3917\n",
      "Epoch: 8/20... Step: 10600... Loss: 1.4264... Val Loss: 1.3909\n",
      "Epoch: 8/20... Step: 10610... Loss: 1.3729... Val Loss: 1.3899\n",
      "Epoch: 8/20... Step: 10620... Loss: 1.3673... Val Loss: 1.3948\n",
      "Epoch: 8/20... Step: 10630... Loss: 1.3735... Val Loss: 1.3918\n",
      "Epoch: 8/20... Step: 10640... Loss: 1.4048... Val Loss: 1.3863\n",
      "Epoch: 8/20... Step: 10650... Loss: 1.4833... Val Loss: 1.3883\n",
      "Epoch: 8/20... Step: 10660... Loss: 1.4094... Val Loss: 1.3865\n",
      "Epoch: 8/20... Step: 10670... Loss: 1.4748... Val Loss: 1.3899\n",
      "Epoch: 8/20... Step: 10680... Loss: 1.4414... Val Loss: 1.3884\n",
      "Epoch: 8/20... Step: 10690... Loss: 1.3772... Val Loss: 1.3884\n",
      "Epoch: 8/20... Step: 10700... Loss: 1.5063... Val Loss: 1.3867\n",
      "Epoch: 8/20... Step: 10710... Loss: 1.4934... Val Loss: 1.3884\n",
      "Epoch: 8/20... Step: 10720... Loss: 1.4515... Val Loss: 1.3873\n",
      "Epoch: 8/20... Step: 10730... Loss: 1.4963... Val Loss: 1.3884\n",
      "Epoch: 8/20... Step: 10740... Loss: 1.5042... Val Loss: 1.3893\n",
      "Epoch: 8/20... Step: 10750... Loss: 1.4153... Val Loss: 1.3871\n",
      "Epoch: 8/20... Step: 10760... Loss: 1.5015... Val Loss: 1.3916\n",
      "Epoch: 8/20... Step: 10770... Loss: 1.3860... Val Loss: 1.3916\n",
      "Epoch: 8/20... Step: 10780... Loss: 1.4654... Val Loss: 1.3862\n",
      "Epoch: 8/20... Step: 10790... Loss: 1.4330... Val Loss: 1.3858\n",
      "Epoch: 8/20... Step: 10800... Loss: 1.4571... Val Loss: 1.3921\n",
      "Epoch: 8/20... Step: 10810... Loss: 1.4033... Val Loss: 1.3878\n",
      "Epoch: 8/20... Step: 10820... Loss: 1.4226... Val Loss: 1.3886\n",
      "Epoch: 8/20... Step: 10830... Loss: 1.4638... Val Loss: 1.3874\n",
      "Epoch: 8/20... Step: 10840... Loss: 1.3722... Val Loss: 1.3908\n",
      "Epoch: 8/20... Step: 10850... Loss: 1.3563... Val Loss: 1.3846\n",
      "Epoch: 8/20... Step: 10860... Loss: 1.4180... Val Loss: 1.3917\n",
      "Epoch: 8/20... Step: 10870... Loss: 1.4790... Val Loss: 1.3906\n",
      "Epoch: 8/20... Step: 10880... Loss: 1.4935... Val Loss: 1.3905\n",
      "Epoch: 8/20... Step: 10890... Loss: 1.4599... Val Loss: 1.3874\n",
      "Epoch: 8/20... Step: 10900... Loss: 1.4415... Val Loss: 1.3865\n",
      "Epoch: 8/20... Step: 10910... Loss: 1.4025... Val Loss: 1.3884\n",
      "Epoch: 8/20... Step: 10920... Loss: 1.3773... Val Loss: 1.3882\n",
      "Epoch: 8/20... Step: 10930... Loss: 1.4427... Val Loss: 1.3898\n",
      "Epoch: 8/20... Step: 10940... Loss: 1.3500... Val Loss: 1.3861\n",
      "Epoch: 8/20... Step: 10950... Loss: 1.4481... Val Loss: 1.3877\n",
      "Epoch: 8/20... Step: 10960... Loss: 1.4122... Val Loss: 1.3871\n",
      "Epoch: 8/20... Step: 10970... Loss: 1.3979... Val Loss: 1.3872\n",
      "Epoch: 8/20... Step: 10980... Loss: 1.3890... Val Loss: 1.3878\n",
      "Epoch: 8/20... Step: 10990... Loss: 1.4538... Val Loss: 1.3865\n",
      "Epoch: 8/20... Step: 11000... Loss: 1.4736... Val Loss: 1.3901\n",
      "Epoch: 8/20... Step: 11010... Loss: 1.4233... Val Loss: 1.3849\n",
      "Epoch: 8/20... Step: 11020... Loss: 1.4205... Val Loss: 1.3871\n",
      "Epoch: 8/20... Step: 11030... Loss: 1.4406... Val Loss: 1.3860\n",
      "Epoch: 8/20... Step: 11040... Loss: 1.3791... Val Loss: 1.3834\n",
      "Epoch: 8/20... Step: 11050... Loss: 1.4294... Val Loss: 1.3820\n",
      "Epoch: 8/20... Step: 11060... Loss: 1.4106... Val Loss: 1.3827\n",
      "Epoch: 8/20... Step: 11070... Loss: 1.4248... Val Loss: 1.3827\n",
      "Epoch: 8/20... Step: 11080... Loss: 1.4586... Val Loss: 1.3802\n",
      "Epoch: 8/20... Step: 11090... Loss: 1.3787... Val Loss: 1.3895\n",
      "Epoch: 8/20... Step: 11100... Loss: 1.4256... Val Loss: 1.3831\n",
      "Epoch: 8/20... Step: 11110... Loss: 1.3673... Val Loss: 1.3814\n",
      "Epoch: 8/20... Step: 11120... Loss: 1.4736... Val Loss: 1.3816\n",
      "Epoch: 8/20... Step: 11130... Loss: 1.3755... Val Loss: 1.3826\n",
      "Epoch: 8/20... Step: 11140... Loss: 1.3873... Val Loss: 1.3873\n",
      "Epoch: 8/20... Step: 11150... Loss: 1.3486... Val Loss: 1.3834\n",
      "Epoch: 8/20... Step: 11160... Loss: 1.4947... Val Loss: 1.3836\n",
      "Epoch: 8/20... Step: 11170... Loss: 1.4111... Val Loss: 1.3824\n",
      "Epoch: 8/20... Step: 11180... Loss: 1.4566... Val Loss: 1.3800\n",
      "Epoch: 8/20... Step: 11190... Loss: 1.4509... Val Loss: 1.3814\n",
      "Epoch: 8/20... Step: 11200... Loss: 1.4963... Val Loss: 1.3832\n",
      "Epoch: 8/20... Step: 11210... Loss: 1.4938... Val Loss: 1.3835\n",
      "Epoch: 8/20... Step: 11220... Loss: 1.4351... Val Loss: 1.3834\n",
      "Epoch: 8/20... Step: 11230... Loss: 1.4534... Val Loss: 1.3824\n",
      "Epoch: 8/20... Step: 11240... Loss: 1.4533... Val Loss: 1.3817\n",
      "Epoch: 8/20... Step: 11250... Loss: 1.3981... Val Loss: 1.3817\n",
      "Epoch: 8/20... Step: 11260... Loss: 1.4677... Val Loss: 1.3808\n",
      "Epoch: 8/20... Step: 11270... Loss: 1.4371... Val Loss: 1.3822\n",
      "Epoch: 8/20... Step: 11280... Loss: 1.4069... Val Loss: 1.3811\n",
      "Epoch: 8/20... Step: 11290... Loss: 1.4072... Val Loss: 1.3797\n",
      "Epoch: 8/20... Step: 11300... Loss: 1.5013... Val Loss: 1.3865\n",
      "Epoch: 8/20... Step: 11310... Loss: 1.4042... Val Loss: 1.3807\n",
      "Epoch: 8/20... Step: 11320... Loss: 1.3914... Val Loss: 1.3860\n",
      "Epoch: 8/20... Step: 11330... Loss: 1.4248... Val Loss: 1.3785\n",
      "Epoch: 8/20... Step: 11340... Loss: 1.3882... Val Loss: 1.3808\n",
      "Epoch: 9/20... Step: 11350... Loss: 1.4327... Val Loss: 1.3788\n",
      "Epoch: 9/20... Step: 11360... Loss: 1.4018... Val Loss: 1.3753\n",
      "Epoch: 9/20... Step: 11370... Loss: 1.4377... Val Loss: 1.3764\n",
      "Epoch: 9/20... Step: 11380... Loss: 1.3180... Val Loss: 1.3761\n",
      "Epoch: 9/20... Step: 11390... Loss: 1.3860... Val Loss: 1.3769\n",
      "Epoch: 9/20... Step: 11400... Loss: 1.4300... Val Loss: 1.3772\n",
      "Epoch: 9/20... Step: 11410... Loss: 1.4101... Val Loss: 1.3753\n",
      "Epoch: 9/20... Step: 11420... Loss: 1.4761... Val Loss: 1.3754\n",
      "Epoch: 9/20... Step: 11430... Loss: 1.3831... Val Loss: 1.3748\n",
      "Epoch: 9/20... Step: 11440... Loss: 1.4941... Val Loss: 1.3793\n",
      "Epoch: 9/20... Step: 11450... Loss: 1.3778... Val Loss: 1.3803\n",
      "Epoch: 9/20... Step: 11460... Loss: 1.3850... Val Loss: 1.3803\n",
      "Epoch: 9/20... Step: 11470... Loss: 1.4741... Val Loss: 1.3744\n",
      "Epoch: 9/20... Step: 11480... Loss: 1.3662... Val Loss: 1.3782\n",
      "Epoch: 9/20... Step: 11490... Loss: 1.4467... Val Loss: 1.3796\n",
      "Epoch: 9/20... Step: 11500... Loss: 1.4046... Val Loss: 1.3771\n",
      "Epoch: 9/20... Step: 11510... Loss: 1.3763... Val Loss: 1.3736\n",
      "Epoch: 9/20... Step: 11520... Loss: 1.3989... Val Loss: 1.3788\n",
      "Epoch: 9/20... Step: 11530... Loss: 1.4077... Val Loss: 1.3805\n",
      "Epoch: 9/20... Step: 11540... Loss: 1.3652... Val Loss: 1.3784\n",
      "Epoch: 9/20... Step: 11550... Loss: 1.4022... Val Loss: 1.3802\n",
      "Epoch: 9/20... Step: 11560... Loss: 1.4226... Val Loss: 1.3823\n",
      "Epoch: 9/20... Step: 11570... Loss: 1.4811... Val Loss: 1.3799\n",
      "Epoch: 9/20... Step: 11580... Loss: 1.3392... Val Loss: 1.3810\n",
      "Epoch: 9/20... Step: 11590... Loss: 1.4054... Val Loss: 1.3828\n",
      "Epoch: 9/20... Step: 11600... Loss: 1.3798... Val Loss: 1.3802\n",
      "Epoch: 9/20... Step: 11610... Loss: 1.4296... Val Loss: 1.3774\n",
      "Epoch: 9/20... Step: 11620... Loss: 1.4256... Val Loss: 1.3784\n",
      "Epoch: 9/20... Step: 11630... Loss: 1.4709... Val Loss: 1.3805\n",
      "Epoch: 9/20... Step: 11640... Loss: 1.4955... Val Loss: 1.3802\n",
      "Epoch: 9/20... Step: 11650... Loss: 1.3932... Val Loss: 1.3871\n",
      "Epoch: 9/20... Step: 11660... Loss: 1.3884... Val Loss: 1.3768\n",
      "Epoch: 9/20... Step: 11670... Loss: 1.4200... Val Loss: 1.3833\n",
      "Epoch: 9/20... Step: 11680... Loss: 1.3990... Val Loss: 1.3797\n",
      "Epoch: 9/20... Step: 11690... Loss: 1.3898... Val Loss: 1.3780\n",
      "Epoch: 9/20... Step: 11700... Loss: 1.4169... Val Loss: 1.3775\n",
      "Epoch: 9/20... Step: 11710... Loss: 1.3922... Val Loss: 1.3779\n",
      "Epoch: 9/20... Step: 11720... Loss: 1.4724... Val Loss: 1.3783\n",
      "Epoch: 9/20... Step: 11730... Loss: 1.4350... Val Loss: 1.3769\n",
      "Epoch: 9/20... Step: 11740... Loss: 1.4471... Val Loss: 1.3774\n",
      "Epoch: 9/20... Step: 11750... Loss: 1.3000... Val Loss: 1.3784\n",
      "Epoch: 9/20... Step: 11760... Loss: 1.3507... Val Loss: 1.3788\n",
      "Epoch: 9/20... Step: 11770... Loss: 1.4642... Val Loss: 1.3828\n",
      "Epoch: 9/20... Step: 11780... Loss: 1.4353... Val Loss: 1.3776\n",
      "Epoch: 9/20... Step: 11790... Loss: 1.4087... Val Loss: 1.3814\n",
      "Epoch: 9/20... Step: 11800... Loss: 1.3901... Val Loss: 1.3808\n",
      "Epoch: 9/20... Step: 11810... Loss: 1.4363... Val Loss: 1.3758\n",
      "Epoch: 9/20... Step: 11820... Loss: 1.3943... Val Loss: 1.3815\n",
      "Epoch: 9/20... Step: 11830... Loss: 1.4408... Val Loss: 1.3792\n",
      "Epoch: 9/20... Step: 11840... Loss: 1.3874... Val Loss: 1.3780\n",
      "Epoch: 9/20... Step: 11850... Loss: 1.4096... Val Loss: 1.3798\n",
      "Epoch: 9/20... Step: 11860... Loss: 1.4106... Val Loss: 1.3740\n",
      "Epoch: 9/20... Step: 11870... Loss: 1.4330... Val Loss: 1.3798\n",
      "Epoch: 9/20... Step: 11880... Loss: 1.3882... Val Loss: 1.3762\n",
      "Epoch: 9/20... Step: 11890... Loss: 1.3604... Val Loss: 1.3770\n",
      "Epoch: 9/20... Step: 11900... Loss: 1.3980... Val Loss: 1.3815\n",
      "Epoch: 9/20... Step: 11910... Loss: 1.4270... Val Loss: 1.3775\n",
      "Epoch: 9/20... Step: 11920... Loss: 1.3863... Val Loss: 1.3784\n",
      "Epoch: 9/20... Step: 11930... Loss: 1.4112... Val Loss: 1.3774\n",
      "Epoch: 9/20... Step: 11940... Loss: 1.3691... Val Loss: 1.3811\n",
      "Epoch: 9/20... Step: 11950... Loss: 1.4358... Val Loss: 1.3761\n",
      "Epoch: 9/20... Step: 11960... Loss: 1.4364... Val Loss: 1.3746\n",
      "Epoch: 9/20... Step: 11970... Loss: 1.4181... Val Loss: 1.3758\n",
      "Epoch: 9/20... Step: 11980... Loss: 1.4284... Val Loss: 1.3766\n",
      "Epoch: 9/20... Step: 11990... Loss: 1.4556... Val Loss: 1.3753\n",
      "Epoch: 9/20... Step: 12000... Loss: 1.3417... Val Loss: 1.3780\n",
      "Epoch: 9/20... Step: 12010... Loss: 1.4510... Val Loss: 1.3774\n",
      "Epoch: 9/20... Step: 12020... Loss: 1.4183... Val Loss: 1.3772\n",
      "Epoch: 9/20... Step: 12030... Loss: 1.4095... Val Loss: 1.3747\n",
      "Epoch: 9/20... Step: 12040... Loss: 1.3508... Val Loss: 1.3819\n",
      "Epoch: 9/20... Step: 12050... Loss: 1.3638... Val Loss: 1.3757\n",
      "Epoch: 9/20... Step: 12060... Loss: 1.4042... Val Loss: 1.3733\n",
      "Epoch: 9/20... Step: 12070... Loss: 1.4362... Val Loss: 1.3748\n",
      "Epoch: 9/20... Step: 12080... Loss: 1.4578... Val Loss: 1.3741\n",
      "Epoch: 9/20... Step: 12090... Loss: 1.4734... Val Loss: 1.3738\n",
      "Epoch: 9/20... Step: 12100... Loss: 1.4238... Val Loss: 1.3764\n",
      "Epoch: 9/20... Step: 12110... Loss: 1.3540... Val Loss: 1.3766\n",
      "Epoch: 9/20... Step: 12120... Loss: 1.4308... Val Loss: 1.3741\n",
      "Epoch: 9/20... Step: 12130... Loss: 1.4252... Val Loss: 1.3775\n",
      "Epoch: 9/20... Step: 12140... Loss: 1.4066... Val Loss: 1.3767\n",
      "Epoch: 9/20... Step: 12150... Loss: 1.4224... Val Loss: 1.3753\n",
      "Epoch: 9/20... Step: 12160... Loss: 1.4047... Val Loss: 1.3773\n",
      "Epoch: 9/20... Step: 12170... Loss: 1.3450... Val Loss: 1.3747\n",
      "Epoch: 9/20... Step: 12180... Loss: 1.4425... Val Loss: 1.3751\n",
      "Epoch: 9/20... Step: 12190... Loss: 1.3808... Val Loss: 1.3776\n",
      "Epoch: 9/20... Step: 12200... Loss: 1.3787... Val Loss: 1.3744\n",
      "Epoch: 9/20... Step: 12210... Loss: 1.5189... Val Loss: 1.3764\n",
      "Epoch: 9/20... Step: 12220... Loss: 1.3417... Val Loss: 1.3774\n",
      "Epoch: 9/20... Step: 12230... Loss: 1.3968... Val Loss: 1.3759\n",
      "Epoch: 9/20... Step: 12240... Loss: 1.3673... Val Loss: 1.3739\n",
      "Epoch: 9/20... Step: 12250... Loss: 1.4684... Val Loss: 1.3735\n",
      "Epoch: 9/20... Step: 12260... Loss: 1.4194... Val Loss: 1.3722\n",
      "Epoch: 9/20... Step: 12270... Loss: 1.3954... Val Loss: 1.3785\n",
      "Epoch: 9/20... Step: 12280... Loss: 1.3817... Val Loss: 1.3754\n",
      "Epoch: 9/20... Step: 12290... Loss: 1.4057... Val Loss: 1.3748\n",
      "Epoch: 9/20... Step: 12300... Loss: 1.3886... Val Loss: 1.3768\n",
      "Epoch: 9/20... Step: 12310... Loss: 1.4090... Val Loss: 1.3735\n",
      "Epoch: 9/20... Step: 12320... Loss: 1.3546... Val Loss: 1.3723\n",
      "Epoch: 9/20... Step: 12330... Loss: 1.4042... Val Loss: 1.3761\n",
      "Epoch: 9/20... Step: 12340... Loss: 1.3955... Val Loss: 1.3740\n",
      "Epoch: 9/20... Step: 12350... Loss: 1.3840... Val Loss: 1.3740\n",
      "Epoch: 9/20... Step: 12360... Loss: 1.3625... Val Loss: 1.3756\n",
      "Epoch: 9/20... Step: 12370... Loss: 1.3806... Val Loss: 1.3748\n",
      "Epoch: 9/20... Step: 12380... Loss: 1.4199... Val Loss: 1.3737\n",
      "Epoch: 9/20... Step: 12390... Loss: 1.3937... Val Loss: 1.3757\n",
      "Epoch: 9/20... Step: 12400... Loss: 1.3401... Val Loss: 1.3749\n",
      "Epoch: 9/20... Step: 12410... Loss: 1.4140... Val Loss: 1.3717\n",
      "Epoch: 9/20... Step: 12420... Loss: 1.4409... Val Loss: 1.3755\n",
      "Epoch: 9/20... Step: 12430... Loss: 1.3928... Val Loss: 1.3709\n",
      "Epoch: 9/20... Step: 12440... Loss: 1.4853... Val Loss: 1.3708\n",
      "Epoch: 9/20... Step: 12450... Loss: 1.4315... Val Loss: 1.3692\n",
      "Epoch: 9/20... Step: 12460... Loss: 1.4106... Val Loss: 1.3696\n",
      "Epoch: 9/20... Step: 12470... Loss: 1.3429... Val Loss: 1.3699\n",
      "Epoch: 9/20... Step: 12480... Loss: 1.4321... Val Loss: 1.3673\n",
      "Epoch: 9/20... Step: 12490... Loss: 1.4526... Val Loss: 1.3682\n",
      "Epoch: 9/20... Step: 12500... Loss: 1.3617... Val Loss: 1.3678\n",
      "Epoch: 9/20... Step: 12510... Loss: 1.3753... Val Loss: 1.3693\n",
      "Epoch: 9/20... Step: 12520... Loss: 1.2930... Val Loss: 1.3703\n",
      "Epoch: 9/20... Step: 12530... Loss: 1.3652... Val Loss: 1.3686\n",
      "Epoch: 9/20... Step: 12540... Loss: 1.3842... Val Loss: 1.3718\n",
      "Epoch: 9/20... Step: 12550... Loss: 1.4240... Val Loss: 1.3719\n",
      "Epoch: 9/20... Step: 12560... Loss: 1.3460... Val Loss: 1.3725\n",
      "Epoch: 9/20... Step: 12570... Loss: 1.3474... Val Loss: 1.3674\n",
      "Epoch: 9/20... Step: 12580... Loss: 1.3657... Val Loss: 1.3662\n",
      "Epoch: 9/20... Step: 12590... Loss: 1.4053... Val Loss: 1.3700\n",
      "Epoch: 9/20... Step: 12600... Loss: 1.4194... Val Loss: 1.3695\n",
      "Epoch: 9/20... Step: 12610... Loss: 1.4176... Val Loss: 1.3665\n",
      "Epoch: 9/20... Step: 12620... Loss: 1.3719... Val Loss: 1.3729\n",
      "Epoch: 9/20... Step: 12630... Loss: 1.3813... Val Loss: 1.3677\n",
      "Epoch: 9/20... Step: 12640... Loss: 1.3199... Val Loss: 1.3660\n",
      "Epoch: 9/20... Step: 12650... Loss: 1.3723... Val Loss: 1.3717\n",
      "Epoch: 9/20... Step: 12660... Loss: 1.4527... Val Loss: 1.3688\n",
      "Epoch: 9/20... Step: 12670... Loss: 1.4586... Val Loss: 1.3684\n",
      "Epoch: 9/20... Step: 12680... Loss: 1.4604... Val Loss: 1.3685\n",
      "Epoch: 9/20... Step: 12690... Loss: 1.4212... Val Loss: 1.3705\n",
      "Epoch: 9/20... Step: 12700... Loss: 1.4410... Val Loss: 1.3647\n",
      "Epoch: 9/20... Step: 12710... Loss: 1.4672... Val Loss: 1.3678\n",
      "Epoch: 9/20... Step: 12720... Loss: 1.4136... Val Loss: 1.3704\n",
      "Epoch: 9/20... Step: 12730... Loss: 1.4867... Val Loss: 1.3667\n",
      "Epoch: 9/20... Step: 12740... Loss: 1.3835... Val Loss: 1.3719\n",
      "Epoch: 9/20... Step: 12750... Loss: 1.4319... Val Loss: 1.3665\n",
      "Epoch: 9/20... Step: 12760... Loss: 1.4463... Val Loss: 1.3652\n",
      "Epoch: 10/20... Step: 12770... Loss: 1.4167... Val Loss: 1.3672\n",
      "Epoch: 10/20... Step: 12780... Loss: 1.4522... Val Loss: 1.3622\n",
      "Epoch: 10/20... Step: 12790... Loss: 1.3569... Val Loss: 1.3629\n",
      "Epoch: 10/20... Step: 12800... Loss: 1.4159... Val Loss: 1.3645\n",
      "Epoch: 10/20... Step: 12810... Loss: 1.3635... Val Loss: 1.3642\n",
      "Epoch: 10/20... Step: 12820... Loss: 1.4472... Val Loss: 1.3621\n",
      "Epoch: 10/20... Step: 12830... Loss: 1.2804... Val Loss: 1.3624\n",
      "Epoch: 10/20... Step: 12840... Loss: 1.3992... Val Loss: 1.3621\n",
      "Epoch: 10/20... Step: 12850... Loss: 1.3537... Val Loss: 1.3628\n",
      "Epoch: 10/20... Step: 12860... Loss: 1.4194... Val Loss: 1.3641\n",
      "Epoch: 10/20... Step: 12870... Loss: 1.3787... Val Loss: 1.3683\n",
      "Epoch: 10/20... Step: 12880... Loss: 1.3885... Val Loss: 1.3656\n",
      "Epoch: 10/20... Step: 12890... Loss: 1.4073... Val Loss: 1.3617\n",
      "Epoch: 10/20... Step: 12900... Loss: 1.4150... Val Loss: 1.3638\n",
      "Epoch: 10/20... Step: 12910... Loss: 1.4430... Val Loss: 1.3639\n",
      "Epoch: 10/20... Step: 12920... Loss: 1.3536... Val Loss: 1.3663\n",
      "Epoch: 10/20... Step: 12930... Loss: 1.4064... Val Loss: 1.3607\n",
      "Epoch: 10/20... Step: 12940... Loss: 1.4375... Val Loss: 1.3706\n",
      "Epoch: 10/20... Step: 12950... Loss: 1.4113... Val Loss: 1.3659\n",
      "Epoch: 10/20... Step: 12960... Loss: 1.4271... Val Loss: 1.3652\n",
      "Epoch: 10/20... Step: 12970... Loss: 1.3564... Val Loss: 1.3687\n",
      "Epoch: 10/20... Step: 12980... Loss: 1.4394... Val Loss: 1.3673\n",
      "Epoch: 10/20... Step: 12990... Loss: 1.3908... Val Loss: 1.3653\n",
      "Epoch: 10/20... Step: 13000... Loss: 1.3031... Val Loss: 1.3676\n",
      "Epoch: 10/20... Step: 13010... Loss: 1.4208... Val Loss: 1.3691\n",
      "Epoch: 10/20... Step: 13020... Loss: 1.4094... Val Loss: 1.3634\n",
      "Epoch: 10/20... Step: 13030... Loss: 1.3970... Val Loss: 1.3658\n",
      "Epoch: 10/20... Step: 13040... Loss: 1.4434... Val Loss: 1.3700\n",
      "Epoch: 10/20... Step: 13050... Loss: 1.4196... Val Loss: 1.3656\n",
      "Epoch: 10/20... Step: 13060... Loss: 1.4011... Val Loss: 1.3691\n",
      "Epoch: 10/20... Step: 13070... Loss: 1.3367... Val Loss: 1.3692\n",
      "Epoch: 10/20... Step: 13080... Loss: 1.4078... Val Loss: 1.3648\n",
      "Epoch: 10/20... Step: 13090... Loss: 1.4370... Val Loss: 1.3670\n",
      "Epoch: 10/20... Step: 13100... Loss: 1.4639... Val Loss: 1.3668\n",
      "Epoch: 10/20... Step: 13110... Loss: 1.3501... Val Loss: 1.3655\n",
      "Epoch: 10/20... Step: 13120... Loss: 1.4483... Val Loss: 1.3636\n",
      "Epoch: 10/20... Step: 13130... Loss: 1.4280... Val Loss: 1.3647\n",
      "Epoch: 10/20... Step: 13140... Loss: 1.4167... Val Loss: 1.3600\n",
      "Epoch: 10/20... Step: 13150... Loss: 1.4247... Val Loss: 1.3637\n",
      "Epoch: 10/20... Step: 13160... Loss: 1.4370... Val Loss: 1.3615\n",
      "Epoch: 10/20... Step: 13170... Loss: 1.3881... Val Loss: 1.3647\n",
      "Epoch: 10/20... Step: 13180... Loss: 1.3264... Val Loss: 1.3619\n",
      "Epoch: 10/20... Step: 13190... Loss: 1.4095... Val Loss: 1.3625\n",
      "Epoch: 10/20... Step: 13200... Loss: 1.4302... Val Loss: 1.3617\n",
      "Epoch: 10/20... Step: 13210... Loss: 1.3830... Val Loss: 1.3638\n",
      "Epoch: 10/20... Step: 13220... Loss: 1.4136... Val Loss: 1.3652\n",
      "Epoch: 10/20... Step: 13230... Loss: 1.3930... Val Loss: 1.3635\n",
      "Epoch: 10/20... Step: 13240... Loss: 1.2921... Val Loss: 1.3667\n",
      "Epoch: 10/20... Step: 13250... Loss: 1.4119... Val Loss: 1.3664\n",
      "Epoch: 10/20... Step: 13260... Loss: 1.4351... Val Loss: 1.3620\n",
      "Epoch: 10/20... Step: 13270... Loss: 1.3943... Val Loss: 1.3623\n",
      "Epoch: 10/20... Step: 13280... Loss: 1.3929... Val Loss: 1.3647\n",
      "Epoch: 10/20... Step: 13290... Loss: 1.3453... Val Loss: 1.3606\n",
      "Epoch: 10/20... Step: 13300... Loss: 1.4426... Val Loss: 1.3609\n",
      "Epoch: 10/20... Step: 13310... Loss: 1.3550... Val Loss: 1.3667\n",
      "Epoch: 10/20... Step: 13320... Loss: 1.3622... Val Loss: 1.3656\n",
      "Epoch: 10/20... Step: 13330... Loss: 1.4288... Val Loss: 1.3642\n",
      "Epoch: 10/20... Step: 13340... Loss: 1.3617... Val Loss: 1.3650\n",
      "Epoch: 10/20... Step: 13350... Loss: 1.3903... Val Loss: 1.3645\n",
      "Epoch: 10/20... Step: 13360... Loss: 1.3785... Val Loss: 1.3678\n",
      "Epoch: 10/20... Step: 13370... Loss: 1.3877... Val Loss: 1.3641\n",
      "Epoch: 10/20... Step: 13380... Loss: 1.4544... Val Loss: 1.3647\n",
      "Epoch: 10/20... Step: 13390... Loss: 1.4186... Val Loss: 1.3616\n",
      "Epoch: 10/20... Step: 13400... Loss: 1.4494... Val Loss: 1.3653\n",
      "Epoch: 10/20... Step: 13410... Loss: 1.4283... Val Loss: 1.3629\n",
      "Epoch: 10/20... Step: 13420... Loss: 1.4004... Val Loss: 1.3674\n",
      "Epoch: 10/20... Step: 13430... Loss: 1.4335... Val Loss: 1.3659\n",
      "Epoch: 10/20... Step: 13440... Loss: 1.3879... Val Loss: 1.3649\n",
      "Epoch: 10/20... Step: 13450... Loss: 1.3660... Val Loss: 1.3635\n",
      "Epoch: 10/20... Step: 13460... Loss: 1.3545... Val Loss: 1.3680\n",
      "Epoch: 10/20... Step: 13470... Loss: 1.4108... Val Loss: 1.3649\n",
      "Epoch: 10/20... Step: 13480... Loss: 1.3834... Val Loss: 1.3610\n",
      "Epoch: 10/20... Step: 13490... Loss: 1.4162... Val Loss: 1.3644\n",
      "Epoch: 10/20... Step: 13500... Loss: 1.3595... Val Loss: 1.3642\n",
      "Epoch: 10/20... Step: 13510... Loss: 1.4216... Val Loss: 1.3594\n",
      "Epoch: 10/20... Step: 13520... Loss: 1.4427... Val Loss: 1.3643\n",
      "Epoch: 10/20... Step: 13530... Loss: 1.4716... Val Loss: 1.3616\n",
      "Epoch: 10/20... Step: 13540... Loss: 1.4427... Val Loss: 1.3625\n",
      "Epoch: 10/20... Step: 13550... Loss: 1.4676... Val Loss: 1.3670\n",
      "Epoch: 10/20... Step: 13560... Loss: 1.4321... Val Loss: 1.3620\n",
      "Epoch: 10/20... Step: 13570... Loss: 1.4140... Val Loss: 1.3626\n",
      "Epoch: 10/20... Step: 13580... Loss: 1.3813... Val Loss: 1.3664\n",
      "Epoch: 10/20... Step: 13590... Loss: 1.3478... Val Loss: 1.3629\n",
      "Epoch: 10/20... Step: 13600... Loss: 1.4720... Val Loss: 1.3626\n",
      "Epoch: 10/20... Step: 13610... Loss: 1.4385... Val Loss: 1.3652\n",
      "Epoch: 10/20... Step: 13620... Loss: 1.3857... Val Loss: 1.3598\n",
      "Epoch: 10/20... Step: 13630... Loss: 1.4194... Val Loss: 1.3594\n",
      "Epoch: 10/20... Step: 13640... Loss: 1.3484... Val Loss: 1.3644\n",
      "Epoch: 10/20... Step: 13650... Loss: 1.3458... Val Loss: 1.3644\n",
      "Epoch: 10/20... Step: 13660... Loss: 1.3953... Val Loss: 1.3638\n",
      "Epoch: 10/20... Step: 13670... Loss: 1.4001... Val Loss: 1.3616\n",
      "Epoch: 10/20... Step: 13680... Loss: 1.3764... Val Loss: 1.3631\n",
      "Epoch: 10/20... Step: 13690... Loss: 1.3548... Val Loss: 1.3653\n",
      "Epoch: 10/20... Step: 13700... Loss: 1.3673... Val Loss: 1.3638\n",
      "Epoch: 10/20... Step: 13710... Loss: 1.4203... Val Loss: 1.3631\n",
      "Epoch: 10/20... Step: 13720... Loss: 1.3690... Val Loss: 1.3663\n",
      "Epoch: 10/20... Step: 13730... Loss: 1.4050... Val Loss: 1.3646\n",
      "Epoch: 10/20... Step: 13740... Loss: 1.4102... Val Loss: 1.3625\n",
      "Epoch: 10/20... Step: 13750... Loss: 1.4429... Val Loss: 1.3640\n",
      "Epoch: 10/20... Step: 13760... Loss: 1.3192... Val Loss: 1.3659\n",
      "Epoch: 10/20... Step: 13770... Loss: 1.3725... Val Loss: 1.3634\n",
      "Epoch: 10/20... Step: 13780... Loss: 1.4789... Val Loss: 1.3674\n",
      "Epoch: 10/20... Step: 13790... Loss: 1.4633... Val Loss: 1.3649\n",
      "Epoch: 10/20... Step: 13800... Loss: 1.3323... Val Loss: 1.3642\n",
      "Epoch: 10/20... Step: 13810... Loss: 1.3972... Val Loss: 1.3643\n",
      "Epoch: 10/20... Step: 13820... Loss: 1.3696... Val Loss: 1.3644\n",
      "Epoch: 10/20... Step: 13830... Loss: 1.3287... Val Loss: 1.3609\n",
      "Epoch: 10/20... Step: 13840... Loss: 1.4378... Val Loss: 1.3652\n",
      "Epoch: 10/20... Step: 13850... Loss: 1.3958... Val Loss: 1.3603\n",
      "Epoch: 10/20... Step: 13860... Loss: 1.3855... Val Loss: 1.3618\n",
      "Epoch: 10/20... Step: 13870... Loss: 1.3699... Val Loss: 1.3584\n",
      "Epoch: 10/20... Step: 13880... Loss: 1.4051... Val Loss: 1.3598\n",
      "Epoch: 10/20... Step: 13890... Loss: 1.3888... Val Loss: 1.3599\n",
      "Epoch: 10/20... Step: 13900... Loss: 1.3405... Val Loss: 1.3575\n",
      "Epoch: 10/20... Step: 13910... Loss: 1.4427... Val Loss: 1.3591\n",
      "Epoch: 10/20... Step: 13920... Loss: 1.3364... Val Loss: 1.3592\n",
      "Epoch: 10/20... Step: 13930... Loss: 1.3562... Val Loss: 1.3590\n",
      "Epoch: 10/20... Step: 13940... Loss: 1.4213... Val Loss: 1.3607\n",
      "Epoch: 10/20... Step: 13950... Loss: 1.4308... Val Loss: 1.3595\n",
      "Epoch: 10/20... Step: 13960... Loss: 1.3805... Val Loss: 1.3567\n",
      "Epoch: 10/20... Step: 13970... Loss: 1.4733... Val Loss: 1.3627\n",
      "Epoch: 10/20... Step: 13980... Loss: 1.4183... Val Loss: 1.3615\n",
      "Epoch: 10/20... Step: 13990... Loss: 1.4313... Val Loss: 1.3578\n",
      "Epoch: 10/20... Step: 14000... Loss: 1.3740... Val Loss: 1.3582\n",
      "Epoch: 10/20... Step: 14010... Loss: 1.3911... Val Loss: 1.3563\n",
      "Epoch: 10/20... Step: 14020... Loss: 1.4445... Val Loss: 1.3584\n",
      "Epoch: 10/20... Step: 14030... Loss: 1.3682... Val Loss: 1.3559\n",
      "Epoch: 10/20... Step: 14040... Loss: 1.3998... Val Loss: 1.3616\n",
      "Epoch: 10/20... Step: 14050... Loss: 1.3685... Val Loss: 1.3581\n",
      "Epoch: 10/20... Step: 14060... Loss: 1.3601... Val Loss: 1.3592\n",
      "Epoch: 10/20... Step: 14070... Loss: 1.3707... Val Loss: 1.3585\n",
      "Epoch: 10/20... Step: 14080... Loss: 1.4614... Val Loss: 1.3592\n",
      "Epoch: 10/20... Step: 14090... Loss: 1.4480... Val Loss: 1.3580\n",
      "Epoch: 10/20... Step: 14100... Loss: 1.4086... Val Loss: 1.3588\n",
      "Epoch: 10/20... Step: 14110... Loss: 1.3094... Val Loss: 1.3582\n",
      "Epoch: 10/20... Step: 14120... Loss: 1.3685... Val Loss: 1.3568\n",
      "Epoch: 10/20... Step: 14130... Loss: 1.3998... Val Loss: 1.3591\n",
      "Epoch: 10/20... Step: 14140... Loss: 1.3420... Val Loss: 1.3589\n",
      "Epoch: 10/20... Step: 14150... Loss: 1.3724... Val Loss: 1.3561\n",
      "Epoch: 10/20... Step: 14160... Loss: 1.4566... Val Loss: 1.3576\n",
      "Epoch: 10/20... Step: 14170... Loss: 1.4740... Val Loss: 1.3553\n",
      "Epoch: 10/20... Step: 14180... Loss: 1.6013... Val Loss: 1.3553\n",
      "Epoch: 11/20... Step: 14190... Loss: 1.4413... Val Loss: 1.3618\n",
      "Epoch: 11/20... Step: 14200... Loss: 1.4459... Val Loss: 1.3508\n",
      "Epoch: 11/20... Step: 14210... Loss: 1.3703... Val Loss: 1.3514\n",
      "Epoch: 11/20... Step: 14220... Loss: 1.2955... Val Loss: 1.3543\n",
      "Epoch: 11/20... Step: 14230... Loss: 1.3887... Val Loss: 1.3515\n",
      "Epoch: 11/20... Step: 14240... Loss: 1.3978... Val Loss: 1.3537\n",
      "Epoch: 11/20... Step: 14250... Loss: 1.3665... Val Loss: 1.3522\n",
      "Epoch: 11/20... Step: 14260... Loss: 1.3194... Val Loss: 1.3509\n",
      "Epoch: 11/20... Step: 14270... Loss: 1.3554... Val Loss: 1.3525\n",
      "Epoch: 11/20... Step: 14280... Loss: 1.4037... Val Loss: 1.3526\n",
      "Epoch: 11/20... Step: 14290... Loss: 1.4956... Val Loss: 1.3578\n",
      "Epoch: 11/20... Step: 14300... Loss: 1.3327... Val Loss: 1.3534\n",
      "Epoch: 11/20... Step: 14310... Loss: 1.4462... Val Loss: 1.3484\n",
      "Epoch: 11/20... Step: 14320... Loss: 1.3831... Val Loss: 1.3540\n",
      "Epoch: 11/20... Step: 14330... Loss: 1.3103... Val Loss: 1.3541\n",
      "Epoch: 11/20... Step: 14340... Loss: 1.3856... Val Loss: 1.3524\n",
      "Epoch: 11/20... Step: 14350... Loss: 1.2946... Val Loss: 1.3525\n",
      "Epoch: 11/20... Step: 14360... Loss: 1.3515... Val Loss: 1.3552\n",
      "Epoch: 11/20... Step: 14370... Loss: 1.3670... Val Loss: 1.3573\n",
      "Epoch: 11/20... Step: 14380... Loss: 1.2954... Val Loss: 1.3539\n",
      "Epoch: 11/20... Step: 14390... Loss: 1.3563... Val Loss: 1.3575\n",
      "Epoch: 11/20... Step: 14400... Loss: 1.3523... Val Loss: 1.3533\n",
      "Epoch: 11/20... Step: 14410... Loss: 1.4169... Val Loss: 1.3570\n",
      "Epoch: 11/20... Step: 14420... Loss: 1.4156... Val Loss: 1.3565\n",
      "Epoch: 11/20... Step: 14430... Loss: 1.3897... Val Loss: 1.3545\n",
      "Epoch: 11/20... Step: 14440... Loss: 1.3699... Val Loss: 1.3548\n",
      "Epoch: 11/20... Step: 14450... Loss: 1.3498... Val Loss: 1.3541\n",
      "Epoch: 11/20... Step: 14460... Loss: 1.3759... Val Loss: 1.3556\n",
      "Epoch: 11/20... Step: 14470... Loss: 1.3408... Val Loss: 1.3547\n",
      "Epoch: 11/20... Step: 14480... Loss: 1.3906... Val Loss: 1.3558\n",
      "Epoch: 11/20... Step: 14490... Loss: 1.3990... Val Loss: 1.3567\n",
      "Epoch: 11/20... Step: 14500... Loss: 1.4045... Val Loss: 1.3550\n",
      "Epoch: 11/20... Step: 14510... Loss: 1.4089... Val Loss: 1.3532\n",
      "Epoch: 11/20... Step: 14520... Loss: 1.4006... Val Loss: 1.3571\n",
      "Epoch: 11/20... Step: 14530... Loss: 1.3960... Val Loss: 1.3535\n",
      "Epoch: 11/20... Step: 14540... Loss: 1.3925... Val Loss: 1.3542\n",
      "Epoch: 11/20... Step: 14550... Loss: 1.3614... Val Loss: 1.3531\n",
      "Epoch: 11/20... Step: 14560... Loss: 1.4429... Val Loss: 1.3503\n",
      "Epoch: 11/20... Step: 14570... Loss: 1.4533... Val Loss: 1.3588\n",
      "Epoch: 11/20... Step: 14580... Loss: 1.3599... Val Loss: 1.3509\n",
      "Epoch: 11/20... Step: 14590... Loss: 1.3375... Val Loss: 1.3569\n",
      "Epoch: 11/20... Step: 14600... Loss: 1.3396... Val Loss: 1.3555\n",
      "Epoch: 11/20... Step: 14610... Loss: 1.4673... Val Loss: 1.3550\n",
      "Epoch: 11/20... Step: 14620... Loss: 1.3954... Val Loss: 1.3552\n",
      "Epoch: 11/20... Step: 14630... Loss: 1.4045... Val Loss: 1.3584\n",
      "Epoch: 11/20... Step: 14640... Loss: 1.4122... Val Loss: 1.3552\n",
      "Epoch: 11/20... Step: 14650... Loss: 1.4138... Val Loss: 1.3550\n",
      "Epoch: 11/20... Step: 14660... Loss: 1.3912... Val Loss: 1.3547\n",
      "Epoch: 11/20... Step: 14670... Loss: 1.4416... Val Loss: 1.3585\n",
      "Epoch: 11/20... Step: 14680... Loss: 1.4086... Val Loss: 1.3567\n",
      "Epoch: 11/20... Step: 14690... Loss: 1.3456... Val Loss: 1.3533\n",
      "Epoch: 11/20... Step: 14700... Loss: 1.4039... Val Loss: 1.3555\n",
      "Epoch: 11/20... Step: 14710... Loss: 1.3567... Val Loss: 1.3520\n",
      "Epoch: 11/20... Step: 14720... Loss: 1.4089... Val Loss: 1.3519\n",
      "Epoch: 11/20... Step: 14730... Loss: 1.4213... Val Loss: 1.3587\n",
      "Epoch: 11/20... Step: 14740... Loss: 1.3842... Val Loss: 1.3549\n",
      "Epoch: 11/20... Step: 14750... Loss: 1.3746... Val Loss: 1.3517\n",
      "Epoch: 11/20... Step: 14760... Loss: 1.4304... Val Loss: 1.3543\n",
      "Epoch: 11/20... Step: 14770... Loss: 1.3118... Val Loss: 1.3576\n",
      "Epoch: 11/20... Step: 14780... Loss: 1.3894... Val Loss: 1.3581\n",
      "Epoch: 11/20... Step: 14790... Loss: 1.3419... Val Loss: 1.3523\n",
      "Epoch: 11/20... Step: 14800... Loss: 1.5139... Val Loss: 1.3520\n",
      "Epoch: 11/20... Step: 14810... Loss: 1.4083... Val Loss: 1.3532\n",
      "Epoch: 11/20... Step: 14820... Loss: 1.4023... Val Loss: 1.3514\n",
      "Epoch: 11/20... Step: 14830... Loss: 1.3832... Val Loss: 1.3534\n",
      "Epoch: 11/20... Step: 14840... Loss: 1.3622... Val Loss: 1.3582\n",
      "Epoch: 11/20... Step: 14850... Loss: 1.3645... Val Loss: 1.3532\n",
      "Epoch: 11/20... Step: 14860... Loss: 1.2486... Val Loss: 1.3582\n",
      "Epoch: 11/20... Step: 14870... Loss: 1.4421... Val Loss: 1.3537\n",
      "Epoch: 11/20... Step: 14880... Loss: 1.4023... Val Loss: 1.3551\n",
      "Epoch: 11/20... Step: 14890... Loss: 1.3512... Val Loss: 1.3573\n",
      "Epoch: 11/20... Step: 14900... Loss: 1.4154... Val Loss: 1.3530\n",
      "Epoch: 11/20... Step: 14910... Loss: 1.3836... Val Loss: 1.3543\n",
      "Epoch: 11/20... Step: 14920... Loss: 1.4305... Val Loss: 1.3540\n",
      "Epoch: 11/20... Step: 14930... Loss: 1.3819... Val Loss: 1.3526\n",
      "Epoch: 11/20... Step: 14940... Loss: 1.4304... Val Loss: 1.3540\n",
      "Epoch: 11/20... Step: 14950... Loss: 1.4178... Val Loss: 1.3539\n",
      "Epoch: 11/20... Step: 14960... Loss: 1.3486... Val Loss: 1.3529\n",
      "Epoch: 11/20... Step: 14970... Loss: 1.5088... Val Loss: 1.3537\n",
      "Epoch: 11/20... Step: 14980... Loss: 1.4292... Val Loss: 1.3557\n",
      "Epoch: 11/20... Step: 14990... Loss: 1.3461... Val Loss: 1.3554\n",
      "Epoch: 11/20... Step: 15000... Loss: 1.3846... Val Loss: 1.3566\n",
      "Epoch: 11/20... Step: 15010... Loss: 1.4372... Val Loss: 1.3573\n",
      "Epoch: 11/20... Step: 15020... Loss: 1.4328... Val Loss: 1.3556\n",
      "Epoch: 11/20... Step: 15030... Loss: 1.3355... Val Loss: 1.3583\n",
      "Epoch: 11/20... Step: 15040... Loss: 1.4027... Val Loss: 1.3536\n",
      "Epoch: 11/20... Step: 15050... Loss: 1.3806... Val Loss: 1.3550\n",
      "Epoch: 11/20... Step: 15060... Loss: 1.3070... Val Loss: 1.3599\n",
      "Epoch: 11/20... Step: 15070... Loss: 1.2972... Val Loss: 1.3543\n",
      "Epoch: 11/20... Step: 15080... Loss: 1.3319... Val Loss: 1.3562\n",
      "Epoch: 11/20... Step: 15090... Loss: 1.4012... Val Loss: 1.3560\n",
      "Epoch: 11/20... Step: 15100... Loss: 1.3483... Val Loss: 1.3551\n",
      "Epoch: 11/20... Step: 15110... Loss: 1.3106... Val Loss: 1.3586\n",
      "Epoch: 11/20... Step: 15120... Loss: 1.4299... Val Loss: 1.3541\n",
      "Epoch: 11/20... Step: 15130... Loss: 1.3727... Val Loss: 1.3571\n",
      "Epoch: 11/20... Step: 15140... Loss: 1.3707... Val Loss: 1.3577\n",
      "Epoch: 11/20... Step: 15150... Loss: 1.4344... Val Loss: 1.3517\n",
      "Epoch: 11/20... Step: 15160... Loss: 1.3285... Val Loss: 1.3534\n",
      "Epoch: 11/20... Step: 15170... Loss: 1.3691... Val Loss: 1.3525\n",
      "Epoch: 11/20... Step: 15180... Loss: 1.3540... Val Loss: 1.3549\n",
      "Epoch: 11/20... Step: 15190... Loss: 1.4080... Val Loss: 1.3543\n",
      "Epoch: 11/20... Step: 15200... Loss: 1.3633... Val Loss: 1.3538\n",
      "Epoch: 11/20... Step: 15210... Loss: 1.4464... Val Loss: 1.3561\n",
      "Epoch: 11/20... Step: 15220... Loss: 1.3593... Val Loss: 1.3551\n",
      "Epoch: 11/20... Step: 15230... Loss: 1.3689... Val Loss: 1.3569\n",
      "Epoch: 11/20... Step: 15240... Loss: 1.4434... Val Loss: 1.3521\n",
      "Epoch: 11/20... Step: 15250... Loss: 1.3628... Val Loss: 1.3520\n",
      "Epoch: 11/20... Step: 15260... Loss: 1.3845... Val Loss: 1.3544\n",
      "Epoch: 11/20... Step: 15270... Loss: 1.3801... Val Loss: 1.3502\n",
      "Epoch: 11/20... Step: 15280... Loss: 1.3181... Val Loss: 1.3506\n",
      "Epoch: 11/20... Step: 15290... Loss: 1.3865... Val Loss: 1.3506\n",
      "Epoch: 11/20... Step: 15300... Loss: 1.3691... Val Loss: 1.3498\n",
      "Epoch: 11/20... Step: 15310... Loss: 1.3856... Val Loss: 1.3545\n",
      "Epoch: 11/20... Step: 15320... Loss: 1.3432... Val Loss: 1.3495\n",
      "Epoch: 11/20... Step: 15330... Loss: 1.4298... Val Loss: 1.3534\n",
      "Epoch: 11/20... Step: 15340... Loss: 1.3696... Val Loss: 1.3476\n",
      "Epoch: 11/20... Step: 15350... Loss: 1.3589... Val Loss: 1.3528\n",
      "Epoch: 11/20... Step: 15360... Loss: 1.3323... Val Loss: 1.3518\n",
      "Epoch: 11/20... Step: 15370... Loss: 1.3723... Val Loss: 1.3497\n",
      "Epoch: 11/20... Step: 15380... Loss: 1.3356... Val Loss: 1.3491\n",
      "Epoch: 11/20... Step: 15390... Loss: 1.3786... Val Loss: 1.3550\n",
      "Epoch: 11/20... Step: 15400... Loss: 1.3663... Val Loss: 1.3513\n",
      "Epoch: 11/20... Step: 15410... Loss: 1.3151... Val Loss: 1.3489\n",
      "Epoch: 11/20... Step: 15420... Loss: 1.3752... Val Loss: 1.3509\n",
      "Epoch: 11/20... Step: 15430... Loss: 1.3436... Val Loss: 1.3463\n",
      "Epoch: 11/20... Step: 15440... Loss: 1.3644... Val Loss: 1.3496\n",
      "Epoch: 11/20... Step: 15450... Loss: 1.3971... Val Loss: 1.3477\n",
      "Epoch: 11/20... Step: 15460... Loss: 1.4653... Val Loss: 1.3508\n",
      "Epoch: 11/20... Step: 15470... Loss: 1.3923... Val Loss: 1.3492\n",
      "Epoch: 11/20... Step: 15480... Loss: 1.4323... Val Loss: 1.3514\n",
      "Epoch: 11/20... Step: 15490... Loss: 1.3848... Val Loss: 1.3548\n",
      "Epoch: 11/20... Step: 15500... Loss: 1.4184... Val Loss: 1.3479\n",
      "Epoch: 11/20... Step: 15510... Loss: 1.3333... Val Loss: 1.3489\n",
      "Epoch: 11/20... Step: 15520... Loss: 1.4345... Val Loss: 1.3517\n",
      "Epoch: 11/20... Step: 15530... Loss: 1.3663... Val Loss: 1.3491\n",
      "Epoch: 11/20... Step: 15540... Loss: 1.3514... Val Loss: 1.3487\n",
      "Epoch: 11/20... Step: 15550... Loss: 1.4405... Val Loss: 1.3525\n",
      "Epoch: 11/20... Step: 15560... Loss: 1.3156... Val Loss: 1.3470\n",
      "Epoch: 11/20... Step: 15570... Loss: 1.4110... Val Loss: 1.3488\n",
      "Epoch: 11/20... Step: 15580... Loss: 1.4146... Val Loss: 1.3501\n",
      "Epoch: 11/20... Step: 15590... Loss: 1.3623... Val Loss: 1.3464\n",
      "Epoch: 12/20... Step: 15600... Loss: 1.4327... Val Loss: 1.3467\n",
      "Epoch: 12/20... Step: 15610... Loss: 1.4955... Val Loss: 1.3466\n",
      "Epoch: 12/20... Step: 15620... Loss: 1.4085... Val Loss: 1.3418\n",
      "Epoch: 12/20... Step: 15630... Loss: 1.3480... Val Loss: 1.3452\n",
      "Epoch: 12/20... Step: 15640... Loss: 1.3200... Val Loss: 1.3454\n",
      "Epoch: 12/20... Step: 15650... Loss: 1.3385... Val Loss: 1.3463\n",
      "Epoch: 12/20... Step: 15660... Loss: 1.4035... Val Loss: 1.3434\n",
      "Epoch: 12/20... Step: 15670... Loss: 1.3420... Val Loss: 1.3436\n",
      "Epoch: 12/20... Step: 15680... Loss: 1.4426... Val Loss: 1.3432\n",
      "Epoch: 12/20... Step: 15690... Loss: 1.3517... Val Loss: 1.3456\n",
      "Epoch: 12/20... Step: 15700... Loss: 1.3653... Val Loss: 1.3492\n",
      "Epoch: 12/20... Step: 15710... Loss: 1.3493... Val Loss: 1.3515\n",
      "Epoch: 12/20... Step: 15720... Loss: 1.3241... Val Loss: 1.3444\n",
      "Epoch: 12/20... Step: 15730... Loss: 1.3171... Val Loss: 1.3420\n",
      "Epoch: 12/20... Step: 15740... Loss: 1.3283... Val Loss: 1.3480\n",
      "Epoch: 12/20... Step: 15750... Loss: 1.2839... Val Loss: 1.3455\n",
      "Epoch: 12/20... Step: 15760... Loss: 1.3957... Val Loss: 1.3459\n",
      "Epoch: 12/20... Step: 15770... Loss: 1.3207... Val Loss: 1.3452\n",
      "Epoch: 12/20... Step: 15780... Loss: 1.4054... Val Loss: 1.3479\n",
      "Epoch: 12/20... Step: 15790... Loss: 1.3692... Val Loss: 1.3472\n",
      "Epoch: 12/20... Step: 15800... Loss: 1.3209... Val Loss: 1.3484\n",
      "Epoch: 12/20... Step: 15810... Loss: 1.3863... Val Loss: 1.3494\n",
      "Epoch: 12/20... Step: 15820... Loss: 1.2918... Val Loss: 1.3454\n",
      "Epoch: 12/20... Step: 15830... Loss: 1.3496... Val Loss: 1.3512\n",
      "Epoch: 12/20... Step: 15840... Loss: 1.3936... Val Loss: 1.3509\n",
      "Epoch: 12/20... Step: 15850... Loss: 1.3642... Val Loss: 1.3458\n",
      "Epoch: 12/20... Step: 15860... Loss: 1.3395... Val Loss: 1.3487\n",
      "Epoch: 12/20... Step: 15870... Loss: 1.3144... Val Loss: 1.3433\n",
      "Epoch: 12/20... Step: 15880... Loss: 1.3398... Val Loss: 1.3460\n",
      "Epoch: 12/20... Step: 15890... Loss: 1.3288... Val Loss: 1.3482\n",
      "Epoch: 12/20... Step: 15900... Loss: 1.3110... Val Loss: 1.3460\n",
      "Epoch: 12/20... Step: 15910... Loss: 1.4174... Val Loss: 1.3483\n",
      "Epoch: 12/20... Step: 15920... Loss: 1.3389... Val Loss: 1.3465\n",
      "Epoch: 12/20... Step: 15930... Loss: 1.4223... Val Loss: 1.3465\n",
      "Epoch: 12/20... Step: 15940... Loss: 1.4640... Val Loss: 1.3464\n",
      "Epoch: 12/20... Step: 15950... Loss: 1.3848... Val Loss: 1.3477\n",
      "Epoch: 12/20... Step: 15960... Loss: 1.4265... Val Loss: 1.3427\n",
      "Epoch: 12/20... Step: 15970... Loss: 1.3656... Val Loss: 1.3482\n",
      "Epoch: 12/20... Step: 15980... Loss: 1.3930... Val Loss: 1.3435\n",
      "Epoch: 12/20... Step: 15990... Loss: 1.3734... Val Loss: 1.3470\n",
      "Epoch: 12/20... Step: 16000... Loss: 1.3208... Val Loss: 1.3442\n",
      "Epoch: 12/20... Step: 16010... Loss: 1.3317... Val Loss: 1.3489\n",
      "Epoch: 12/20... Step: 16020... Loss: 1.4196... Val Loss: 1.3469\n",
      "Epoch: 12/20... Step: 16030... Loss: 1.3743... Val Loss: 1.3450\n",
      "Epoch: 12/20... Step: 16040... Loss: 1.3933... Val Loss: 1.3497\n",
      "Epoch: 12/20... Step: 16050... Loss: 1.3936... Val Loss: 1.3471\n",
      "Epoch: 12/20... Step: 16060... Loss: 1.3163... Val Loss: 1.3450\n",
      "Epoch: 12/20... Step: 16070... Loss: 1.3497... Val Loss: 1.3465\n",
      "Epoch: 12/20... Step: 16080... Loss: 1.3666... Val Loss: 1.3462\n",
      "Epoch: 12/20... Step: 16090... Loss: 1.4387... Val Loss: 1.3481\n",
      "Epoch: 12/20... Step: 16100... Loss: 1.3653... Val Loss: 1.3455\n",
      "Epoch: 12/20... Step: 16110... Loss: 1.3648... Val Loss: 1.3435\n",
      "Epoch: 12/20... Step: 16120... Loss: 1.4183... Val Loss: 1.3511\n",
      "Epoch: 12/20... Step: 16130... Loss: 1.4177... Val Loss: 1.3443\n",
      "Epoch: 12/20... Step: 16140... Loss: 1.5016... Val Loss: 1.3437\n",
      "Epoch: 12/20... Step: 16150... Loss: 1.2765... Val Loss: 1.3467\n",
      "Epoch: 12/20... Step: 16160... Loss: 1.3974... Val Loss: 1.3452\n",
      "Epoch: 12/20... Step: 16170... Loss: 1.3768... Val Loss: 1.3430\n",
      "Epoch: 12/20... Step: 16180... Loss: 1.4549... Val Loss: 1.3444\n",
      "Epoch: 12/20... Step: 16190... Loss: 1.3255... Val Loss: 1.3477\n",
      "Epoch: 12/20... Step: 16200... Loss: 1.3040... Val Loss: 1.3474\n",
      "Epoch: 12/20... Step: 16210... Loss: 1.3898... Val Loss: 1.3466\n",
      "Epoch: 12/20... Step: 16220... Loss: 1.3253... Val Loss: 1.3432\n",
      "Epoch: 12/20... Step: 16230... Loss: 1.3482... Val Loss: 1.3459\n",
      "Epoch: 12/20... Step: 16240... Loss: 1.3640... Val Loss: 1.3438\n",
      "Epoch: 12/20... Step: 16250... Loss: 1.3941... Val Loss: 1.3460\n",
      "Epoch: 12/20... Step: 16260... Loss: 1.3741... Val Loss: 1.3478\n",
      "Epoch: 12/20... Step: 16270... Loss: 1.3465... Val Loss: 1.3466\n",
      "Epoch: 12/20... Step: 16280... Loss: 1.2951... Val Loss: 1.3480\n",
      "Epoch: 12/20... Step: 16290... Loss: 1.3381... Val Loss: 1.3471\n",
      "Epoch: 12/20... Step: 16300... Loss: 1.3502... Val Loss: 1.3456\n",
      "Epoch: 12/20... Step: 16310... Loss: 1.3353... Val Loss: 1.3455\n",
      "Epoch: 12/20... Step: 16320... Loss: 1.4406... Val Loss: 1.3432\n",
      "Epoch: 12/20... Step: 16330... Loss: 1.2955... Val Loss: 1.3448\n",
      "Epoch: 12/20... Step: 16340... Loss: 1.4177... Val Loss: 1.3454\n",
      "Epoch: 12/20... Step: 16350... Loss: 1.3949... Val Loss: 1.3440\n",
      "Epoch: 12/20... Step: 16360... Loss: 1.4002... Val Loss: 1.3442\n",
      "Epoch: 12/20... Step: 16370... Loss: 1.4337... Val Loss: 1.3438\n",
      "Epoch: 12/20... Step: 16380... Loss: 1.4247... Val Loss: 1.3465\n",
      "Epoch: 12/20... Step: 16390... Loss: 1.3788... Val Loss: 1.3443\n",
      "Epoch: 12/20... Step: 16400... Loss: 1.4252... Val Loss: 1.3423\n",
      "Epoch: 12/20... Step: 16410... Loss: 1.3706... Val Loss: 1.3502\n",
      "Epoch: 12/20... Step: 16420... Loss: 1.3345... Val Loss: 1.3436\n",
      "Epoch: 12/20... Step: 16430... Loss: 1.3546... Val Loss: 1.3473\n",
      "Epoch: 12/20... Step: 16440... Loss: 1.3701... Val Loss: 1.3456\n",
      "Epoch: 12/20... Step: 16450... Loss: 1.3358... Val Loss: 1.3446\n",
      "Epoch: 12/20... Step: 16460... Loss: 1.3804... Val Loss: 1.3431\n",
      "Epoch: 12/20... Step: 16470... Loss: 1.3833... Val Loss: 1.3477\n",
      "Epoch: 12/20... Step: 16480... Loss: 1.3602... Val Loss: 1.3461\n",
      "Epoch: 12/20... Step: 16490... Loss: 1.3620... Val Loss: 1.3461\n",
      "Epoch: 12/20... Step: 16500... Loss: 1.3932... Val Loss: 1.3457\n",
      "Epoch: 12/20... Step: 16510... Loss: 1.4014... Val Loss: 1.3461\n",
      "Epoch: 12/20... Step: 16520... Loss: 1.3878... Val Loss: 1.3431\n",
      "Epoch: 12/20... Step: 16530... Loss: 1.3600... Val Loss: 1.3549\n",
      "Epoch: 12/20... Step: 16540... Loss: 1.3519... Val Loss: 1.3469\n",
      "Epoch: 12/20... Step: 16550... Loss: 1.3304... Val Loss: 1.3490\n",
      "Epoch: 12/20... Step: 16560... Loss: 1.3748... Val Loss: 1.3486\n",
      "Epoch: 12/20... Step: 16570... Loss: 1.3258... Val Loss: 1.3448\n",
      "Epoch: 12/20... Step: 16580... Loss: 1.4132... Val Loss: 1.3475\n",
      "Epoch: 12/20... Step: 16590... Loss: 1.3494... Val Loss: 1.3467\n",
      "Epoch: 12/20... Step: 16600... Loss: 1.3804... Val Loss: 1.3474\n",
      "Epoch: 12/20... Step: 16610... Loss: 1.3324... Val Loss: 1.3477\n",
      "Epoch: 12/20... Step: 16620... Loss: 1.3524... Val Loss: 1.3456\n",
      "Epoch: 12/20... Step: 16630... Loss: 1.3047... Val Loss: 1.3456\n",
      "Epoch: 12/20... Step: 16640... Loss: 1.4038... Val Loss: 1.3479\n",
      "Epoch: 12/20... Step: 16650... Loss: 1.3711... Val Loss: 1.3484\n",
      "Epoch: 12/20... Step: 16660... Loss: 1.3537... Val Loss: 1.3444\n",
      "Epoch: 12/20... Step: 16670... Loss: 1.3974... Val Loss: 1.3447\n",
      "Epoch: 12/20... Step: 16680... Loss: 1.3757... Val Loss: 1.3438\n",
      "Epoch: 12/20... Step: 16690... Loss: 1.3480... Val Loss: 1.3423\n",
      "Epoch: 12/20... Step: 16700... Loss: 1.3625... Val Loss: 1.3467\n",
      "Epoch: 12/20... Step: 16710... Loss: 1.3901... Val Loss: 1.3422\n",
      "Epoch: 12/20... Step: 16720... Loss: 1.3638... Val Loss: 1.3430\n",
      "Epoch: 12/20... Step: 16730... Loss: 1.3936... Val Loss: 1.3447\n",
      "Epoch: 12/20... Step: 16740... Loss: 1.3598... Val Loss: 1.3399\n",
      "Epoch: 12/20... Step: 16750... Loss: 1.3879... Val Loss: 1.3411\n",
      "Epoch: 12/20... Step: 16760... Loss: 1.4096... Val Loss: 1.3403\n",
      "Epoch: 12/20... Step: 16770... Loss: 1.3076... Val Loss: 1.3438\n",
      "Epoch: 12/20... Step: 16780... Loss: 1.3518... Val Loss: 1.3450\n",
      "Epoch: 12/20... Step: 16790... Loss: 1.3190... Val Loss: 1.3390\n",
      "Epoch: 12/20... Step: 16800... Loss: 1.3187... Val Loss: 1.3397\n",
      "Epoch: 12/20... Step: 16810... Loss: 1.3312... Val Loss: 1.3476\n",
      "Epoch: 12/20... Step: 16820... Loss: 1.4026... Val Loss: 1.3454\n",
      "Epoch: 12/20... Step: 16830... Loss: 1.3548... Val Loss: 1.3400\n",
      "Epoch: 12/20... Step: 16840... Loss: 1.3708... Val Loss: 1.3397\n",
      "Epoch: 12/20... Step: 16850... Loss: 1.3256... Val Loss: 1.3412\n",
      "Epoch: 12/20... Step: 16860... Loss: 1.3557... Val Loss: 1.3421\n",
      "Epoch: 12/20... Step: 16870... Loss: 1.3460... Val Loss: 1.3404\n",
      "Epoch: 12/20... Step: 16880... Loss: 1.3402... Val Loss: 1.3437\n",
      "Epoch: 12/20... Step: 16890... Loss: 1.4187... Val Loss: 1.3422\n",
      "Epoch: 12/20... Step: 16900... Loss: 1.3737... Val Loss: 1.3420\n",
      "Epoch: 12/20... Step: 16910... Loss: 1.4592... Val Loss: 1.3432\n",
      "Epoch: 12/20... Step: 16920... Loss: 1.2910... Val Loss: 1.3389\n",
      "Epoch: 12/20... Step: 16930... Loss: 1.3605... Val Loss: 1.3419\n",
      "Epoch: 12/20... Step: 16940... Loss: 1.3403... Val Loss: 1.3425\n",
      "Epoch: 12/20... Step: 16950... Loss: 1.4632... Val Loss: 1.3405\n",
      "Epoch: 12/20... Step: 16960... Loss: 1.3663... Val Loss: 1.3390\n",
      "Epoch: 12/20... Step: 16970... Loss: 1.3290... Val Loss: 1.3450\n",
      "Epoch: 12/20... Step: 16980... Loss: 1.3826... Val Loss: 1.3417\n",
      "Epoch: 12/20... Step: 16990... Loss: 1.3743... Val Loss: 1.3420\n",
      "Epoch: 12/20... Step: 17000... Loss: 1.3704... Val Loss: 1.3421\n",
      "Epoch: 12/20... Step: 17010... Loss: 1.3708... Val Loss: 1.3398\n",
      "Epoch: 13/20... Step: 17020... Loss: 1.4298... Val Loss: 1.3423\n",
      "Epoch: 13/20... Step: 17030... Loss: 1.3638... Val Loss: 1.3373\n",
      "Epoch: 13/20... Step: 17040... Loss: 1.3525... Val Loss: 1.3348\n",
      "Epoch: 13/20... Step: 17050... Loss: 1.2657... Val Loss: 1.3416\n",
      "Epoch: 13/20... Step: 17060... Loss: 1.4231... Val Loss: 1.3384\n",
      "Epoch: 13/20... Step: 17070... Loss: 1.3623... Val Loss: 1.3364\n",
      "Epoch: 13/20... Step: 17080... Loss: 1.2966... Val Loss: 1.3398\n",
      "Epoch: 13/20... Step: 17090... Loss: 1.3207... Val Loss: 1.3354\n",
      "Epoch: 13/20... Step: 17100... Loss: 1.3322... Val Loss: 1.3358\n",
      "Epoch: 13/20... Step: 17110... Loss: 1.3380... Val Loss: 1.3406\n",
      "Epoch: 13/20... Step: 17120... Loss: 1.2905... Val Loss: 1.3371\n",
      "Epoch: 13/20... Step: 17130... Loss: 1.4063... Val Loss: 1.3430\n",
      "Epoch: 13/20... Step: 17140... Loss: 1.3791... Val Loss: 1.3374\n",
      "Epoch: 13/20... Step: 17150... Loss: 1.3510... Val Loss: 1.3342\n",
      "Epoch: 13/20... Step: 17160... Loss: 1.3163... Val Loss: 1.3396\n",
      "Epoch: 13/20... Step: 17170... Loss: 1.3541... Val Loss: 1.3401\n",
      "Epoch: 13/20... Step: 17180... Loss: 1.3672... Val Loss: 1.3363\n",
      "Epoch: 13/20... Step: 17190... Loss: 1.3335... Val Loss: 1.3380\n",
      "Epoch: 13/20... Step: 17200... Loss: 1.3316... Val Loss: 1.3398\n",
      "Epoch: 13/20... Step: 17210... Loss: 1.3263... Val Loss: 1.3400\n",
      "Epoch: 13/20... Step: 17220... Loss: 1.2876... Val Loss: 1.3391\n",
      "Epoch: 13/20... Step: 17230... Loss: 1.3331... Val Loss: 1.3424\n",
      "Epoch: 13/20... Step: 17240... Loss: 1.4164... Val Loss: 1.3403\n",
      "Epoch: 13/20... Step: 17250... Loss: 1.3412... Val Loss: 1.3403\n",
      "Epoch: 13/20... Step: 17260... Loss: 1.2821... Val Loss: 1.3404\n",
      "Epoch: 13/20... Step: 17270... Loss: 1.3829... Val Loss: 1.3418\n",
      "Epoch: 13/20... Step: 17280... Loss: 1.3474... Val Loss: 1.3368\n",
      "Epoch: 13/20... Step: 17290... Loss: 1.2945... Val Loss: 1.3384\n",
      "Epoch: 13/20... Step: 17300... Loss: 1.4002... Val Loss: 1.3423\n",
      "Epoch: 13/20... Step: 17310... Loss: 1.3360... Val Loss: 1.3402\n",
      "Epoch: 13/20... Step: 17320... Loss: 1.3531... Val Loss: 1.3418\n",
      "Epoch: 13/20... Step: 17330... Loss: 1.4183... Val Loss: 1.3419\n",
      "Epoch: 13/20... Step: 17340... Loss: 1.4184... Val Loss: 1.3404\n",
      "Epoch: 13/20... Step: 17350... Loss: 1.3734... Val Loss: 1.3362\n",
      "Epoch: 13/20... Step: 17360... Loss: 1.4164... Val Loss: 1.3403\n",
      "Epoch: 13/20... Step: 17370... Loss: 1.4024... Val Loss: 1.3366\n",
      "Epoch: 13/20... Step: 17380... Loss: 1.3489... Val Loss: 1.3385\n",
      "Epoch: 13/20... Step: 17390... Loss: 1.3734... Val Loss: 1.3400\n",
      "Epoch: 13/20... Step: 17400... Loss: 1.3522... Val Loss: 1.3354\n",
      "Epoch: 13/20... Step: 17410... Loss: 1.4039... Val Loss: 1.3374\n",
      "Epoch: 13/20... Step: 17420... Loss: 1.3218... Val Loss: 1.3370\n",
      "Epoch: 13/20... Step: 17430... Loss: 1.3493... Val Loss: 1.3385\n",
      "Epoch: 13/20... Step: 17440... Loss: 1.3461... Val Loss: 1.3404\n",
      "Epoch: 13/20... Step: 17450... Loss: 1.4703... Val Loss: 1.3375\n",
      "Epoch: 13/20... Step: 17460... Loss: 1.3398... Val Loss: 1.3390\n",
      "Epoch: 13/20... Step: 17470... Loss: 1.3251... Val Loss: 1.3374\n",
      "Epoch: 13/20... Step: 17480... Loss: 1.3974... Val Loss: 1.3376\n",
      "Epoch: 13/20... Step: 17490... Loss: 1.3481... Val Loss: 1.3390\n",
      "Epoch: 13/20... Step: 17500... Loss: 1.3159... Val Loss: 1.3410\n",
      "Epoch: 13/20... Step: 17510... Loss: 1.2973... Val Loss: 1.3393\n",
      "Epoch: 13/20... Step: 17520... Loss: 1.3493... Val Loss: 1.3403\n",
      "Epoch: 13/20... Step: 17530... Loss: 1.3805... Val Loss: 1.3379\n",
      "Epoch: 13/20... Step: 17540... Loss: 1.4818... Val Loss: 1.3430\n",
      "Epoch: 13/20... Step: 17550... Loss: 1.3624... Val Loss: 1.3356\n",
      "Epoch: 13/20... Step: 17560... Loss: 1.3342... Val Loss: 1.3365\n",
      "Epoch: 13/20... Step: 17570... Loss: 1.3583... Val Loss: 1.3435\n",
      "Epoch: 13/20... Step: 17580... Loss: 1.4552... Val Loss: 1.3385\n",
      "Epoch: 13/20... Step: 17590... Loss: 1.4049... Val Loss: 1.3384\n",
      "Epoch: 13/20... Step: 17600... Loss: 1.4637... Val Loss: 1.3385\n",
      "Epoch: 13/20... Step: 17610... Loss: 1.4338... Val Loss: 1.3456\n",
      "Epoch: 13/20... Step: 17620... Loss: 1.3610... Val Loss: 1.3386\n",
      "Epoch: 13/20... Step: 17630... Loss: 1.3649... Val Loss: 1.3390\n",
      "Epoch: 13/20... Step: 17640... Loss: 1.4171... Val Loss: 1.3377\n",
      "Epoch: 13/20... Step: 17650... Loss: 1.3855... Val Loss: 1.3396\n",
      "Epoch: 13/20... Step: 17660... Loss: 1.3711... Val Loss: 1.3387\n",
      "Epoch: 13/20... Step: 17670... Loss: 1.3707... Val Loss: 1.3389\n",
      "Epoch: 13/20... Step: 17680... Loss: 1.3897... Val Loss: 1.3401\n",
      "Epoch: 13/20... Step: 17690... Loss: 1.3639... Val Loss: 1.3390\n",
      "Epoch: 13/20... Step: 17700... Loss: 1.2871... Val Loss: 1.3393\n",
      "Epoch: 13/20... Step: 17710... Loss: 1.3091... Val Loss: 1.3387\n",
      "Epoch: 13/20... Step: 17720... Loss: 1.3260... Val Loss: 1.3407\n",
      "Epoch: 13/20... Step: 17730... Loss: 1.3257... Val Loss: 1.3354\n",
      "Epoch: 13/20... Step: 17740... Loss: 1.4087... Val Loss: 1.3398\n",
      "Epoch: 13/20... Step: 17750... Loss: 1.3480... Val Loss: 1.3363\n",
      "Epoch: 13/20... Step: 17760... Loss: 1.4032... Val Loss: 1.3399\n",
      "Epoch: 13/20... Step: 17770... Loss: 1.3612... Val Loss: 1.3390\n",
      "Epoch: 13/20... Step: 17780... Loss: 1.3132... Val Loss: 1.3375\n",
      "Epoch: 13/20... Step: 17790... Loss: 1.4094... Val Loss: 1.3366\n",
      "Epoch: 13/20... Step: 17800... Loss: 1.4109... Val Loss: 1.3365\n",
      "Epoch: 13/20... Step: 17810... Loss: 1.3727... Val Loss: 1.3384\n",
      "Epoch: 13/20... Step: 17820... Loss: 1.4143... Val Loss: 1.3378\n",
      "Epoch: 13/20... Step: 17830... Loss: 1.4116... Val Loss: 1.3408\n",
      "Epoch: 13/20... Step: 17840... Loss: 1.3710... Val Loss: 1.3406\n",
      "Epoch: 13/20... Step: 17850... Loss: 1.4304... Val Loss: 1.3380\n",
      "Epoch: 13/20... Step: 17860... Loss: 1.3141... Val Loss: 1.3388\n",
      "Epoch: 13/20... Step: 17870... Loss: 1.3800... Val Loss: 1.3378\n",
      "Epoch: 13/20... Step: 17880... Loss: 1.3765... Val Loss: 1.3367\n",
      "Epoch: 13/20... Step: 17890... Loss: 1.3743... Val Loss: 1.3407\n",
      "Epoch: 13/20... Step: 17900... Loss: 1.3276... Val Loss: 1.3433\n",
      "Epoch: 13/20... Step: 17910... Loss: 1.3513... Val Loss: 1.3390\n",
      "Epoch: 13/20... Step: 17920... Loss: 1.3804... Val Loss: 1.3379\n",
      "Epoch: 13/20... Step: 17930... Loss: 1.2772... Val Loss: 1.3406\n",
      "Epoch: 13/20... Step: 17940... Loss: 1.3065... Val Loss: 1.3368\n",
      "Epoch: 13/20... Step: 17950... Loss: 1.3487... Val Loss: 1.3431\n",
      "Epoch: 13/20... Step: 17960... Loss: 1.4089... Val Loss: 1.3392\n",
      "Epoch: 13/20... Step: 17970... Loss: 1.4199... Val Loss: 1.3417\n",
      "Epoch: 13/20... Step: 17980... Loss: 1.3942... Val Loss: 1.3388\n",
      "Epoch: 13/20... Step: 17990... Loss: 1.3871... Val Loss: 1.3364\n",
      "Epoch: 13/20... Step: 18000... Loss: 1.3506... Val Loss: 1.3382\n",
      "Epoch: 13/20... Step: 18010... Loss: 1.3252... Val Loss: 1.3372\n",
      "Epoch: 13/20... Step: 18020... Loss: 1.3665... Val Loss: 1.3367\n",
      "Epoch: 13/20... Step: 18030... Loss: 1.2891... Val Loss: 1.3404\n",
      "Epoch: 13/20... Step: 18040... Loss: 1.3894... Val Loss: 1.3377\n",
      "Epoch: 13/20... Step: 18050... Loss: 1.3650... Val Loss: 1.3387\n",
      "Epoch: 13/20... Step: 18060... Loss: 1.3358... Val Loss: 1.3402\n",
      "Epoch: 13/20... Step: 18070... Loss: 1.3323... Val Loss: 1.3392\n",
      "Epoch: 13/20... Step: 18080... Loss: 1.3798... Val Loss: 1.3380\n",
      "Epoch: 13/20... Step: 18090... Loss: 1.4005... Val Loss: 1.3364\n",
      "Epoch: 13/20... Step: 18100... Loss: 1.3372... Val Loss: 1.3383\n",
      "Epoch: 13/20... Step: 18110... Loss: 1.3618... Val Loss: 1.3360\n",
      "Epoch: 13/20... Step: 18120... Loss: 1.3683... Val Loss: 1.3367\n",
      "Epoch: 13/20... Step: 18130... Loss: 1.3225... Val Loss: 1.3362\n",
      "Epoch: 13/20... Step: 18140... Loss: 1.3618... Val Loss: 1.3341\n",
      "Epoch: 13/20... Step: 18150... Loss: 1.3304... Val Loss: 1.3341\n",
      "Epoch: 13/20... Step: 18160... Loss: 1.3581... Val Loss: 1.3356\n",
      "Epoch: 13/20... Step: 18170... Loss: 1.3675... Val Loss: 1.3334\n",
      "Epoch: 13/20... Step: 18180... Loss: 1.3118... Val Loss: 1.3349\n",
      "Epoch: 13/20... Step: 18190... Loss: 1.3617... Val Loss: 1.3355\n",
      "Epoch: 13/20... Step: 18200... Loss: 1.3046... Val Loss: 1.3377\n",
      "Epoch: 13/20... Step: 18210... Loss: 1.4311... Val Loss: 1.3321\n",
      "Epoch: 13/20... Step: 18220... Loss: 1.3224... Val Loss: 1.3355\n",
      "Epoch: 13/20... Step: 18230... Loss: 1.3101... Val Loss: 1.3390\n",
      "Epoch: 13/20... Step: 18240... Loss: 1.2838... Val Loss: 1.3392\n",
      "Epoch: 13/20... Step: 18250... Loss: 1.4225... Val Loss: 1.3332\n",
      "Epoch: 13/20... Step: 18260... Loss: 1.3492... Val Loss: 1.3363\n",
      "Epoch: 13/20... Step: 18270... Loss: 1.3714... Val Loss: 1.3358\n",
      "Epoch: 13/20... Step: 18280... Loss: 1.3673... Val Loss: 1.3331\n",
      "Epoch: 13/20... Step: 18290... Loss: 1.4003... Val Loss: 1.3357\n",
      "Epoch: 13/20... Step: 18300... Loss: 1.4255... Val Loss: 1.3362\n",
      "Epoch: 13/20... Step: 18310... Loss: 1.3950... Val Loss: 1.3360\n",
      "Epoch: 13/20... Step: 18320... Loss: 1.3598... Val Loss: 1.3339\n",
      "Epoch: 13/20... Step: 18330... Loss: 1.3897... Val Loss: 1.3378\n",
      "Epoch: 13/20... Step: 18340... Loss: 1.3391... Val Loss: 1.3313\n",
      "Epoch: 13/20... Step: 18350... Loss: 1.3849... Val Loss: 1.3326\n",
      "Epoch: 13/20... Step: 18360... Loss: 1.3781... Val Loss: 1.3355\n",
      "Epoch: 13/20... Step: 18370... Loss: 1.3602... Val Loss: 1.3321\n",
      "Epoch: 13/20... Step: 18380... Loss: 1.3529... Val Loss: 1.3329\n",
      "Epoch: 13/20... Step: 18390... Loss: 1.4200... Val Loss: 1.3384\n",
      "Epoch: 13/20... Step: 18400... Loss: 1.3253... Val Loss: 1.3315\n",
      "Epoch: 13/20... Step: 18410... Loss: 1.3408... Val Loss: 1.3349\n",
      "Epoch: 13/20... Step: 18420... Loss: 1.3645... Val Loss: 1.3335\n",
      "Epoch: 13/20... Step: 18430... Loss: 1.3408... Val Loss: 1.3323\n",
      "Epoch: 14/20... Step: 18440... Loss: 1.3753... Val Loss: 1.3354\n",
      "Epoch: 14/20... Step: 18450... Loss: 1.3619... Val Loss: 1.3285\n",
      "Epoch: 14/20... Step: 18460... Loss: 1.3813... Val Loss: 1.3277\n",
      "Epoch: 14/20... Step: 18470... Loss: 1.2493... Val Loss: 1.3344\n",
      "Epoch: 14/20... Step: 18480... Loss: 1.3266... Val Loss: 1.3301\n",
      "Epoch: 14/20... Step: 18490... Loss: 1.3536... Val Loss: 1.3320\n",
      "Epoch: 14/20... Step: 18500... Loss: 1.3531... Val Loss: 1.3308\n",
      "Epoch: 14/20... Step: 18510... Loss: 1.4035... Val Loss: 1.3298\n",
      "Epoch: 14/20... Step: 18520... Loss: 1.3202... Val Loss: 1.3303\n",
      "Epoch: 14/20... Step: 18530... Loss: 1.4425... Val Loss: 1.3344\n",
      "Epoch: 14/20... Step: 18540... Loss: 1.3203... Val Loss: 1.3326\n",
      "Epoch: 14/20... Step: 18550... Loss: 1.3404... Val Loss: 1.3352\n",
      "Epoch: 14/20... Step: 18560... Loss: 1.3825... Val Loss: 1.3300\n",
      "Epoch: 14/20... Step: 18570... Loss: 1.3339... Val Loss: 1.3303\n",
      "Epoch: 14/20... Step: 18580... Loss: 1.3944... Val Loss: 1.3354\n",
      "Epoch: 14/20... Step: 18590... Loss: 1.3305... Val Loss: 1.3341\n",
      "Epoch: 14/20... Step: 18600... Loss: 1.2972... Val Loss: 1.3288\n",
      "Epoch: 14/20... Step: 18610... Loss: 1.3174... Val Loss: 1.3333\n",
      "Epoch: 14/20... Step: 18620... Loss: 1.3429... Val Loss: 1.3373\n",
      "Epoch: 14/20... Step: 18630... Loss: 1.3060... Val Loss: 1.3343\n",
      "Epoch: 14/20... Step: 18640... Loss: 1.3326... Val Loss: 1.3352\n",
      "Epoch: 14/20... Step: 18650... Loss: 1.3323... Val Loss: 1.3363\n",
      "Epoch: 14/20... Step: 18660... Loss: 1.4197... Val Loss: 1.3313\n",
      "Epoch: 14/20... Step: 18670... Loss: 1.3036... Val Loss: 1.3392\n",
      "Epoch: 14/20... Step: 18680... Loss: 1.3800... Val Loss: 1.3321\n",
      "Epoch: 14/20... Step: 18690... Loss: 1.2964... Val Loss: 1.3367\n",
      "Epoch: 14/20... Step: 18700... Loss: 1.3311... Val Loss: 1.3326\n",
      "Epoch: 14/20... Step: 18710... Loss: 1.3509... Val Loss: 1.3346\n",
      "Epoch: 14/20... Step: 18720... Loss: 1.4045... Val Loss: 1.3344\n",
      "Epoch: 14/20... Step: 18730... Loss: 1.4328... Val Loss: 1.3348\n",
      "Epoch: 14/20... Step: 18740... Loss: 1.3341... Val Loss: 1.3322\n",
      "Epoch: 14/20... Step: 18750... Loss: 1.3016... Val Loss: 1.3331\n",
      "Epoch: 14/20... Step: 18760... Loss: 1.3314... Val Loss: 1.3323\n",
      "Epoch: 14/20... Step: 18770... Loss: 1.3298... Val Loss: 1.3297\n",
      "Epoch: 14/20... Step: 18780... Loss: 1.3011... Val Loss: 1.3334\n",
      "Epoch: 14/20... Step: 18790... Loss: 1.3605... Val Loss: 1.3320\n",
      "Epoch: 14/20... Step: 18800... Loss: 1.3592... Val Loss: 1.3302\n",
      "Epoch: 14/20... Step: 18810... Loss: 1.3893... Val Loss: 1.3308\n",
      "Epoch: 14/20... Step: 18820... Loss: 1.3848... Val Loss: 1.3319\n",
      "Epoch: 14/20... Step: 18830... Loss: 1.3819... Val Loss: 1.3293\n",
      "Epoch: 14/20... Step: 18840... Loss: 1.2697... Val Loss: 1.3329\n",
      "Epoch: 14/20... Step: 18850... Loss: 1.3049... Val Loss: 1.3316\n",
      "Epoch: 14/20... Step: 18860... Loss: 1.3923... Val Loss: 1.3321\n",
      "Epoch: 14/20... Step: 18870... Loss: 1.3581... Val Loss: 1.3318\n",
      "Epoch: 14/20... Step: 18880... Loss: 1.3561... Val Loss: 1.3359\n",
      "Epoch: 14/20... Step: 18890... Loss: 1.3375... Val Loss: 1.3297\n",
      "Epoch: 14/20... Step: 18900... Loss: 1.3590... Val Loss: 1.3320\n",
      "Epoch: 14/20... Step: 18910... Loss: 1.3312... Val Loss: 1.3347\n",
      "Epoch: 14/20... Step: 18920... Loss: 1.3380... Val Loss: 1.3329\n",
      "Epoch: 14/20... Step: 18930... Loss: 1.3425... Val Loss: 1.3333\n",
      "Epoch: 14/20... Step: 18940... Loss: 1.3702... Val Loss: 1.3309\n",
      "Epoch: 14/20... Step: 18950... Loss: 1.3568... Val Loss: 1.3314\n",
      "Epoch: 14/20... Step: 18960... Loss: 1.3683... Val Loss: 1.3344\n",
      "Epoch: 14/20... Step: 18970... Loss: 1.3153... Val Loss: 1.3306\n",
      "Epoch: 14/20... Step: 18980... Loss: 1.3154... Val Loss: 1.3328\n",
      "Epoch: 14/20... Step: 18990... Loss: 1.3337... Val Loss: 1.3338\n",
      "Epoch: 14/20... Step: 19000... Loss: 1.3618... Val Loss: 1.3295\n",
      "Epoch: 14/20... Step: 19010... Loss: 1.3160... Val Loss: 1.3321\n",
      "Epoch: 14/20... Step: 19020... Loss: 1.3697... Val Loss: 1.3308\n",
      "Epoch: 14/20... Step: 19030... Loss: 1.3202... Val Loss: 1.3347\n",
      "Epoch: 14/20... Step: 19040... Loss: 1.3630... Val Loss: 1.3343\n",
      "Epoch: 14/20... Step: 19050... Loss: 1.3659... Val Loss: 1.3328\n",
      "Epoch: 14/20... Step: 19060... Loss: 1.3311... Val Loss: 1.3317\n",
      "Epoch: 14/20... Step: 19070... Loss: 1.3877... Val Loss: 1.3332\n",
      "Epoch: 14/20... Step: 19080... Loss: 1.3947... Val Loss: 1.3369\n",
      "Epoch: 14/20... Step: 19090... Loss: 1.2860... Val Loss: 1.3335\n",
      "Epoch: 14/20... Step: 19100... Loss: 1.3889... Val Loss: 1.3317\n",
      "Epoch: 14/20... Step: 19110... Loss: 1.3615... Val Loss: 1.3318\n",
      "Epoch: 14/20... Step: 19120... Loss: 1.3552... Val Loss: 1.3318\n",
      "Epoch: 14/20... Step: 19130... Loss: 1.3102... Val Loss: 1.3351\n",
      "Epoch: 14/20... Step: 19140... Loss: 1.2930... Val Loss: 1.3342\n",
      "Epoch: 14/20... Step: 19150... Loss: 1.3564... Val Loss: 1.3304\n",
      "Epoch: 14/20... Step: 19160... Loss: 1.3839... Val Loss: 1.3305\n",
      "Epoch: 14/20... Step: 19170... Loss: 1.3808... Val Loss: 1.3302\n",
      "Epoch: 14/20... Step: 19180... Loss: 1.4031... Val Loss: 1.3320\n",
      "Epoch: 14/20... Step: 19190... Loss: 1.3527... Val Loss: 1.3297\n",
      "Epoch: 14/20... Step: 19200... Loss: 1.3013... Val Loss: 1.3317\n",
      "Epoch: 14/20... Step: 19210... Loss: 1.3889... Val Loss: 1.3295\n",
      "Epoch: 14/20... Step: 19220... Loss: 1.3621... Val Loss: 1.3312\n",
      "Epoch: 14/20... Step: 19230... Loss: 1.3577... Val Loss: 1.3321\n",
      "Epoch: 14/20... Step: 19240... Loss: 1.3857... Val Loss: 1.3293\n",
      "Epoch: 14/20... Step: 19250... Loss: 1.3418... Val Loss: 1.3337\n",
      "Epoch: 14/20... Step: 19260... Loss: 1.3027... Val Loss: 1.3336\n",
      "Epoch: 14/20... Step: 19270... Loss: 1.4085... Val Loss: 1.3303\n",
      "Epoch: 14/20... Step: 19280... Loss: 1.3204... Val Loss: 1.3320\n",
      "Epoch: 14/20... Step: 19290... Loss: 1.3150... Val Loss: 1.3322\n",
      "Epoch: 14/20... Step: 19300... Loss: 1.4538... Val Loss: 1.3291\n",
      "Epoch: 14/20... Step: 19310... Loss: 1.2835... Val Loss: 1.3334\n",
      "Epoch: 14/20... Step: 19320... Loss: 1.3450... Val Loss: 1.3316\n",
      "Epoch: 14/20... Step: 19330... Loss: 1.3225... Val Loss: 1.3311\n",
      "Epoch: 14/20... Step: 19340... Loss: 1.4199... Val Loss: 1.3287\n",
      "Epoch: 14/20... Step: 19350... Loss: 1.3783... Val Loss: 1.3315\n",
      "Epoch: 14/20... Step: 19360... Loss: 1.3371... Val Loss: 1.3289\n",
      "Epoch: 14/20... Step: 19370... Loss: 1.3179... Val Loss: 1.3335\n",
      "Epoch: 14/20... Step: 19380... Loss: 1.3241... Val Loss: 1.3305\n",
      "Epoch: 14/20... Step: 19390... Loss: 1.3337... Val Loss: 1.3332\n",
      "Epoch: 14/20... Step: 19400... Loss: 1.3507... Val Loss: 1.3308\n",
      "Epoch: 14/20... Step: 19410... Loss: 1.3025... Val Loss: 1.3313\n",
      "Epoch: 14/20... Step: 19420... Loss: 1.3523... Val Loss: 1.3315\n",
      "Epoch: 14/20... Step: 19430... Loss: 1.3275... Val Loss: 1.3327\n",
      "Epoch: 14/20... Step: 19440... Loss: 1.3159... Val Loss: 1.3337\n",
      "Epoch: 14/20... Step: 19450... Loss: 1.3099... Val Loss: 1.3335\n",
      "Epoch: 14/20... Step: 19460... Loss: 1.3361... Val Loss: 1.3356\n",
      "Epoch: 14/20... Step: 19470... Loss: 1.3474... Val Loss: 1.3336\n",
      "Epoch: 14/20... Step: 19480... Loss: 1.3040... Val Loss: 1.3351\n",
      "Epoch: 14/20... Step: 19490... Loss: 1.2740... Val Loss: 1.3340\n",
      "Epoch: 14/20... Step: 19500... Loss: 1.3745... Val Loss: 1.3298\n",
      "Epoch: 14/20... Step: 19510... Loss: 1.3785... Val Loss: 1.3324\n",
      "Epoch: 14/20... Step: 19520... Loss: 1.3655... Val Loss: 1.3308\n",
      "Epoch: 14/20... Step: 19530... Loss: 1.4073... Val Loss: 1.3301\n",
      "Epoch: 14/20... Step: 19540... Loss: 1.3489... Val Loss: 1.3308\n",
      "Epoch: 14/20... Step: 19550... Loss: 1.3495... Val Loss: 1.3267\n",
      "Epoch: 14/20... Step: 19560... Loss: 1.2966... Val Loss: 1.3293\n",
      "Epoch: 14/20... Step: 19570... Loss: 1.3806... Val Loss: 1.3287\n",
      "Epoch: 14/20... Step: 19580... Loss: 1.4125... Val Loss: 1.3265\n",
      "Epoch: 14/20... Step: 19590... Loss: 1.3142... Val Loss: 1.3271\n",
      "Epoch: 14/20... Step: 19600... Loss: 1.2978... Val Loss: 1.3271\n",
      "Epoch: 14/20... Step: 19610... Loss: 1.2410... Val Loss: 1.3267\n",
      "Epoch: 14/20... Step: 19620... Loss: 1.3023... Val Loss: 1.3299\n",
      "Epoch: 14/20... Step: 19630... Loss: 1.3267... Val Loss: 1.3258\n",
      "Epoch: 14/20... Step: 19640... Loss: 1.3840... Val Loss: 1.3289\n",
      "Epoch: 14/20... Step: 19650... Loss: 1.2809... Val Loss: 1.3322\n",
      "Epoch: 14/20... Step: 19660... Loss: 1.3125... Val Loss: 1.3312\n",
      "Epoch: 14/20... Step: 19670... Loss: 1.3013... Val Loss: 1.3263\n",
      "Epoch: 14/20... Step: 19680... Loss: 1.3316... Val Loss: 1.3283\n",
      "Epoch: 14/20... Step: 19690... Loss: 1.3642... Val Loss: 1.3277\n",
      "Epoch: 14/20... Step: 19700... Loss: 1.3774... Val Loss: 1.3269\n",
      "Epoch: 14/20... Step: 19710... Loss: 1.3324... Val Loss: 1.3309\n",
      "Epoch: 14/20... Step: 19720... Loss: 1.3532... Val Loss: 1.3283\n",
      "Epoch: 14/20... Step: 19730... Loss: 1.2715... Val Loss: 1.3282\n",
      "Epoch: 14/20... Step: 19740... Loss: 1.3203... Val Loss: 1.3296\n",
      "Epoch: 14/20... Step: 19750... Loss: 1.3803... Val Loss: 1.3286\n",
      "Epoch: 14/20... Step: 19760... Loss: 1.3795... Val Loss: 1.3251\n",
      "Epoch: 14/20... Step: 19770... Loss: 1.3908... Val Loss: 1.3294\n",
      "Epoch: 14/20... Step: 19780... Loss: 1.3567... Val Loss: 1.3284\n",
      "Epoch: 14/20... Step: 19790... Loss: 1.3694... Val Loss: 1.3275\n",
      "Epoch: 14/20... Step: 19800... Loss: 1.3753... Val Loss: 1.3278\n",
      "Epoch: 14/20... Step: 19810... Loss: 1.3453... Val Loss: 1.3323\n",
      "Epoch: 14/20... Step: 19820... Loss: 1.4326... Val Loss: 1.3287\n",
      "Epoch: 14/20... Step: 19830... Loss: 1.3430... Val Loss: 1.3289\n",
      "Epoch: 14/20... Step: 19840... Loss: 1.3578... Val Loss: 1.3258\n",
      "Epoch: 14/20... Step: 19850... Loss: 1.3721... Val Loss: 1.3269\n",
      "Epoch: 15/20... Step: 19860... Loss: 1.3639... Val Loss: 1.3282\n",
      "Epoch: 15/20... Step: 19870... Loss: 1.3903... Val Loss: 1.3221\n",
      "Epoch: 15/20... Step: 19880... Loss: 1.2991... Val Loss: 1.3229\n",
      "Epoch: 15/20... Step: 19890... Loss: 1.3459... Val Loss: 1.3268\n",
      "Epoch: 15/20... Step: 19900... Loss: 1.2993... Val Loss: 1.3249\n",
      "Epoch: 15/20... Step: 19910... Loss: 1.3879... Val Loss: 1.3244\n",
      "Epoch: 15/20... Step: 19920... Loss: 1.2622... Val Loss: 1.3260\n",
      "Epoch: 15/20... Step: 19930... Loss: 1.3373... Val Loss: 1.3226\n",
      "Epoch: 15/20... Step: 19940... Loss: 1.3001... Val Loss: 1.3234\n",
      "Epoch: 15/20... Step: 19950... Loss: 1.3627... Val Loss: 1.3294\n",
      "Epoch: 15/20... Step: 19960... Loss: 1.3219... Val Loss: 1.3259\n",
      "Epoch: 15/20... Step: 19970... Loss: 1.3445... Val Loss: 1.3296\n",
      "Epoch: 15/20... Step: 19980... Loss: 1.3409... Val Loss: 1.3237\n",
      "Epoch: 15/20... Step: 19990... Loss: 1.3335... Val Loss: 1.3247\n",
      "Epoch: 15/20... Step: 20000... Loss: 1.3826... Val Loss: 1.3275\n",
      "Epoch: 15/20... Step: 20010... Loss: 1.2996... Val Loss: 1.3296\n",
      "Epoch: 15/20... Step: 20020... Loss: 1.3378... Val Loss: 1.3240\n",
      "Epoch: 15/20... Step: 20030... Loss: 1.3788... Val Loss: 1.3298\n",
      "Epoch: 15/20... Step: 20040... Loss: 1.3774... Val Loss: 1.3292\n",
      "Epoch: 15/20... Step: 20050... Loss: 1.3605... Val Loss: 1.3297\n",
      "Epoch: 15/20... Step: 20060... Loss: 1.2834... Val Loss: 1.3291\n",
      "Epoch: 15/20... Step: 20070... Loss: 1.3918... Val Loss: 1.3286\n",
      "Epoch: 15/20... Step: 20080... Loss: 1.3132... Val Loss: 1.3290\n",
      "Epoch: 15/20... Step: 20090... Loss: 1.2752... Val Loss: 1.3296\n",
      "Epoch: 15/20... Step: 20100... Loss: 1.3699... Val Loss: 1.3300\n",
      "Epoch: 15/20... Step: 20110... Loss: 1.3507... Val Loss: 1.3269\n",
      "Epoch: 15/20... Step: 20120... Loss: 1.3277... Val Loss: 1.3287\n",
      "Epoch: 15/20... Step: 20130... Loss: 1.3819... Val Loss: 1.3281\n",
      "Epoch: 15/20... Step: 20140... Loss: 1.3668... Val Loss: 1.3278\n",
      "Epoch: 15/20... Step: 20150... Loss: 1.3620... Val Loss: 1.3316\n",
      "Epoch: 15/20... Step: 20160... Loss: 1.2898... Val Loss: 1.3292\n",
      "Epoch: 15/20... Step: 20170... Loss: 1.3496... Val Loss: 1.3290\n",
      "Epoch: 15/20... Step: 20180... Loss: 1.3927... Val Loss: 1.3275\n",
      "Epoch: 15/20... Step: 20190... Loss: 1.4080... Val Loss: 1.3250\n",
      "Epoch: 15/20... Step: 20200... Loss: 1.3034... Val Loss: 1.3275\n",
      "Epoch: 15/20... Step: 20210... Loss: 1.3916... Val Loss: 1.3281\n",
      "Epoch: 15/20... Step: 20220... Loss: 1.3898... Val Loss: 1.3241\n",
      "Epoch: 15/20... Step: 20230... Loss: 1.3659... Val Loss: 1.3237\n",
      "Epoch: 15/20... Step: 20240... Loss: 1.3569... Val Loss: 1.3282\n",
      "Epoch: 15/20... Step: 20250... Loss: 1.3850... Val Loss: 1.3241\n",
      "Epoch: 15/20... Step: 20260... Loss: 1.3600... Val Loss: 1.3277\n",
      "Epoch: 15/20... Step: 20270... Loss: 1.2734... Val Loss: 1.3296\n",
      "Epoch: 15/20... Step: 20280... Loss: 1.3568... Val Loss: 1.3272\n",
      "Epoch: 15/20... Step: 20290... Loss: 1.3572... Val Loss: 1.3272\n",
      "Epoch: 15/20... Step: 20300... Loss: 1.2920... Val Loss: 1.3270\n",
      "Epoch: 15/20... Step: 20310... Loss: 1.3668... Val Loss: 1.3307\n",
      "Epoch: 15/20... Step: 20320... Loss: 1.3453... Val Loss: 1.3278\n",
      "Epoch: 15/20... Step: 20330... Loss: 1.2394... Val Loss: 1.3304\n",
      "Epoch: 15/20... Step: 20340... Loss: 1.3771... Val Loss: 1.3288\n",
      "Epoch: 15/20... Step: 20350... Loss: 1.3750... Val Loss: 1.3286\n",
      "Epoch: 15/20... Step: 20360... Loss: 1.3515... Val Loss: 1.3294\n",
      "Epoch: 15/20... Step: 20370... Loss: 1.3242... Val Loss: 1.3260\n",
      "Epoch: 15/20... Step: 20380... Loss: 1.2939... Val Loss: 1.3280\n",
      "Epoch: 15/20... Step: 20390... Loss: 1.3958... Val Loss: 1.3244\n",
      "Epoch: 15/20... Step: 20400... Loss: 1.3176... Val Loss: 1.3305\n",
      "Epoch: 15/20... Step: 20410... Loss: 1.3145... Val Loss: 1.3298\n",
      "Epoch: 15/20... Step: 20420... Loss: 1.3681... Val Loss: 1.3278\n",
      "Epoch: 15/20... Step: 20430... Loss: 1.3183... Val Loss: 1.3296\n",
      "Epoch: 15/20... Step: 20440... Loss: 1.3123... Val Loss: 1.3252\n",
      "Epoch: 15/20... Step: 20450... Loss: 1.3086... Val Loss: 1.3318\n",
      "Epoch: 15/20... Step: 20460... Loss: 1.3333... Val Loss: 1.3295\n",
      "Epoch: 15/20... Step: 20470... Loss: 1.3795... Val Loss: 1.3268\n",
      "Epoch: 15/20... Step: 20480... Loss: 1.3473... Val Loss: 1.3244\n",
      "Epoch: 15/20... Step: 20490... Loss: 1.3845... Val Loss: 1.3296\n",
      "Epoch: 15/20... Step: 20500... Loss: 1.3792... Val Loss: 1.3263\n",
      "Epoch: 15/20... Step: 20510... Loss: 1.3435... Val Loss: 1.3288\n",
      "Epoch: 15/20... Step: 20520... Loss: 1.4072... Val Loss: 1.3276\n",
      "Epoch: 15/20... Step: 20530... Loss: 1.3415... Val Loss: 1.3318\n",
      "Epoch: 15/20... Step: 20540... Loss: 1.3019... Val Loss: 1.3247\n",
      "Epoch: 15/20... Step: 20550... Loss: 1.3043... Val Loss: 1.3288\n",
      "Epoch: 15/20... Step: 20560... Loss: 1.3552... Val Loss: 1.3259\n",
      "Epoch: 15/20... Step: 20570... Loss: 1.3377... Val Loss: 1.3261\n",
      "Epoch: 15/20... Step: 20580... Loss: 1.3873... Val Loss: 1.3267\n",
      "Epoch: 15/20... Step: 20590... Loss: 1.3086... Val Loss: 1.3270\n",
      "Epoch: 15/20... Step: 20600... Loss: 1.3679... Val Loss: 1.3244\n",
      "Epoch: 15/20... Step: 20610... Loss: 1.3774... Val Loss: 1.3228\n",
      "Epoch: 15/20... Step: 20620... Loss: 1.3829... Val Loss: 1.3277\n",
      "Epoch: 15/20... Step: 20630... Loss: 1.3773... Val Loss: 1.3239\n",
      "Epoch: 15/20... Step: 20640... Loss: 1.4053... Val Loss: 1.3237\n",
      "Epoch: 15/20... Step: 20650... Loss: 1.3651... Val Loss: 1.3238\n",
      "Epoch: 15/20... Step: 20660... Loss: 1.3403... Val Loss: 1.3265\n",
      "Epoch: 15/20... Step: 20670... Loss: 1.3275... Val Loss: 1.3263\n",
      "Epoch: 15/20... Step: 20680... Loss: 1.2775... Val Loss: 1.3284\n",
      "Epoch: 15/20... Step: 20690... Loss: 1.4008... Val Loss: 1.3253\n",
      "Epoch: 15/20... Step: 20700... Loss: 1.3738... Val Loss: 1.3295\n",
      "Epoch: 15/20... Step: 20710... Loss: 1.3330... Val Loss: 1.3239\n",
      "Epoch: 15/20... Step: 20720... Loss: 1.3417... Val Loss: 1.3262\n",
      "Epoch: 15/20... Step: 20730... Loss: 1.3016... Val Loss: 1.3305\n",
      "Epoch: 15/20... Step: 20740... Loss: 1.2873... Val Loss: 1.3284\n",
      "Epoch: 15/20... Step: 20750... Loss: 1.3430... Val Loss: 1.3245\n",
      "Epoch: 15/20... Step: 20760... Loss: 1.3592... Val Loss: 1.3269\n",
      "Epoch: 15/20... Step: 20770... Loss: 1.3117... Val Loss: 1.3279\n",
      "Epoch: 15/20... Step: 20780... Loss: 1.2995... Val Loss: 1.3273\n",
      "Epoch: 15/20... Step: 20790... Loss: 1.3256... Val Loss: 1.3287\n",
      "Epoch: 15/20... Step: 20800... Loss: 1.3653... Val Loss: 1.3274\n",
      "Epoch: 15/20... Step: 20810... Loss: 1.3302... Val Loss: 1.3292\n",
      "Epoch: 15/20... Step: 20820... Loss: 1.3451... Val Loss: 1.3249\n",
      "Epoch: 15/20... Step: 20830... Loss: 1.3947... Val Loss: 1.3252\n",
      "Epoch: 15/20... Step: 20840... Loss: 1.4062... Val Loss: 1.3265\n",
      "Epoch: 15/20... Step: 20850... Loss: 1.2737... Val Loss: 1.3312\n",
      "Epoch: 15/20... Step: 20860... Loss: 1.3237... Val Loss: 1.3277\n",
      "Epoch: 15/20... Step: 20870... Loss: 1.4365... Val Loss: 1.3282\n",
      "Epoch: 15/20... Step: 20880... Loss: 1.4006... Val Loss: 1.3297\n",
      "Epoch: 15/20... Step: 20890... Loss: 1.2874... Val Loss: 1.3311\n",
      "Epoch: 15/20... Step: 20900... Loss: 1.3401... Val Loss: 1.3311\n",
      "Epoch: 15/20... Step: 20910... Loss: 1.3088... Val Loss: 1.3290\n",
      "Epoch: 15/20... Step: 20920... Loss: 1.2976... Val Loss: 1.3283\n",
      "Epoch: 15/20... Step: 20930... Loss: 1.3929... Val Loss: 1.3294\n",
      "Epoch: 15/20... Step: 20940... Loss: 1.3240... Val Loss: 1.3281\n",
      "Epoch: 15/20... Step: 20950... Loss: 1.3576... Val Loss: 1.3242\n",
      "Epoch: 15/20... Step: 20960... Loss: 1.3345... Val Loss: 1.3268\n",
      "Epoch: 15/20... Step: 20970... Loss: 1.3552... Val Loss: 1.3247\n",
      "Epoch: 15/20... Step: 20980... Loss: 1.3320... Val Loss: 1.3232\n",
      "Epoch: 15/20... Step: 20990... Loss: 1.2874... Val Loss: 1.3268\n",
      "Epoch: 15/20... Step: 21000... Loss: 1.4051... Val Loss: 1.3236\n",
      "Epoch: 15/20... Step: 21010... Loss: 1.2967... Val Loss: 1.3216\n",
      "Epoch: 15/20... Step: 21020... Loss: 1.2953... Val Loss: 1.3270\n",
      "Epoch: 15/20... Step: 21030... Loss: 1.3529... Val Loss: 1.3239\n",
      "Epoch: 15/20... Step: 21040... Loss: 1.3788... Val Loss: 1.3238\n",
      "Epoch: 15/20... Step: 21050... Loss: 1.3426... Val Loss: 1.3204\n",
      "Epoch: 15/20... Step: 21060... Loss: 1.4309... Val Loss: 1.3291\n",
      "Epoch: 15/20... Step: 21070... Loss: 1.3573... Val Loss: 1.3282\n",
      "Epoch: 15/20... Step: 21080... Loss: 1.3720... Val Loss: 1.3213\n",
      "Epoch: 15/20... Step: 21090... Loss: 1.3167... Val Loss: 1.3257\n",
      "Epoch: 15/20... Step: 21100... Loss: 1.3343... Val Loss: 1.3239\n",
      "Epoch: 15/20... Step: 21110... Loss: 1.3730... Val Loss: 1.3220\n",
      "Epoch: 15/20... Step: 21120... Loss: 1.2987... Val Loss: 1.3222\n",
      "Epoch: 15/20... Step: 21130... Loss: 1.3667... Val Loss: 1.3292\n",
      "Epoch: 15/20... Step: 21140... Loss: 1.3386... Val Loss: 1.3233\n",
      "Epoch: 15/20... Step: 21150... Loss: 1.2950... Val Loss: 1.3222\n",
      "Epoch: 15/20... Step: 21160... Loss: 1.3151... Val Loss: 1.3244\n",
      "Epoch: 15/20... Step: 21170... Loss: 1.3997... Val Loss: 1.3223\n",
      "Epoch: 15/20... Step: 21180... Loss: 1.3698... Val Loss: 1.3200\n",
      "Epoch: 15/20... Step: 21190... Loss: 1.3516... Val Loss: 1.3219\n",
      "Epoch: 15/20... Step: 21200... Loss: 1.2690... Val Loss: 1.3257\n",
      "Epoch: 15/20... Step: 21210... Loss: 1.3336... Val Loss: 1.3220\n",
      "Epoch: 15/20... Step: 21220... Loss: 1.3365... Val Loss: 1.3230\n",
      "Epoch: 15/20... Step: 21230... Loss: 1.3118... Val Loss: 1.3255\n",
      "Epoch: 15/20... Step: 21240... Loss: 1.3157... Val Loss: 1.3236\n",
      "Epoch: 15/20... Step: 21250... Loss: 1.4014... Val Loss: 1.3253\n",
      "Epoch: 15/20... Step: 21260... Loss: 1.4410... Val Loss: 1.3213\n",
      "Epoch: 15/20... Step: 21270... Loss: 1.5419... Val Loss: 1.3257\n",
      "Epoch: 16/20... Step: 21280... Loss: 1.3782... Val Loss: 1.3199\n",
      "Epoch: 16/20... Step: 21290... Loss: 1.3856... Val Loss: 1.3190\n",
      "Epoch: 16/20... Step: 21300... Loss: 1.3187... Val Loss: 1.3222\n",
      "Epoch: 16/20... Step: 21310... Loss: 1.2152... Val Loss: 1.3212\n",
      "Epoch: 16/20... Step: 21320... Loss: 1.3069... Val Loss: 1.3185\n",
      "Epoch: 16/20... Step: 21330... Loss: 1.3691... Val Loss: 1.3202\n",
      "Epoch: 16/20... Step: 21340... Loss: 1.3347... Val Loss: 1.3207\n",
      "Epoch: 16/20... Step: 21350... Loss: 1.2659... Val Loss: 1.3187\n",
      "Epoch: 16/20... Step: 21360... Loss: 1.2909... Val Loss: 1.3184\n",
      "Epoch: 16/20... Step: 21370... Loss: 1.3417... Val Loss: 1.3231\n",
      "Epoch: 16/20... Step: 21380... Loss: 1.4091... Val Loss: 1.3221\n",
      "Epoch: 16/20... Step: 21390... Loss: 1.2763... Val Loss: 1.3206\n",
      "Epoch: 16/20... Step: 21400... Loss: 1.4249... Val Loss: 1.3158\n",
      "Epoch: 16/20... Step: 21410... Loss: 1.3452... Val Loss: 1.3197\n",
      "Epoch: 16/20... Step: 21420... Loss: 1.2633... Val Loss: 1.3212\n",
      "Epoch: 16/20... Step: 21430... Loss: 1.3153... Val Loss: 1.3217\n",
      "Epoch: 16/20... Step: 21440... Loss: 1.2521... Val Loss: 1.3193\n",
      "Epoch: 16/20... Step: 21450... Loss: 1.3154... Val Loss: 1.3239\n",
      "Epoch: 16/20... Step: 21460... Loss: 1.3387... Val Loss: 1.3230\n",
      "Epoch: 16/20... Step: 21470... Loss: 1.2724... Val Loss: 1.3216\n",
      "Epoch: 16/20... Step: 21480... Loss: 1.2916... Val Loss: 1.3264\n",
      "Epoch: 16/20... Step: 21490... Loss: 1.3149... Val Loss: 1.3231\n",
      "Epoch: 16/20... Step: 21500... Loss: 1.3469... Val Loss: 1.3219\n",
      "Epoch: 16/20... Step: 21510... Loss: 1.3711... Val Loss: 1.3238\n",
      "Epoch: 16/20... Step: 21520... Loss: 1.3481... Val Loss: 1.3214\n",
      "Epoch: 16/20... Step: 21530... Loss: 1.3182... Val Loss: 1.3210\n",
      "Epoch: 16/20... Step: 21540... Loss: 1.2978... Val Loss: 1.3245\n",
      "Epoch: 16/20... Step: 21550... Loss: 1.3068... Val Loss: 1.3238\n",
      "Epoch: 16/20... Step: 21560... Loss: 1.2868... Val Loss: 1.3234\n",
      "Epoch: 16/20... Step: 21570... Loss: 1.3457... Val Loss: 1.3232\n",
      "Epoch: 16/20... Step: 21580... Loss: 1.3487... Val Loss: 1.3242\n",
      "Epoch: 16/20... Step: 21590... Loss: 1.3440... Val Loss: 1.3230\n",
      "Epoch: 16/20... Step: 21600... Loss: 1.3467... Val Loss: 1.3221\n",
      "Epoch: 16/20... Step: 21610... Loss: 1.3304... Val Loss: 1.3190\n",
      "Epoch: 16/20... Step: 21620... Loss: 1.3618... Val Loss: 1.3225\n",
      "Epoch: 16/20... Step: 21630... Loss: 1.3441... Val Loss: 1.3224\n",
      "Epoch: 16/20... Step: 21640... Loss: 1.3044... Val Loss: 1.3194\n",
      "Epoch: 16/20... Step: 21650... Loss: 1.4077... Val Loss: 1.3207\n",
      "Epoch: 16/20... Step: 21660... Loss: 1.3756... Val Loss: 1.3224\n",
      "Epoch: 16/20... Step: 21670... Loss: 1.3358... Val Loss: 1.3196\n",
      "Epoch: 16/20... Step: 21680... Loss: 1.3224... Val Loss: 1.3233\n",
      "Epoch: 16/20... Step: 21690... Loss: 1.3178... Val Loss: 1.3232\n",
      "Epoch: 16/20... Step: 21700... Loss: 1.4098... Val Loss: 1.3227\n",
      "Epoch: 16/20... Step: 21710... Loss: 1.3463... Val Loss: 1.3211\n",
      "Epoch: 16/20... Step: 21720... Loss: 1.3607... Val Loss: 1.3223\n",
      "Epoch: 16/20... Step: 21730... Loss: 1.3426... Val Loss: 1.3237\n",
      "Epoch: 16/20... Step: 21740... Loss: 1.3374... Val Loss: 1.3229\n",
      "Epoch: 16/20... Step: 21750... Loss: 1.3637... Val Loss: 1.3243\n",
      "Epoch: 16/20... Step: 21760... Loss: 1.3792... Val Loss: 1.3242\n",
      "Epoch: 16/20... Step: 21770... Loss: 1.3820... Val Loss: 1.3226\n",
      "Epoch: 16/20... Step: 21780... Loss: 1.3005... Val Loss: 1.3223\n",
      "Epoch: 16/20... Step: 21790... Loss: 1.3628... Val Loss: 1.3191\n",
      "Epoch: 16/20... Step: 21800... Loss: 1.3361... Val Loss: 1.3207\n",
      "Epoch: 16/20... Step: 21810... Loss: 1.3615... Val Loss: 1.3195\n",
      "Epoch: 16/20... Step: 21820... Loss: 1.3641... Val Loss: 1.3239\n",
      "Epoch: 16/20... Step: 21830... Loss: 1.3416... Val Loss: 1.3209\n",
      "Epoch: 16/20... Step: 21840... Loss: 1.3478... Val Loss: 1.3208\n",
      "Epoch: 16/20... Step: 21850... Loss: 1.3820... Val Loss: 1.3210\n",
      "Epoch: 16/20... Step: 21860... Loss: 1.2820... Val Loss: 1.3200\n",
      "Epoch: 16/20... Step: 21870... Loss: 1.3261... Val Loss: 1.3280\n",
      "Epoch: 16/20... Step: 21880... Loss: 1.3191... Val Loss: 1.3246\n",
      "Epoch: 16/20... Step: 21890... Loss: 1.4458... Val Loss: 1.3190\n",
      "Epoch: 16/20... Step: 21900... Loss: 1.3413... Val Loss: 1.3215\n",
      "Epoch: 16/20... Step: 21910... Loss: 1.3818... Val Loss: 1.3224\n",
      "Epoch: 16/20... Step: 21920... Loss: 1.3638... Val Loss: 1.3208\n",
      "Epoch: 16/20... Step: 21930... Loss: 1.3148... Val Loss: 1.3229\n",
      "Epoch: 16/20... Step: 21940... Loss: 1.3299... Val Loss: 1.3209\n",
      "Epoch: 16/20... Step: 21950... Loss: 1.2073... Val Loss: 1.3249\n",
      "Epoch: 16/20... Step: 21960... Loss: 1.3719... Val Loss: 1.3227\n",
      "Epoch: 16/20... Step: 21970... Loss: 1.3541... Val Loss: 1.3244\n",
      "Epoch: 16/20... Step: 21980... Loss: 1.3073... Val Loss: 1.3231\n",
      "Epoch: 16/20... Step: 21990... Loss: 1.3379... Val Loss: 1.3198\n",
      "Epoch: 16/20... Step: 22000... Loss: 1.3496... Val Loss: 1.3205\n",
      "Epoch: 16/20... Step: 22010... Loss: 1.3888... Val Loss: 1.3230\n",
      "Epoch: 16/20... Step: 22020... Loss: 1.3311... Val Loss: 1.3207\n",
      "Epoch: 16/20... Step: 22030... Loss: 1.3892... Val Loss: 1.3188\n",
      "Epoch: 16/20... Step: 22040... Loss: 1.3659... Val Loss: 1.3225\n",
      "Epoch: 16/20... Step: 22050... Loss: 1.2946... Val Loss: 1.3210\n",
      "Epoch: 16/20... Step: 22060... Loss: 1.4317... Val Loss: 1.3206\n",
      "Epoch: 16/20... Step: 22070... Loss: 1.3949... Val Loss: 1.3217\n",
      "Epoch: 16/20... Step: 22080... Loss: 1.2913... Val Loss: 1.3220\n",
      "Epoch: 16/20... Step: 22090... Loss: 1.3284... Val Loss: 1.3235\n",
      "Epoch: 16/20... Step: 22100... Loss: 1.4047... Val Loss: 1.3248\n",
      "Epoch: 16/20... Step: 22110... Loss: 1.3801... Val Loss: 1.3214\n",
      "Epoch: 16/20... Step: 22120... Loss: 1.2828... Val Loss: 1.3225\n",
      "Epoch: 16/20... Step: 22130... Loss: 1.3620... Val Loss: 1.3190\n",
      "Epoch: 16/20... Step: 22140... Loss: 1.3427... Val Loss: 1.3191\n",
      "Epoch: 16/20... Step: 22150... Loss: 1.2822... Val Loss: 1.3236\n",
      "Epoch: 16/20... Step: 22160... Loss: 1.2697... Val Loss: 1.3232\n",
      "Epoch: 16/20... Step: 22170... Loss: 1.2908... Val Loss: 1.3207\n",
      "Epoch: 16/20... Step: 22180... Loss: 1.3602... Val Loss: 1.3226\n",
      "Epoch: 16/20... Step: 22190... Loss: 1.2907... Val Loss: 1.3202\n",
      "Epoch: 16/20... Step: 22200... Loss: 1.2647... Val Loss: 1.3221\n",
      "Epoch: 16/20... Step: 22210... Loss: 1.3914... Val Loss: 1.3252\n",
      "Epoch: 16/20... Step: 22220... Loss: 1.3156... Val Loss: 1.3233\n",
      "Epoch: 16/20... Step: 22230... Loss: 1.3395... Val Loss: 1.3226\n",
      "Epoch: 16/20... Step: 22240... Loss: 1.3958... Val Loss: 1.3185\n",
      "Epoch: 16/20... Step: 22250... Loss: 1.3074... Val Loss: 1.3212\n",
      "Epoch: 16/20... Step: 22260... Loss: 1.3137... Val Loss: 1.3203\n",
      "Epoch: 16/20... Step: 22270... Loss: 1.3029... Val Loss: 1.3201\n",
      "Epoch: 16/20... Step: 22280... Loss: 1.3542... Val Loss: 1.3236\n",
      "Epoch: 16/20... Step: 22290... Loss: 1.3305... Val Loss: 1.3207\n",
      "Epoch: 16/20... Step: 22300... Loss: 1.4026... Val Loss: 1.3220\n",
      "Epoch: 16/20... Step: 22310... Loss: 1.3487... Val Loss: 1.3238\n",
      "Epoch: 16/20... Step: 22320... Loss: 1.3219... Val Loss: 1.3208\n",
      "Epoch: 16/20... Step: 22330... Loss: 1.4208... Val Loss: 1.3225\n",
      "Epoch: 16/20... Step: 22340... Loss: 1.3325... Val Loss: 1.3203\n",
      "Epoch: 16/20... Step: 22350... Loss: 1.3266... Val Loss: 1.3213\n",
      "Epoch: 16/20... Step: 22360... Loss: 1.3492... Val Loss: 1.3195\n",
      "Epoch: 16/20... Step: 22370... Loss: 1.2795... Val Loss: 1.3201\n",
      "Epoch: 16/20... Step: 22380... Loss: 1.3379... Val Loss: 1.3188\n",
      "Epoch: 16/20... Step: 22390... Loss: 1.3524... Val Loss: 1.3184\n",
      "Epoch: 16/20... Step: 22400... Loss: 1.3640... Val Loss: 1.3211\n",
      "Epoch: 16/20... Step: 22410... Loss: 1.3019... Val Loss: 1.3195\n",
      "Epoch: 16/20... Step: 22420... Loss: 1.3577... Val Loss: 1.3169\n",
      "Epoch: 16/20... Step: 22430... Loss: 1.3436... Val Loss: 1.3172\n",
      "Epoch: 16/20... Step: 22440... Loss: 1.3339... Val Loss: 1.3190\n",
      "Epoch: 16/20... Step: 22450... Loss: 1.2948... Val Loss: 1.3233\n",
      "Epoch: 16/20... Step: 22460... Loss: 1.3387... Val Loss: 1.3188\n",
      "Epoch: 16/20... Step: 22470... Loss: 1.3051... Val Loss: 1.3172\n",
      "Epoch: 16/20... Step: 22480... Loss: 1.3243... Val Loss: 1.3193\n",
      "Epoch: 16/20... Step: 22490... Loss: 1.3111... Val Loss: 1.3238\n",
      "Epoch: 16/20... Step: 22500... Loss: 1.2762... Val Loss: 1.3158\n",
      "Epoch: 16/20... Step: 22510... Loss: 1.3345... Val Loss: 1.3176\n",
      "Epoch: 16/20... Step: 22520... Loss: 1.3141... Val Loss: 1.3210\n",
      "Epoch: 16/20... Step: 22530... Loss: 1.3307... Val Loss: 1.3169\n",
      "Epoch: 16/20... Step: 22540... Loss: 1.3586... Val Loss: 1.3168\n",
      "Epoch: 16/20... Step: 22550... Loss: 1.4244... Val Loss: 1.3222\n",
      "Epoch: 16/20... Step: 22560... Loss: 1.3474... Val Loss: 1.3203\n",
      "Epoch: 16/20... Step: 22570... Loss: 1.3835... Val Loss: 1.3181\n",
      "Epoch: 16/20... Step: 22580... Loss: 1.3436... Val Loss: 1.3181\n",
      "Epoch: 16/20... Step: 22590... Loss: 1.3741... Val Loss: 1.3175\n",
      "Epoch: 16/20... Step: 22600... Loss: 1.2778... Val Loss: 1.3172\n",
      "Epoch: 16/20... Step: 22610... Loss: 1.3875... Val Loss: 1.3174\n",
      "Epoch: 16/20... Step: 22620... Loss: 1.3321... Val Loss: 1.3164\n",
      "Epoch: 16/20... Step: 22630... Loss: 1.3190... Val Loss: 1.3163\n",
      "Epoch: 16/20... Step: 22640... Loss: 1.3741... Val Loss: 1.3189\n",
      "Epoch: 16/20... Step: 22650... Loss: 1.2427... Val Loss: 1.3180\n",
      "Epoch: 16/20... Step: 22660... Loss: 1.3908... Val Loss: 1.3170\n",
      "Epoch: 16/20... Step: 22670... Loss: 1.3695... Val Loss: 1.3208\n",
      "Epoch: 16/20... Step: 22680... Loss: 1.3219... Val Loss: 1.3169\n",
      "Epoch: 17/20... Step: 22690... Loss: 1.3953... Val Loss: 1.3171\n",
      "Epoch: 17/20... Step: 22700... Loss: 1.4170... Val Loss: 1.3214\n",
      "Epoch: 17/20... Step: 22710... Loss: 1.3276... Val Loss: 1.3127\n",
      "Epoch: 17/20... Step: 22720... Loss: 1.3096... Val Loss: 1.3194\n",
      "Epoch: 17/20... Step: 22730... Loss: 1.2778... Val Loss: 1.3186\n",
      "Epoch: 17/20... Step: 22740... Loss: 1.2887... Val Loss: 1.3150\n",
      "Epoch: 17/20... Step: 22750... Loss: 1.3539... Val Loss: 1.3151\n",
      "Epoch: 17/20... Step: 22760... Loss: 1.2878... Val Loss: 1.3169\n",
      "Epoch: 17/20... Step: 22770... Loss: 1.4324... Val Loss: 1.3143\n",
      "Epoch: 17/20... Step: 22780... Loss: 1.2971... Val Loss: 1.3147\n",
      "Epoch: 17/20... Step: 22790... Loss: 1.3157... Val Loss: 1.3168\n",
      "Epoch: 17/20... Step: 22800... Loss: 1.3184... Val Loss: 1.3167\n",
      "Epoch: 17/20... Step: 22810... Loss: 1.2950... Val Loss: 1.3139\n",
      "Epoch: 17/20... Step: 22820... Loss: 1.2630... Val Loss: 1.3136\n",
      "Epoch: 17/20... Step: 22830... Loss: 1.2729... Val Loss: 1.3169\n",
      "Epoch: 17/20... Step: 22840... Loss: 1.2460... Val Loss: 1.3157\n",
      "Epoch: 17/20... Step: 22850... Loss: 1.3340... Val Loss: 1.3173\n",
      "Epoch: 17/20... Step: 22860... Loss: 1.2964... Val Loss: 1.3161\n",
      "Epoch: 17/20... Step: 22870... Loss: 1.3820... Val Loss: 1.3176\n",
      "Epoch: 17/20... Step: 22880... Loss: 1.3166... Val Loss: 1.3218\n",
      "Epoch: 17/20... Step: 22890... Loss: 1.2638... Val Loss: 1.3170\n",
      "Epoch: 17/20... Step: 22900... Loss: 1.3417... Val Loss: 1.3208\n",
      "Epoch: 17/20... Step: 22910... Loss: 1.2588... Val Loss: 1.3177\n",
      "Epoch: 17/20... Step: 22920... Loss: 1.3158... Val Loss: 1.3173\n",
      "Epoch: 17/20... Step: 22930... Loss: 1.3473... Val Loss: 1.3207\n",
      "Epoch: 17/20... Step: 22940... Loss: 1.3091... Val Loss: 1.3190\n",
      "Epoch: 17/20... Step: 22950... Loss: 1.2905... Val Loss: 1.3180\n",
      "Epoch: 17/20... Step: 22960... Loss: 1.2821... Val Loss: 1.3168\n",
      "Epoch: 17/20... Step: 22970... Loss: 1.2971... Val Loss: 1.3206\n",
      "Epoch: 17/20... Step: 22980... Loss: 1.3087... Val Loss: 1.3179\n",
      "Epoch: 17/20... Step: 22990... Loss: 1.2730... Val Loss: 1.3196\n",
      "Epoch: 17/20... Step: 23000... Loss: 1.3541... Val Loss: 1.3180\n",
      "Epoch: 17/20... Step: 23010... Loss: 1.3132... Val Loss: 1.3181\n",
      "Epoch: 17/20... Step: 23020... Loss: 1.3660... Val Loss: 1.3187\n",
      "Epoch: 17/20... Step: 23030... Loss: 1.3964... Val Loss: 1.3166\n",
      "Epoch: 17/20... Step: 23040... Loss: 1.3479... Val Loss: 1.3155\n",
      "Epoch: 17/20... Step: 23050... Loss: 1.3868... Val Loss: 1.3177\n",
      "Epoch: 17/20... Step: 23060... Loss: 1.3466... Val Loss: 1.3171\n",
      "Epoch: 17/20... Step: 23070... Loss: 1.3561... Val Loss: 1.3140\n",
      "Epoch: 17/20... Step: 23080... Loss: 1.3145... Val Loss: 1.3211\n",
      "Epoch: 17/20... Step: 23090... Loss: 1.2875... Val Loss: 1.3140\n",
      "Epoch: 17/20... Step: 23100... Loss: 1.3097... Val Loss: 1.3171\n",
      "Epoch: 17/20... Step: 23110... Loss: 1.3698... Val Loss: 1.3206\n",
      "Epoch: 17/20... Step: 23120... Loss: 1.3413... Val Loss: 1.3147\n",
      "Epoch: 17/20... Step: 23130... Loss: 1.3556... Val Loss: 1.3186\n",
      "Epoch: 17/20... Step: 23140... Loss: 1.3485... Val Loss: 1.3162\n",
      "Epoch: 17/20... Step: 23150... Loss: 1.2779... Val Loss: 1.3171\n",
      "Epoch: 17/20... Step: 23160... Loss: 1.3231... Val Loss: 1.3195\n",
      "Epoch: 17/20... Step: 23170... Loss: 1.3049... Val Loss: 1.3183\n",
      "Epoch: 17/20... Step: 23180... Loss: 1.3671... Val Loss: 1.3201\n",
      "Epoch: 17/20... Step: 23190... Loss: 1.3018... Val Loss: 1.3187\n",
      "Epoch: 17/20... Step: 23200... Loss: 1.3341... Val Loss: 1.3165\n",
      "Epoch: 17/20... Step: 23210... Loss: 1.3844... Val Loss: 1.3210\n",
      "Epoch: 17/20... Step: 23220... Loss: 1.3519... Val Loss: 1.3182\n",
      "Epoch: 17/20... Step: 23230... Loss: 1.4600... Val Loss: 1.3135\n",
      "Epoch: 17/20... Step: 23240... Loss: 1.2614... Val Loss: 1.3225\n",
      "Epoch: 17/20... Step: 23250... Loss: 1.3471... Val Loss: 1.3175\n",
      "Epoch: 17/20... Step: 23260... Loss: 1.3162... Val Loss: 1.3179\n",
      "Epoch: 17/20... Step: 23270... Loss: 1.3921... Val Loss: 1.3165\n",
      "Epoch: 17/20... Step: 23280... Loss: 1.3073... Val Loss: 1.3168\n",
      "Epoch: 17/20... Step: 23290... Loss: 1.2563... Val Loss: 1.3230\n",
      "Epoch: 17/20... Step: 23300... Loss: 1.3392... Val Loss: 1.3177\n",
      "Epoch: 17/20... Step: 23310... Loss: 1.2777... Val Loss: 1.3141\n",
      "Epoch: 17/20... Step: 23320... Loss: 1.2986... Val Loss: 1.3191\n",
      "Epoch: 17/20... Step: 23330... Loss: 1.3338... Val Loss: 1.3207\n",
      "Epoch: 17/20... Step: 23340... Loss: 1.3536... Val Loss: 1.3176\n",
      "Epoch: 17/20... Step: 23350... Loss: 1.3490... Val Loss: 1.3191\n",
      "Epoch: 17/20... Step: 23360... Loss: 1.3198... Val Loss: 1.3177\n",
      "Epoch: 17/20... Step: 23370... Loss: 1.2391... Val Loss: 1.3211\n",
      "Epoch: 17/20... Step: 23380... Loss: 1.3006... Val Loss: 1.3167\n",
      "Epoch: 17/20... Step: 23390... Loss: 1.3242... Val Loss: 1.3181\n",
      "Epoch: 17/20... Step: 23400... Loss: 1.2664... Val Loss: 1.3176\n",
      "Epoch: 17/20... Step: 23410... Loss: 1.3937... Val Loss: 1.3151\n",
      "Epoch: 17/20... Step: 23420... Loss: 1.2903... Val Loss: 1.3155\n",
      "Epoch: 17/20... Step: 23430... Loss: 1.3877... Val Loss: 1.3183\n",
      "Epoch: 17/20... Step: 23440... Loss: 1.3807... Val Loss: 1.3164\n",
      "Epoch: 17/20... Step: 23450... Loss: 1.3468... Val Loss: 1.3165\n",
      "Epoch: 17/20... Step: 23460... Loss: 1.3839... Val Loss: 1.3173\n",
      "Epoch: 17/20... Step: 23470... Loss: 1.3813... Val Loss: 1.3193\n",
      "Epoch: 17/20... Step: 23480... Loss: 1.3445... Val Loss: 1.3165\n",
      "Epoch: 17/20... Step: 23490... Loss: 1.3818... Val Loss: 1.3173\n",
      "Epoch: 17/20... Step: 23500... Loss: 1.3117... Val Loss: 1.3221\n",
      "Epoch: 17/20... Step: 23510... Loss: 1.3173... Val Loss: 1.3197\n",
      "Epoch: 17/20... Step: 23520... Loss: 1.2980... Val Loss: 1.3152\n",
      "Epoch: 17/20... Step: 23530... Loss: 1.3372... Val Loss: 1.3168\n",
      "Epoch: 17/20... Step: 23540... Loss: 1.2988... Val Loss: 1.3173\n",
      "Epoch: 17/20... Step: 23550... Loss: 1.3276... Val Loss: 1.3164\n",
      "Epoch: 17/20... Step: 23560... Loss: 1.3288... Val Loss: 1.3174\n",
      "Epoch: 17/20... Step: 23570... Loss: 1.3052... Val Loss: 1.3176\n",
      "Epoch: 17/20... Step: 23580... Loss: 1.3215... Val Loss: 1.3191\n",
      "Epoch: 17/20... Step: 23590... Loss: 1.3796... Val Loss: 1.3164\n",
      "Epoch: 17/20... Step: 23600... Loss: 1.3569... Val Loss: 1.3157\n",
      "Epoch: 17/20... Step: 23610... Loss: 1.3665... Val Loss: 1.3149\n",
      "Epoch: 17/20... Step: 23620... Loss: 1.3118... Val Loss: 1.3198\n",
      "Epoch: 17/20... Step: 23630... Loss: 1.3083... Val Loss: 1.3181\n",
      "Epoch: 17/20... Step: 23640... Loss: 1.2806... Val Loss: 1.3192\n",
      "Epoch: 17/20... Step: 23650... Loss: 1.3328... Val Loss: 1.3192\n",
      "Epoch: 17/20... Step: 23660... Loss: 1.2708... Val Loss: 1.3155\n",
      "Epoch: 17/20... Step: 23670... Loss: 1.3501... Val Loss: 1.3157\n",
      "Epoch: 17/20... Step: 23680... Loss: 1.3002... Val Loss: 1.3148\n",
      "Epoch: 17/20... Step: 23690... Loss: 1.3441... Val Loss: 1.3170\n",
      "Epoch: 17/20... Step: 23700... Loss: 1.2643... Val Loss: 1.3175\n",
      "Epoch: 17/20... Step: 23710... Loss: 1.2911... Val Loss: 1.3195\n",
      "Epoch: 17/20... Step: 23720... Loss: 1.2607... Val Loss: 1.3167\n",
      "Epoch: 17/20... Step: 23730... Loss: 1.3768... Val Loss: 1.3184\n",
      "Epoch: 17/20... Step: 23740... Loss: 1.3443... Val Loss: 1.3190\n",
      "Epoch: 17/20... Step: 23750... Loss: 1.3162... Val Loss: 1.3166\n",
      "Epoch: 17/20... Step: 23760... Loss: 1.3330... Val Loss: 1.3148\n",
      "Epoch: 17/20... Step: 23770... Loss: 1.3478... Val Loss: 1.3161\n",
      "Epoch: 17/20... Step: 23780... Loss: 1.3063... Val Loss: 1.3164\n",
      "Epoch: 17/20... Step: 23790... Loss: 1.3263... Val Loss: 1.3127\n",
      "Epoch: 17/20... Step: 23800... Loss: 1.3704... Val Loss: 1.3140\n",
      "Epoch: 17/20... Step: 23810... Loss: 1.3131... Val Loss: 1.3112\n",
      "Epoch: 17/20... Step: 23820... Loss: 1.3418... Val Loss: 1.3148\n",
      "Epoch: 17/20... Step: 23830... Loss: 1.3060... Val Loss: 1.3128\n",
      "Epoch: 17/20... Step: 23840... Loss: 1.3603... Val Loss: 1.3115\n",
      "Epoch: 17/20... Step: 23850... Loss: 1.3988... Val Loss: 1.3123\n",
      "Epoch: 17/20... Step: 23860... Loss: 1.2914... Val Loss: 1.3175\n",
      "Epoch: 17/20... Step: 23870... Loss: 1.3269... Val Loss: 1.3129\n",
      "Epoch: 17/20... Step: 23880... Loss: 1.2681... Val Loss: 1.3125\n",
      "Epoch: 17/20... Step: 23890... Loss: 1.2851... Val Loss: 1.3114\n",
      "Epoch: 17/20... Step: 23900... Loss: 1.2999... Val Loss: 1.3200\n",
      "Epoch: 17/20... Step: 23910... Loss: 1.3587... Val Loss: 1.3166\n",
      "Epoch: 17/20... Step: 23920... Loss: 1.2994... Val Loss: 1.3119\n",
      "Epoch: 17/20... Step: 23930... Loss: 1.3286... Val Loss: 1.3162\n",
      "Epoch: 17/20... Step: 23940... Loss: 1.2729... Val Loss: 1.3162\n",
      "Epoch: 17/20... Step: 23950... Loss: 1.3172... Val Loss: 1.3136\n",
      "Epoch: 17/20... Step: 23960... Loss: 1.3170... Val Loss: 1.3173\n",
      "Epoch: 17/20... Step: 23970... Loss: 1.3080... Val Loss: 1.3170\n",
      "Epoch: 17/20... Step: 23980... Loss: 1.3781... Val Loss: 1.3134\n",
      "Epoch: 17/20... Step: 23990... Loss: 1.3115... Val Loss: 1.3132\n",
      "Epoch: 17/20... Step: 24000... Loss: 1.4042... Val Loss: 1.3152\n",
      "Epoch: 17/20... Step: 24010... Loss: 1.2513... Val Loss: 1.3135\n",
      "Epoch: 17/20... Step: 24020... Loss: 1.3142... Val Loss: 1.3124\n",
      "Epoch: 17/20... Step: 24030... Loss: 1.3045... Val Loss: 1.3148\n",
      "Epoch: 17/20... Step: 24040... Loss: 1.3998... Val Loss: 1.3139\n",
      "Epoch: 17/20... Step: 24050... Loss: 1.3364... Val Loss: 1.3114\n",
      "Epoch: 17/20... Step: 24060... Loss: 1.3023... Val Loss: 1.3148\n",
      "Epoch: 17/20... Step: 24070... Loss: 1.3485... Val Loss: 1.3135\n",
      "Epoch: 17/20... Step: 24080... Loss: 1.3202... Val Loss: 1.3115\n",
      "Epoch: 17/20... Step: 24090... Loss: 1.3262... Val Loss: 1.3125\n",
      "Epoch: 17/20... Step: 24100... Loss: 1.3518... Val Loss: 1.3126\n",
      "Epoch: 18/20... Step: 24110... Loss: 1.3930... Val Loss: 1.3144\n",
      "Epoch: 18/20... Step: 24120... Loss: 1.3392... Val Loss: 1.3094\n",
      "Epoch: 18/20... Step: 24130... Loss: 1.2900... Val Loss: 1.3102\n",
      "Epoch: 18/20... Step: 24140... Loss: 1.2420... Val Loss: 1.3102\n",
      "Epoch: 18/20... Step: 24150... Loss: 1.3803... Val Loss: 1.3114\n",
      "Epoch: 18/20... Step: 24160... Loss: 1.3440... Val Loss: 1.3098\n",
      "Epoch: 18/20... Step: 24170... Loss: 1.2856... Val Loss: 1.3092\n",
      "Epoch: 18/20... Step: 24180... Loss: 1.2782... Val Loss: 1.3112\n",
      "Epoch: 18/20... Step: 24190... Loss: 1.2827... Val Loss: 1.3077\n",
      "Epoch: 18/20... Step: 24200... Loss: 1.3152... Val Loss: 1.3120\n",
      "Epoch: 18/20... Step: 24210... Loss: 1.2619... Val Loss: 1.3101\n",
      "Epoch: 18/20... Step: 24220... Loss: 1.3513... Val Loss: 1.3136\n",
      "Epoch: 18/20... Step: 24230... Loss: 1.3409... Val Loss: 1.3099\n",
      "Epoch: 18/20... Step: 24240... Loss: 1.2961... Val Loss: 1.3097\n",
      "Epoch: 18/20... Step: 24250... Loss: 1.2619... Val Loss: 1.3119\n",
      "Epoch: 18/20... Step: 24260... Loss: 1.3072... Val Loss: 1.3101\n",
      "Epoch: 18/20... Step: 24270... Loss: 1.3464... Val Loss: 1.3104\n",
      "Epoch: 18/20... Step: 24280... Loss: 1.2944... Val Loss: 1.3130\n",
      "Epoch: 18/20... Step: 24290... Loss: 1.3012... Val Loss: 1.3141\n",
      "Epoch: 18/20... Step: 24300... Loss: 1.3111... Val Loss: 1.3137\n",
      "Epoch: 18/20... Step: 24310... Loss: 1.2447... Val Loss: 1.3119\n",
      "Epoch: 18/20... Step: 24320... Loss: 1.3232... Val Loss: 1.3151\n",
      "Epoch: 18/20... Step: 24330... Loss: 1.3406... Val Loss: 1.3156\n",
      "Epoch: 18/20... Step: 24340... Loss: 1.2991... Val Loss: 1.3125\n",
      "Epoch: 18/20... Step: 24350... Loss: 1.2850... Val Loss: 1.3152\n",
      "Epoch: 18/20... Step: 24360... Loss: 1.3463... Val Loss: 1.3146\n",
      "Epoch: 18/20... Step: 24370... Loss: 1.3132... Val Loss: 1.3139\n",
      "Epoch: 18/20... Step: 24380... Loss: 1.2578... Val Loss: 1.3129\n",
      "Epoch: 18/20... Step: 24390... Loss: 1.3354... Val Loss: 1.3148\n",
      "Epoch: 18/20... Step: 24400... Loss: 1.2951... Val Loss: 1.3162\n",
      "Epoch: 18/20... Step: 24410... Loss: 1.3329... Val Loss: 1.3151\n",
      "Epoch: 18/20... Step: 24420... Loss: 1.3583... Val Loss: 1.3132\n",
      "Epoch: 18/20... Step: 24430... Loss: 1.3667... Val Loss: 1.3142\n",
      "Epoch: 18/20... Step: 24440... Loss: 1.3481... Val Loss: 1.3131\n",
      "Epoch: 18/20... Step: 24450... Loss: 1.3760... Val Loss: 1.3121\n",
      "Epoch: 18/20... Step: 24460... Loss: 1.3542... Val Loss: 1.3128\n",
      "Epoch: 18/20... Step: 24470... Loss: 1.3209... Val Loss: 1.3128\n",
      "Epoch: 18/20... Step: 24480... Loss: 1.3216... Val Loss: 1.3132\n",
      "Epoch: 18/20... Step: 24490... Loss: 1.3123... Val Loss: 1.3110\n",
      "Epoch: 18/20... Step: 24500... Loss: 1.3511... Val Loss: 1.3130\n",
      "Epoch: 18/20... Step: 24510... Loss: 1.2928... Val Loss: 1.3131\n",
      "Epoch: 18/20... Step: 24520... Loss: 1.3145... Val Loss: 1.3161\n",
      "Epoch: 18/20... Step: 24530... Loss: 1.3074... Val Loss: 1.3154\n",
      "Epoch: 18/20... Step: 24540... Loss: 1.4278... Val Loss: 1.3105\n",
      "Epoch: 18/20... Step: 24550... Loss: 1.2954... Val Loss: 1.3159\n",
      "Epoch: 18/20... Step: 24560... Loss: 1.2918... Val Loss: 1.3134\n",
      "Epoch: 18/20... Step: 24570... Loss: 1.3571... Val Loss: 1.3162\n",
      "Epoch: 18/20... Step: 24580... Loss: 1.2949... Val Loss: 1.3134\n",
      "Epoch: 18/20... Step: 24590... Loss: 1.3254... Val Loss: 1.3151\n",
      "Epoch: 18/20... Step: 24600... Loss: 1.2699... Val Loss: 1.3147\n",
      "Epoch: 18/20... Step: 24610... Loss: 1.3001... Val Loss: 1.3140\n",
      "Epoch: 18/20... Step: 24620... Loss: 1.3419... Val Loss: 1.3138\n",
      "Epoch: 18/20... Step: 24630... Loss: 1.4458... Val Loss: 1.3156\n",
      "Epoch: 18/20... Step: 24640... Loss: 1.3304... Val Loss: 1.3128\n",
      "Epoch: 18/20... Step: 24650... Loss: 1.2939... Val Loss: 1.3114\n",
      "Epoch: 18/20... Step: 24660... Loss: 1.3422... Val Loss: 1.3180\n",
      "Epoch: 18/20... Step: 24670... Loss: 1.4176... Val Loss: 1.3115\n",
      "Epoch: 18/20... Step: 24680... Loss: 1.3601... Val Loss: 1.3115\n",
      "Epoch: 18/20... Step: 24690... Loss: 1.4055... Val Loss: 1.3132\n",
      "Epoch: 18/20... Step: 24700... Loss: 1.3878... Val Loss: 1.3157\n",
      "Epoch: 18/20... Step: 24710... Loss: 1.3166... Val Loss: 1.3207\n",
      "Epoch: 18/20... Step: 24720... Loss: 1.3242... Val Loss: 1.3137\n",
      "Epoch: 18/20... Step: 24730... Loss: 1.3923... Val Loss: 1.3112\n",
      "Epoch: 18/20... Step: 24740... Loss: 1.3409... Val Loss: 1.3154\n",
      "Epoch: 18/20... Step: 24750... Loss: 1.3316... Val Loss: 1.3131\n",
      "Epoch: 18/20... Step: 24760... Loss: 1.3360... Val Loss: 1.3156\n",
      "Epoch: 18/20... Step: 24770... Loss: 1.3598... Val Loss: 1.3153\n",
      "Epoch: 18/20... Step: 24780... Loss: 1.3215... Val Loss: 1.3133\n",
      "Epoch: 18/20... Step: 24790... Loss: 1.2187... Val Loss: 1.3130\n",
      "Epoch: 18/20... Step: 24800... Loss: 1.2729... Val Loss: 1.3168\n",
      "Epoch: 18/20... Step: 24810... Loss: 1.2968... Val Loss: 1.3157\n",
      "Epoch: 18/20... Step: 24820... Loss: 1.2922... Val Loss: 1.3124\n",
      "Epoch: 18/20... Step: 24830... Loss: 1.3807... Val Loss: 1.3115\n",
      "Epoch: 18/20... Step: 24840... Loss: 1.3044... Val Loss: 1.3108\n",
      "Epoch: 18/20... Step: 24850... Loss: 1.4122... Val Loss: 1.3139\n",
      "Epoch: 18/20... Step: 24860... Loss: 1.3444... Val Loss: 1.3101\n",
      "Epoch: 18/20... Step: 24870... Loss: 1.2670... Val Loss: 1.3100\n",
      "Epoch: 18/20... Step: 24880... Loss: 1.3875... Val Loss: 1.3105\n",
      "Epoch: 18/20... Step: 24890... Loss: 1.3552... Val Loss: 1.3109\n",
      "Epoch: 18/20... Step: 24900... Loss: 1.3337... Val Loss: 1.3144\n",
      "Epoch: 18/20... Step: 24910... Loss: 1.3818... Val Loss: 1.3121\n",
      "Epoch: 18/20... Step: 24920... Loss: 1.3958... Val Loss: 1.3141\n",
      "Epoch: 18/20... Step: 24930... Loss: 1.3269... Val Loss: 1.3131\n",
      "Epoch: 18/20... Step: 24940... Loss: 1.3808... Val Loss: 1.3138\n",
      "Epoch: 18/20... Step: 24950... Loss: 1.2991... Val Loss: 1.3117\n",
      "Epoch: 18/20... Step: 24960... Loss: 1.3414... Val Loss: 1.3128\n",
      "Epoch: 18/20... Step: 24970... Loss: 1.3150... Val Loss: 1.3123\n",
      "Epoch: 18/20... Step: 24980... Loss: 1.3244... Val Loss: 1.3125\n",
      "Epoch: 18/20... Step: 24990... Loss: 1.3169... Val Loss: 1.3181\n",
      "Epoch: 18/20... Step: 25000... Loss: 1.3374... Val Loss: 1.3150\n",
      "Epoch: 18/20... Step: 25010... Loss: 1.3512... Val Loss: 1.3126\n",
      "Epoch: 18/20... Step: 25020... Loss: 1.2771... Val Loss: 1.3134\n",
      "Epoch: 18/20... Step: 25030... Loss: 1.2621... Val Loss: 1.3116\n",
      "Epoch: 18/20... Step: 25040... Loss: 1.3189... Val Loss: 1.3154\n",
      "Epoch: 18/20... Step: 25050... Loss: 1.3779... Val Loss: 1.3131\n",
      "Epoch: 18/20... Step: 25060... Loss: 1.3779... Val Loss: 1.3145\n",
      "Epoch: 18/20... Step: 25070... Loss: 1.3569... Val Loss: 1.3153\n",
      "Epoch: 18/20... Step: 25080... Loss: 1.3698... Val Loss: 1.3133\n",
      "Epoch: 18/20... Step: 25090... Loss: 1.3103... Val Loss: 1.3118\n",
      "Epoch: 18/20... Step: 25100... Loss: 1.2981... Val Loss: 1.3140\n",
      "Epoch: 18/20... Step: 25110... Loss: 1.3251... Val Loss: 1.3146\n",
      "Epoch: 18/20... Step: 25120... Loss: 1.2693... Val Loss: 1.3198\n",
      "Epoch: 18/20... Step: 25130... Loss: 1.3527... Val Loss: 1.3139\n",
      "Epoch: 18/20... Step: 25140... Loss: 1.3014... Val Loss: 1.3173\n",
      "Epoch: 18/20... Step: 25150... Loss: 1.3289... Val Loss: 1.3178\n",
      "Epoch: 18/20... Step: 25160... Loss: 1.3063... Val Loss: 1.3132\n",
      "Epoch: 18/20... Step: 25170... Loss: 1.3321... Val Loss: 1.3134\n",
      "Epoch: 18/20... Step: 25180... Loss: 1.3681... Val Loss: 1.3134\n",
      "Epoch: 18/20... Step: 25190... Loss: 1.2837... Val Loss: 1.3143\n",
      "Epoch: 18/20... Step: 25200... Loss: 1.3049... Val Loss: 1.3099\n",
      "Epoch: 18/20... Step: 25210... Loss: 1.3293... Val Loss: 1.3122\n",
      "Epoch: 18/20... Step: 25220... Loss: 1.2855... Val Loss: 1.3136\n",
      "Epoch: 18/20... Step: 25230... Loss: 1.3554... Val Loss: 1.3098\n",
      "Epoch: 18/20... Step: 25240... Loss: 1.2865... Val Loss: 1.3121\n",
      "Epoch: 18/20... Step: 25250... Loss: 1.3496... Val Loss: 1.3104\n",
      "Epoch: 18/20... Step: 25260... Loss: 1.3303... Val Loss: 1.3076\n",
      "Epoch: 18/20... Step: 25270... Loss: 1.2761... Val Loss: 1.3124\n",
      "Epoch: 18/20... Step: 25280... Loss: 1.3247... Val Loss: 1.3110\n",
      "Epoch: 18/20... Step: 25290... Loss: 1.2778... Val Loss: 1.3102\n",
      "Epoch: 18/20... Step: 25300... Loss: 1.3968... Val Loss: 1.3092\n",
      "Epoch: 18/20... Step: 25310... Loss: 1.2936... Val Loss: 1.3108\n",
      "Epoch: 18/20... Step: 25320... Loss: 1.2702... Val Loss: 1.3144\n",
      "Epoch: 18/20... Step: 25330... Loss: 1.2665... Val Loss: 1.3118\n",
      "Epoch: 18/20... Step: 25340... Loss: 1.3995... Val Loss: 1.3077\n",
      "Epoch: 18/20... Step: 25350... Loss: 1.3013... Val Loss: 1.3109\n",
      "Epoch: 18/20... Step: 25360... Loss: 1.3373... Val Loss: 1.3086\n",
      "Epoch: 18/20... Step: 25370... Loss: 1.3264... Val Loss: 1.3092\n",
      "Epoch: 18/20... Step: 25380... Loss: 1.3625... Val Loss: 1.3097\n",
      "Epoch: 18/20... Step: 25390... Loss: 1.3763... Val Loss: 1.3095\n",
      "Epoch: 18/20... Step: 25400... Loss: 1.3544... Val Loss: 1.3120\n",
      "Epoch: 18/20... Step: 25410... Loss: 1.3438... Val Loss: 1.3087\n",
      "Epoch: 18/20... Step: 25420... Loss: 1.3624... Val Loss: 1.3102\n",
      "Epoch: 18/20... Step: 25430... Loss: 1.2938... Val Loss: 1.3090\n",
      "Epoch: 18/20... Step: 25440... Loss: 1.3504... Val Loss: 1.3099\n",
      "Epoch: 18/20... Step: 25450... Loss: 1.3688... Val Loss: 1.3093\n",
      "Epoch: 18/20... Step: 25460... Loss: 1.3187... Val Loss: 1.3084\n",
      "Epoch: 18/20... Step: 25470... Loss: 1.3201... Val Loss: 1.3090\n",
      "Epoch: 18/20... Step: 25480... Loss: 1.3901... Val Loss: 1.3108\n",
      "Epoch: 18/20... Step: 25490... Loss: 1.3043... Val Loss: 1.3101\n",
      "Epoch: 18/20... Step: 25500... Loss: 1.3176... Val Loss: 1.3107\n",
      "Epoch: 18/20... Step: 25510... Loss: 1.3439... Val Loss: 1.3081\n",
      "Epoch: 18/20... Step: 25520... Loss: 1.2954... Val Loss: 1.3098\n",
      "Epoch: 19/20... Step: 25530... Loss: 1.3517... Val Loss: 1.3085\n",
      "Epoch: 19/20... Step: 25540... Loss: 1.3026... Val Loss: 1.3079\n",
      "Epoch: 19/20... Step: 25550... Loss: 1.3387... Val Loss: 1.3074\n",
      "Epoch: 19/20... Step: 25560... Loss: 1.2070... Val Loss: 1.3073\n",
      "Epoch: 19/20... Step: 25570... Loss: 1.2854... Val Loss: 1.3068\n",
      "Epoch: 19/20... Step: 25580... Loss: 1.3081... Val Loss: 1.3089\n",
      "Epoch: 19/20... Step: 25590... Loss: 1.3153... Val Loss: 1.3065\n",
      "Epoch: 19/20... Step: 25600... Loss: 1.3533... Val Loss: 1.3052\n",
      "Epoch: 19/20... Step: 25610... Loss: 1.2937... Val Loss: 1.3040\n",
      "Epoch: 19/20... Step: 25620... Loss: 1.3944... Val Loss: 1.3091\n",
      "Epoch: 19/20... Step: 25630... Loss: 1.2722... Val Loss: 1.3073\n",
      "Epoch: 19/20... Step: 25640... Loss: 1.2762... Val Loss: 1.3096\n",
      "Epoch: 19/20... Step: 25650... Loss: 1.3639... Val Loss: 1.3049\n",
      "Epoch: 19/20... Step: 25660... Loss: 1.2720... Val Loss: 1.3063\n",
      "Epoch: 19/20... Step: 25670... Loss: 1.3670... Val Loss: 1.3114\n",
      "Epoch: 19/20... Step: 25680... Loss: 1.3109... Val Loss: 1.3105\n",
      "Epoch: 19/20... Step: 25690... Loss: 1.2800... Val Loss: 1.3081\n",
      "Epoch: 19/20... Step: 25700... Loss: 1.2715... Val Loss: 1.3098\n",
      "Epoch: 19/20... Step: 25710... Loss: 1.3144... Val Loss: 1.3111\n",
      "Epoch: 19/20... Step: 25720... Loss: 1.2864... Val Loss: 1.3108\n",
      "Epoch: 19/20... Step: 25730... Loss: 1.3095... Val Loss: 1.3145\n",
      "Epoch: 19/20... Step: 25740... Loss: 1.3135... Val Loss: 1.3103\n",
      "Epoch: 19/20... Step: 25750... Loss: 1.3924... Val Loss: 1.3109\n",
      "Epoch: 19/20... Step: 25760... Loss: 1.2672... Val Loss: 1.3111\n",
      "Epoch: 19/20... Step: 25770... Loss: 1.3167... Val Loss: 1.3122\n",
      "Epoch: 19/20... Step: 25780... Loss: 1.2518... Val Loss: 1.3095\n",
      "Epoch: 19/20... Step: 25790... Loss: 1.3225... Val Loss: 1.3102\n",
      "Epoch: 19/20... Step: 25800... Loss: 1.3031... Val Loss: 1.3121\n",
      "Epoch: 19/20... Step: 25810... Loss: 1.3603... Val Loss: 1.3101\n",
      "Epoch: 19/20... Step: 25820... Loss: 1.3844... Val Loss: 1.3126\n",
      "Epoch: 19/20... Step: 25830... Loss: 1.2741... Val Loss: 1.3138\n",
      "Epoch: 19/20... Step: 25840... Loss: 1.2796... Val Loss: 1.3084\n",
      "Epoch: 19/20... Step: 25850... Loss: 1.2982... Val Loss: 1.3105\n",
      "Epoch: 19/20... Step: 25860... Loss: 1.3124... Val Loss: 1.3098\n",
      "Epoch: 19/20... Step: 25870... Loss: 1.2677... Val Loss: 1.3104\n",
      "Epoch: 19/20... Step: 25880... Loss: 1.3206... Val Loss: 1.3089\n",
      "Epoch: 19/20... Step: 25890... Loss: 1.3268... Val Loss: 1.3081\n",
      "Epoch: 19/20... Step: 25900... Loss: 1.3522... Val Loss: 1.3095\n",
      "Epoch: 19/20... Step: 25910... Loss: 1.3414... Val Loss: 1.3072\n",
      "Epoch: 19/20... Step: 25920... Loss: 1.3550... Val Loss: 1.3092\n",
      "Epoch: 19/20... Step: 25930... Loss: 1.2375... Val Loss: 1.3082\n",
      "Epoch: 19/20... Step: 25940... Loss: 1.2555... Val Loss: 1.3143\n",
      "Epoch: 19/20... Step: 25950... Loss: 1.3323... Val Loss: 1.3109\n",
      "Epoch: 19/20... Step: 25960... Loss: 1.3124... Val Loss: 1.3082\n",
      "Epoch: 19/20... Step: 25970... Loss: 1.3093... Val Loss: 1.3163\n",
      "Epoch: 19/20... Step: 25980... Loss: 1.3036... Val Loss: 1.3120\n",
      "Epoch: 19/20... Step: 25990... Loss: 1.2991... Val Loss: 1.3107\n",
      "Epoch: 19/20... Step: 26000... Loss: 1.2953... Val Loss: 1.3140\n",
      "Epoch: 19/20... Step: 26010... Loss: 1.3261... Val Loss: 1.3107\n",
      "Epoch: 19/20... Step: 26020... Loss: 1.3078... Val Loss: 1.3112\n",
      "Epoch: 19/20... Step: 26030... Loss: 1.3235... Val Loss: 1.3106\n",
      "Epoch: 19/20... Step: 26040... Loss: 1.3295... Val Loss: 1.3081\n",
      "Epoch: 19/20... Step: 26050... Loss: 1.3444... Val Loss: 1.3133\n",
      "Epoch: 19/20... Step: 26060... Loss: 1.2969... Val Loss: 1.3085\n",
      "Epoch: 19/20... Step: 26070... Loss: 1.2807... Val Loss: 1.3056\n",
      "Epoch: 19/20... Step: 26080... Loss: 1.2991... Val Loss: 1.3145\n",
      "Epoch: 19/20... Step: 26090... Loss: 1.3465... Val Loss: 1.3094\n",
      "Epoch: 19/20... Step: 26100... Loss: 1.2903... Val Loss: 1.3083\n",
      "Epoch: 19/20... Step: 26110... Loss: 1.3309... Val Loss: 1.3083\n",
      "Epoch: 19/20... Step: 26120... Loss: 1.2576... Val Loss: 1.3109\n",
      "Epoch: 19/20... Step: 26130... Loss: 1.3730... Val Loss: 1.3147\n",
      "Epoch: 19/20... Step: 26140... Loss: 1.2901... Val Loss: 1.3078\n",
      "Epoch: 19/20... Step: 26150... Loss: 1.3204... Val Loss: 1.3069\n",
      "Epoch: 19/20... Step: 26160... Loss: 1.3356... Val Loss: 1.3116\n",
      "Epoch: 19/20... Step: 26170... Loss: 1.3498... Val Loss: 1.3083\n",
      "Epoch: 19/20... Step: 26180... Loss: 1.2444... Val Loss: 1.3117\n",
      "Epoch: 19/20... Step: 26190... Loss: 1.3617... Val Loss: 1.3112\n",
      "Epoch: 19/20... Step: 26200... Loss: 1.2982... Val Loss: 1.3100\n",
      "Epoch: 19/20... Step: 26210... Loss: 1.3263... Val Loss: 1.3115\n",
      "Epoch: 19/20... Step: 26220... Loss: 1.2520... Val Loss: 1.3111\n",
      "Epoch: 19/20... Step: 26230... Loss: 1.2900... Val Loss: 1.3125\n",
      "Epoch: 19/20... Step: 26240... Loss: 1.3206... Val Loss: 1.3097\n",
      "Epoch: 19/20... Step: 26250... Loss: 1.3594... Val Loss: 1.3099\n",
      "Epoch: 19/20... Step: 26260... Loss: 1.3405... Val Loss: 1.3082\n",
      "Epoch: 19/20... Step: 26270... Loss: 1.3890... Val Loss: 1.3097\n",
      "Epoch: 19/20... Step: 26280... Loss: 1.3364... Val Loss: 1.3060\n",
      "Epoch: 19/20... Step: 26290... Loss: 1.2866... Val Loss: 1.3116\n",
      "Epoch: 19/20... Step: 26300... Loss: 1.3631... Val Loss: 1.3102\n",
      "Epoch: 19/20... Step: 26310... Loss: 1.3574... Val Loss: 1.3090\n",
      "Epoch: 19/20... Step: 26320... Loss: 1.3253... Val Loss: 1.3069\n",
      "Epoch: 19/20... Step: 26330... Loss: 1.3301... Val Loss: 1.3104\n",
      "Epoch: 19/20... Step: 26340... Loss: 1.3012... Val Loss: 1.3109\n",
      "Epoch: 19/20... Step: 26350... Loss: 1.2603... Val Loss: 1.3080\n",
      "Epoch: 19/20... Step: 26360... Loss: 1.3533... Val Loss: 1.3093\n",
      "Epoch: 19/20... Step: 26370... Loss: 1.2997... Val Loss: 1.3096\n",
      "Epoch: 19/20... Step: 26380... Loss: 1.2887... Val Loss: 1.3080\n",
      "Epoch: 19/20... Step: 26390... Loss: 1.4054... Val Loss: 1.3085\n",
      "Epoch: 19/20... Step: 26400... Loss: 1.2891... Val Loss: 1.3092\n",
      "Epoch: 19/20... Step: 26410... Loss: 1.3278... Val Loss: 1.3160\n",
      "Epoch: 19/20... Step: 26420... Loss: 1.2841... Val Loss: 1.3110\n",
      "Epoch: 19/20... Step: 26430... Loss: 1.3931... Val Loss: 1.3077\n",
      "Epoch: 19/20... Step: 26440... Loss: 1.3334... Val Loss: 1.3117\n",
      "Epoch: 19/20... Step: 26450... Loss: 1.3152... Val Loss: 1.3095\n",
      "Epoch: 19/20... Step: 26460... Loss: 1.2793... Val Loss: 1.3107\n",
      "Epoch: 19/20... Step: 26470... Loss: 1.3052... Val Loss: 1.3101\n",
      "Epoch: 19/20... Step: 26480... Loss: 1.3064... Val Loss: 1.3140\n",
      "Epoch: 19/20... Step: 26490... Loss: 1.3262... Val Loss: 1.3103\n",
      "Epoch: 19/20... Step: 26500... Loss: 1.2741... Val Loss: 1.3102\n",
      "Epoch: 19/20... Step: 26510... Loss: 1.3065... Val Loss: 1.3115\n",
      "Epoch: 19/20... Step: 26520... Loss: 1.3320... Val Loss: 1.3102\n",
      "Epoch: 19/20... Step: 26530... Loss: 1.2965... Val Loss: 1.3117\n",
      "Epoch: 19/20... Step: 26540... Loss: 1.2909... Val Loss: 1.3111\n",
      "Epoch: 19/20... Step: 26550... Loss: 1.3122... Val Loss: 1.3109\n",
      "Epoch: 19/20... Step: 26560... Loss: 1.3374... Val Loss: 1.3107\n",
      "Epoch: 19/20... Step: 26570... Loss: 1.3028... Val Loss: 1.3114\n",
      "Epoch: 19/20... Step: 26580... Loss: 1.2482... Val Loss: 1.3100\n",
      "Epoch: 19/20... Step: 26590... Loss: 1.3353... Val Loss: 1.3118\n",
      "Epoch: 19/20... Step: 26600... Loss: 1.3496... Val Loss: 1.3099\n",
      "Epoch: 19/20... Step: 26610... Loss: 1.3319... Val Loss: 1.3103\n",
      "Epoch: 19/20... Step: 26620... Loss: 1.3742... Val Loss: 1.3083\n",
      "Epoch: 19/20... Step: 26630... Loss: 1.3323... Val Loss: 1.3074\n",
      "Epoch: 19/20... Step: 26640... Loss: 1.3191... Val Loss: 1.3085\n",
      "Epoch: 19/20... Step: 26650... Loss: 1.2688... Val Loss: 1.3068\n",
      "Epoch: 19/20... Step: 26660... Loss: 1.3363... Val Loss: 1.3060\n",
      "Epoch: 19/20... Step: 26670... Loss: 1.3722... Val Loss: 1.3071\n",
      "Epoch: 19/20... Step: 26680... Loss: 1.2745... Val Loss: 1.3053\n",
      "Epoch: 19/20... Step: 26690... Loss: 1.2692... Val Loss: 1.3095\n",
      "Epoch: 19/20... Step: 26700... Loss: 1.2030... Val Loss: 1.3073\n",
      "Epoch: 19/20... Step: 26710... Loss: 1.2572... Val Loss: 1.3085\n",
      "Epoch: 19/20... Step: 26720... Loss: 1.2963... Val Loss: 1.3057\n",
      "Epoch: 19/20... Step: 26730... Loss: 1.3264... Val Loss: 1.3101\n",
      "Epoch: 19/20... Step: 26740... Loss: 1.2418... Val Loss: 1.3127\n",
      "Epoch: 19/20... Step: 26750... Loss: 1.2790... Val Loss: 1.3093\n",
      "Epoch: 19/20... Step: 26760... Loss: 1.2398... Val Loss: 1.3034\n",
      "Epoch: 19/20... Step: 26770... Loss: 1.3135... Val Loss: 1.3065\n",
      "Epoch: 19/20... Step: 26780... Loss: 1.3440... Val Loss: 1.3072\n",
      "Epoch: 19/20... Step: 26790... Loss: 1.3628... Val Loss: 1.3047\n",
      "Epoch: 19/20... Step: 26800... Loss: 1.2831... Val Loss: 1.3082\n",
      "Epoch: 19/20... Step: 26810... Loss: 1.2921... Val Loss: 1.3079\n",
      "Epoch: 19/20... Step: 26820... Loss: 1.2210... Val Loss: 1.3074\n",
      "Epoch: 19/20... Step: 26830... Loss: 1.2856... Val Loss: 1.3090\n",
      "Epoch: 19/20... Step: 26840... Loss: 1.3562... Val Loss: 1.3082\n",
      "Epoch: 19/20... Step: 26850... Loss: 1.3736... Val Loss: 1.3056\n",
      "Epoch: 19/20... Step: 26860... Loss: 1.3626... Val Loss: 1.3075\n",
      "Epoch: 19/20... Step: 26870... Loss: 1.3248... Val Loss: 1.3074\n",
      "Epoch: 19/20... Step: 26880... Loss: 1.3524... Val Loss: 1.3046\n",
      "Epoch: 19/20... Step: 26890... Loss: 1.3514... Val Loss: 1.3071\n",
      "Epoch: 19/20... Step: 26900... Loss: 1.2909... Val Loss: 1.3068\n",
      "Epoch: 19/20... Step: 26910... Loss: 1.3810... Val Loss: 1.3070\n",
      "Epoch: 19/20... Step: 26920... Loss: 1.3295... Val Loss: 1.3087\n",
      "Epoch: 19/20... Step: 26930... Loss: 1.3396... Val Loss: 1.3048\n",
      "Epoch: 19/20... Step: 26940... Loss: 1.3559... Val Loss: 1.3067\n",
      "Epoch: 20/20... Step: 26950... Loss: 1.3452... Val Loss: 1.3070\n",
      "Epoch: 20/20... Step: 26960... Loss: 1.3668... Val Loss: 1.3012\n",
      "Epoch: 20/20... Step: 26970... Loss: 1.2450... Val Loss: 1.3019\n",
      "Epoch: 20/20... Step: 26980... Loss: 1.3137... Val Loss: 1.3080\n",
      "Epoch: 20/20... Step: 26990... Loss: 1.2636... Val Loss: 1.3040\n",
      "Epoch: 20/20... Step: 27000... Loss: 1.3598... Val Loss: 1.3038\n",
      "Epoch: 20/20... Step: 27010... Loss: 1.2133... Val Loss: 1.3018\n",
      "Epoch: 20/20... Step: 27020... Loss: 1.3069... Val Loss: 1.3033\n",
      "Epoch: 20/20... Step: 27030... Loss: 1.2813... Val Loss: 1.3042\n",
      "Epoch: 20/20... Step: 27040... Loss: 1.3423... Val Loss: 1.3048\n",
      "Epoch: 20/20... Step: 27050... Loss: 1.3072... Val Loss: 1.3060\n",
      "Epoch: 20/20... Step: 27060... Loss: 1.3128... Val Loss: 1.3056\n",
      "Epoch: 20/20... Step: 27070... Loss: 1.3316... Val Loss: 1.3033\n",
      "Epoch: 20/20... Step: 27080... Loss: 1.3116... Val Loss: 1.3049\n",
      "Epoch: 20/20... Step: 27090... Loss: 1.3421... Val Loss: 1.3062\n",
      "Epoch: 20/20... Step: 27100... Loss: 1.2526... Val Loss: 1.3105\n",
      "Epoch: 20/20... Step: 27110... Loss: 1.3257... Val Loss: 1.3048\n",
      "Epoch: 20/20... Step: 27120... Loss: 1.3443... Val Loss: 1.3053\n",
      "Epoch: 20/20... Step: 27130... Loss: 1.3255... Val Loss: 1.3122\n",
      "Epoch: 20/20... Step: 27140... Loss: 1.3202... Val Loss: 1.3076\n",
      "Epoch: 20/20... Step: 27150... Loss: 1.2495... Val Loss: 1.3102\n",
      "Epoch: 20/20... Step: 27160... Loss: 1.3810... Val Loss: 1.3072\n",
      "Epoch: 20/20... Step: 27170... Loss: 1.2849... Val Loss: 1.3076\n",
      "Epoch: 20/20... Step: 27180... Loss: 1.2539... Val Loss: 1.3100\n",
      "Epoch: 20/20... Step: 27190... Loss: 1.3341... Val Loss: 1.3101\n",
      "Epoch: 20/20... Step: 27200... Loss: 1.3248... Val Loss: 1.3104\n",
      "Epoch: 20/20... Step: 27210... Loss: 1.3181... Val Loss: 1.3093\n",
      "Epoch: 20/20... Step: 27220... Loss: 1.3671... Val Loss: 1.3095\n",
      "Epoch: 20/20... Step: 27230... Loss: 1.3186... Val Loss: 1.3099\n",
      "Epoch: 20/20... Step: 27240... Loss: 1.3139... Val Loss: 1.3109\n",
      "Epoch: 20/20... Step: 27250... Loss: 1.2662... Val Loss: 1.3084\n",
      "Epoch: 20/20... Step: 27260... Loss: 1.3231... Val Loss: 1.3085\n",
      "Epoch: 20/20... Step: 27270... Loss: 1.3660... Val Loss: 1.3076\n",
      "Epoch: 20/20... Step: 27280... Loss: 1.3839... Val Loss: 1.3065\n",
      "Epoch: 20/20... Step: 27290... Loss: 1.2678... Val Loss: 1.3072\n",
      "Epoch: 20/20... Step: 27300... Loss: 1.3509... Val Loss: 1.3059\n",
      "Epoch: 20/20... Step: 27310... Loss: 1.3557... Val Loss: 1.3052\n",
      "Epoch: 20/20... Step: 27320... Loss: 1.3171... Val Loss: 1.3074\n",
      "Epoch: 20/20... Step: 27330... Loss: 1.3310... Val Loss: 1.3061\n",
      "Epoch: 20/20... Step: 27340... Loss: 1.3333... Val Loss: 1.3062\n",
      "Epoch: 20/20... Step: 27350... Loss: 1.3230... Val Loss: 1.3078\n",
      "Epoch: 20/20... Step: 27360... Loss: 1.2567... Val Loss: 1.3057\n",
      "Epoch: 20/20... Step: 27370... Loss: 1.3304... Val Loss: 1.3072\n",
      "Epoch: 20/20... Step: 27380... Loss: 1.3395... Val Loss: 1.3065\n",
      "Epoch: 20/20... Step: 27390... Loss: 1.2580... Val Loss: 1.3071\n",
      "Epoch: 20/20... Step: 27400... Loss: 1.3200... Val Loss: 1.3078\n",
      "Epoch: 20/20... Step: 27410... Loss: 1.3119... Val Loss: 1.3091\n",
      "Epoch: 20/20... Step: 27420... Loss: 1.2255... Val Loss: 1.3101\n",
      "Epoch: 20/20... Step: 27430... Loss: 1.3286... Val Loss: 1.3081\n",
      "Epoch: 20/20... Step: 27440... Loss: 1.3417... Val Loss: 1.3074\n",
      "Epoch: 20/20... Step: 27450... Loss: 1.3238... Val Loss: 1.3095\n",
      "Epoch: 20/20... Step: 27460... Loss: 1.2852... Val Loss: 1.3050\n",
      "Epoch: 20/20... Step: 27470... Loss: 1.2780... Val Loss: 1.3071\n",
      "Epoch: 20/20... Step: 27480... Loss: 1.3555... Val Loss: 1.3055\n",
      "Epoch: 20/20... Step: 27490... Loss: 1.2789... Val Loss: 1.3052\n",
      "Epoch: 20/20... Step: 27500... Loss: 1.2605... Val Loss: 1.3090\n",
      "Epoch: 20/20... Step: 27510... Loss: 1.3015... Val Loss: 1.3055\n",
      "Epoch: 20/20... Step: 27520... Loss: 1.3059... Val Loss: 1.3067\n",
      "Epoch: 20/20... Step: 27530... Loss: 1.3105... Val Loss: 1.3049\n",
      "Epoch: 20/20... Step: 27540... Loss: 1.3015... Val Loss: 1.3097\n",
      "Epoch: 20/20... Step: 27550... Loss: 1.3109... Val Loss: 1.3101\n",
      "Epoch: 20/20... Step: 27560... Loss: 1.3248... Val Loss: 1.3034\n",
      "Epoch: 20/20... Step: 27570... Loss: 1.3191... Val Loss: 1.3063\n",
      "Epoch: 20/20... Step: 27580... Loss: 1.3524... Val Loss: 1.3073\n",
      "Epoch: 20/20... Step: 27590... Loss: 1.3138... Val Loss: 1.3095\n",
      "Epoch: 20/20... Step: 27600... Loss: 1.2987... Val Loss: 1.3094\n",
      "Epoch: 20/20... Step: 27610... Loss: 1.3840... Val Loss: 1.3074\n",
      "Epoch: 20/20... Step: 27620... Loss: 1.2854... Val Loss: 1.3059\n",
      "Epoch: 20/20... Step: 27630... Loss: 1.2659... Val Loss: 1.3096\n",
      "Epoch: 20/20... Step: 27640... Loss: 1.2929... Val Loss: 1.3079\n",
      "Epoch: 20/20... Step: 27650... Loss: 1.3284... Val Loss: 1.3066\n",
      "Epoch: 20/20... Step: 27660... Loss: 1.2964... Val Loss: 1.3053\n",
      "Epoch: 20/20... Step: 27670... Loss: 1.4001... Val Loss: 1.3069\n",
      "Epoch: 20/20... Step: 27680... Loss: 1.3012... Val Loss: 1.3038\n",
      "Epoch: 20/20... Step: 27690... Loss: 1.3370... Val Loss: 1.3049\n",
      "Epoch: 20/20... Step: 27700... Loss: 1.3635... Val Loss: 1.3029\n",
      "Epoch: 20/20... Step: 27710... Loss: 1.3678... Val Loss: 1.3092\n",
      "Epoch: 20/20... Step: 27720... Loss: 1.3617... Val Loss: 1.3057\n",
      "Epoch: 20/20... Step: 27730... Loss: 1.3891... Val Loss: 1.3048\n",
      "Epoch: 20/20... Step: 27740... Loss: 1.3516... Val Loss: 1.3066\n",
      "Epoch: 20/20... Step: 27750... Loss: 1.3132... Val Loss: 1.3066\n",
      "Epoch: 20/20... Step: 27760... Loss: 1.3056... Val Loss: 1.3077\n",
      "Epoch: 20/20... Step: 27770... Loss: 1.2273... Val Loss: 1.3074\n",
      "Epoch: 20/20... Step: 27780... Loss: 1.3897... Val Loss: 1.3048\n",
      "Epoch: 20/20... Step: 27790... Loss: 1.3502... Val Loss: 1.3046\n",
      "Epoch: 20/20... Step: 27800... Loss: 1.3135... Val Loss: 1.3066\n",
      "Epoch: 20/20... Step: 27810... Loss: 1.3201... Val Loss: 1.3056\n",
      "Epoch: 20/20... Step: 27820... Loss: 1.2741... Val Loss: 1.3111\n",
      "Epoch: 20/20... Step: 27830... Loss: 1.2631... Val Loss: 1.3114\n",
      "Epoch: 20/20... Step: 27840... Loss: 1.2985... Val Loss: 1.3058\n",
      "Epoch: 20/20... Step: 27850... Loss: 1.3387... Val Loss: 1.3067\n",
      "Epoch: 20/20... Step: 27860... Loss: 1.2874... Val Loss: 1.3090\n",
      "Epoch: 20/20... Step: 27870... Loss: 1.2674... Val Loss: 1.3081\n",
      "Epoch: 20/20... Step: 27880... Loss: 1.3185... Val Loss: 1.3079\n",
      "Epoch: 20/20... Step: 27890... Loss: 1.3168... Val Loss: 1.3078\n",
      "Epoch: 20/20... Step: 27900... Loss: 1.2787... Val Loss: 1.3087\n",
      "Epoch: 20/20... Step: 27910... Loss: 1.3577... Val Loss: 1.3082\n",
      "Epoch: 20/20... Step: 27920... Loss: 1.3437... Val Loss: 1.3104\n",
      "Epoch: 20/20... Step: 27930... Loss: 1.3436... Val Loss: 1.3100\n",
      "Epoch: 20/20... Step: 27940... Loss: 1.2477... Val Loss: 1.3069\n",
      "Epoch: 20/20... Step: 27950... Loss: 1.3045... Val Loss: 1.3092\n",
      "Epoch: 20/20... Step: 27960... Loss: 1.3632... Val Loss: 1.3093\n",
      "Epoch: 20/20... Step: 27970... Loss: 1.3688... Val Loss: 1.3083\n",
      "Epoch: 20/20... Step: 27980... Loss: 1.2803... Val Loss: 1.3088\n",
      "Epoch: 20/20... Step: 27990... Loss: 1.3072... Val Loss: 1.3070\n",
      "Epoch: 20/20... Step: 28000... Loss: 1.3090... Val Loss: 1.3074\n",
      "Epoch: 20/20... Step: 28010... Loss: 1.2317... Val Loss: 1.3075\n",
      "Epoch: 20/20... Step: 28020... Loss: 1.3659... Val Loss: 1.3122\n",
      "Epoch: 20/20... Step: 28030... Loss: 1.3121... Val Loss: 1.3079\n",
      "Epoch: 20/20... Step: 28040... Loss: 1.3414... Val Loss: 1.3041\n",
      "Epoch: 20/20... Step: 28050... Loss: 1.2867... Val Loss: 1.3056\n",
      "Epoch: 20/20... Step: 28060... Loss: 1.3271... Val Loss: 1.3057\n",
      "Epoch: 20/20... Step: 28070... Loss: 1.3060... Val Loss: 1.3035\n",
      "Epoch: 20/20... Step: 28080... Loss: 1.2583... Val Loss: 1.3057\n",
      "Epoch: 20/20... Step: 28090... Loss: 1.4004... Val Loss: 1.3039\n",
      "Epoch: 20/20... Step: 28100... Loss: 1.2498... Val Loss: 1.3034\n",
      "Epoch: 20/20... Step: 28110... Loss: 1.2581... Val Loss: 1.3038\n",
      "Epoch: 20/20... Step: 28120... Loss: 1.3506... Val Loss: 1.3048\n",
      "Epoch: 20/20... Step: 28130... Loss: 1.3362... Val Loss: 1.3068\n",
      "Epoch: 20/20... Step: 28140... Loss: 1.3061... Val Loss: 1.3024\n",
      "Epoch: 20/20... Step: 28150... Loss: 1.4035... Val Loss: 1.3057\n",
      "Epoch: 20/20... Step: 28160... Loss: 1.3302... Val Loss: 1.3092\n",
      "Epoch: 20/20... Step: 28170... Loss: 1.3413... Val Loss: 1.3033\n",
      "Epoch: 20/20... Step: 28180... Loss: 1.3046... Val Loss: 1.3012\n",
      "Epoch: 20/20... Step: 28190... Loss: 1.2977... Val Loss: 1.3061\n",
      "Epoch: 20/20... Step: 28200... Loss: 1.3536... Val Loss: 1.3059\n",
      "Epoch: 20/20... Step: 28210... Loss: 1.2836... Val Loss: 1.3041\n",
      "Epoch: 20/20... Step: 28220... Loss: 1.3314... Val Loss: 1.3050\n",
      "Epoch: 20/20... Step: 28230... Loss: 1.3233... Val Loss: 1.3062\n",
      "Epoch: 20/20... Step: 28240... Loss: 1.2939... Val Loss: 1.3052\n",
      "Epoch: 20/20... Step: 28250... Loss: 1.2805... Val Loss: 1.3066\n",
      "Epoch: 20/20... Step: 28260... Loss: 1.3562... Val Loss: 1.3048\n",
      "Epoch: 20/20... Step: 28270... Loss: 1.3340... Val Loss: 1.3030\n",
      "Epoch: 20/20... Step: 28280... Loss: 1.3296... Val Loss: 1.3020\n",
      "Epoch: 20/20... Step: 28290... Loss: 1.2470... Val Loss: 1.3041\n",
      "Epoch: 20/20... Step: 28300... Loss: 1.2879... Val Loss: 1.3031\n",
      "Epoch: 20/20... Step: 28310... Loss: 1.3241... Val Loss: 1.3018\n",
      "Epoch: 20/20... Step: 28320... Loss: 1.3040... Val Loss: 1.3037\n",
      "Epoch: 20/20... Step: 28330... Loss: 1.2977... Val Loss: 1.3065\n",
      "Epoch: 20/20... Step: 28340... Loss: 1.3661... Val Loss: 1.3039\n",
      "Epoch: 20/20... Step: 28350... Loss: 1.3928... Val Loss: 1.3008\n",
      "Epoch: 20/20... Step: 28360... Loss: 1.5199... Val Loss: 1.3011\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "batch_size = 64\n",
    "seq_length = 32\n",
    "# start small if you are just testing initial behavior\n",
    "n_epochs = 20\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-Nqbv1CICR7"
   },
   "source": [
    "## Step 6. Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2sJhx5iceEm"
   },
   "source": [
    "Now that the model is trained, we want to predict the next character for the sequence of input symbols. We pass a character as input, and the network predicts the next character. Then we take that character, pass it as input, and get another predicted character, and so on.\n",
    "\n",
    "### Details of a `predict`  function\n",
    "\n",
    "Our RNN's output comes from a fully connected layer and outputs the **distribution of the next character scores**. To actually get the next character, we use the softmax function, which gives us a **probability** distribution that we can then choose to predict the next character.\n",
    "\n",
    "**Top K sampling**\n",
    "\n",
    "Our predictions are based on a categorical distribution of probabilities for all possible characters. We can make the sampling process smarter by looking at only some of the most likely $K$ characters. This will prevent the network from giving us completely absurd characters, and will also allow some noise and randomness to be introduced into the selected text. This technique is called [top K](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk) sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QEIRW_B2ceEm"
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG38j3gQceEm"
   },
   "source": [
    "To generate the text, you need to feed the initial characters, let's call them `prime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "P9vpB5gRceEm"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqmFA9eEceEm",
    "outputId": "2584a289-464f-4fc2-a764-fc99c2b48c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Pávlovna was silent to the prince.\n",
      "\n",
      "“Well, who is\n",
      "the most marches... I\n",
      "want?” the\n",
      "patted smull the hair.\n",
      "\n",
      "“What did you have been\n",
      "soldiers, I should be bring to an officer,” said Denísov, trying to return\n",
      "on the course there were a power to such bring\n",
      "her and all his staff of his sang, and the commander\n",
      "of the second pates talking it.\n",
      "\n",
      "“Ah,\n",
      "what and he does not though this time this is the princess! When it would be to be time to say it to still\n",
      "the pleasure that the table.”\n",
      "\n",
      "“You know, I do not, there!” cried Denísov’s hinds. “To the service to him to a sendence, it is a continuous cries of the commander’s\n",
      "friend, the countess went on to see a face. It was a mild time all the time in the seminor and a few\n",
      "man is as an offer single, and said to the capture, who alone assuged her.\n",
      "\n",
      "“What a man the commander and was the significance!”\n",
      "\n",
      "“I am a man answered in the party of the personal charming and a perplexed\n",
      "music way. We will distate with me,” he\n",
      "said, and several simple.\n",
      "\n",
      "As if it wa\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fgy2IqXIFiWV"
   },
   "source": [
    "### Model_2: Two LSTM-layers\n",
    "n_hidden = 1024  \n",
    "n_layers = 2  \n",
    "drop_prob=0.5   \n",
    "batch_size = 128  \n",
    "seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ji-atY1YFORU",
    "outputId": "44947695-2726-403d-92a3-8b63bbf48c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(112, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=1024, out_features=112, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set your model hyperparameters\n",
    "\n",
    "## YOUR CODE HERE\n",
    "n_hidden = 1024\n",
    "n_layers = 2\n",
    "drop_prob=0.5\n",
    "\n",
    "net = CharRNN(tuple(set(text)), n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxG6qlQ5FOUg",
    "outputId": "fcffeb53-5f38-42aa-841b-e323a676eaaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.1969... Val Loss: 3.1200\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1645... Val Loss: 3.1006\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1255... Val Loss: 3.0878\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1580... Val Loss: 3.0873\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1372... Val Loss: 3.0815\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1241... Val Loss: 3.0679\n",
      "Epoch: 1/20... Step: 70... Loss: 3.0970... Val Loss: 3.0537\n",
      "Epoch: 1/20... Step: 80... Loss: 3.0362... Val Loss: 2.9766\n",
      "Epoch: 1/20... Step: 90... Loss: 2.9786... Val Loss: 2.9444\n",
      "Epoch: 1/20... Step: 100... Loss: 2.9401... Val Loss: 2.8677\n",
      "Epoch: 1/20... Step: 110... Loss: 2.8098... Val Loss: 2.7358\n",
      "Epoch: 1/20... Step: 120... Loss: 2.6859... Val Loss: 2.6211\n",
      "Epoch: 1/20... Step: 130... Loss: 2.5672... Val Loss: 2.5168\n",
      "Epoch: 1/20... Step: 140... Loss: 2.5003... Val Loss: 2.4472\n",
      "Epoch: 1/20... Step: 150... Loss: 2.4102... Val Loss: 3.1818\n",
      "Epoch: 1/20... Step: 160... Loss: 2.4001... Val Loss: 2.3528\n",
      "Epoch: 1/20... Step: 170... Loss: 2.3812... Val Loss: 2.3104\n",
      "Epoch: 2/20... Step: 180... Loss: 2.3320... Val Loss: 2.2725\n",
      "Epoch: 2/20... Step: 190... Loss: 2.2753... Val Loss: 2.2284\n",
      "Epoch: 2/20... Step: 200... Loss: 2.2647... Val Loss: 2.1836\n",
      "Epoch: 2/20... Step: 210... Loss: 2.1890... Val Loss: 2.1419\n",
      "Epoch: 2/20... Step: 220... Loss: 2.1743... Val Loss: 2.1019\n",
      "Epoch: 2/20... Step: 230... Loss: 2.1065... Val Loss: 2.0766\n",
      "Epoch: 2/20... Step: 240... Loss: 2.0941... Val Loss: 2.0376\n",
      "Epoch: 2/20... Step: 250... Loss: 2.0556... Val Loss: 2.0202\n",
      "Epoch: 2/20... Step: 260... Loss: 2.0195... Val Loss: 1.9884\n",
      "Epoch: 2/20... Step: 270... Loss: 1.9995... Val Loss: 1.9545\n",
      "Epoch: 2/20... Step: 280... Loss: 1.9670... Val Loss: 1.9279\n",
      "Epoch: 2/20... Step: 290... Loss: 1.9657... Val Loss: 1.9155\n",
      "Epoch: 2/20... Step: 300... Loss: 1.9331... Val Loss: 1.8813\n",
      "Epoch: 2/20... Step: 310... Loss: 1.8787... Val Loss: 1.8558\n",
      "Epoch: 2/20... Step: 320... Loss: 1.8813... Val Loss: 1.8359\n",
      "Epoch: 2/20... Step: 330... Loss: 1.8622... Val Loss: 1.8093\n",
      "Epoch: 2/20... Step: 340... Loss: 1.8368... Val Loss: 1.7893\n",
      "Epoch: 2/20... Step: 350... Loss: 1.8158... Val Loss: 1.7795\n",
      "Epoch: 3/20... Step: 360... Loss: 1.8233... Val Loss: 1.7564\n",
      "Epoch: 3/20... Step: 370... Loss: 1.7841... Val Loss: 1.7423\n",
      "Epoch: 3/20... Step: 380... Loss: 1.7791... Val Loss: 1.7252\n",
      "Epoch: 3/20... Step: 390... Loss: 1.7389... Val Loss: 1.7071\n",
      "Epoch: 3/20... Step: 400... Loss: 1.7316... Val Loss: 1.6977\n",
      "Epoch: 3/20... Step: 410... Loss: 1.7080... Val Loss: 1.6816\n",
      "Epoch: 3/20... Step: 420... Loss: 1.7023... Val Loss: 1.6662\n",
      "Epoch: 3/20... Step: 430... Loss: 1.6957... Val Loss: 1.6584\n",
      "Epoch: 3/20... Step: 440... Loss: 1.6988... Val Loss: 1.6462\n",
      "Epoch: 3/20... Step: 450... Loss: 1.6440... Val Loss: 1.6322\n",
      "Epoch: 3/20... Step: 460... Loss: 1.6654... Val Loss: 1.6236\n",
      "Epoch: 3/20... Step: 470... Loss: 1.6540... Val Loss: 1.6108\n",
      "Epoch: 3/20... Step: 480... Loss: 1.6673... Val Loss: 1.6044\n",
      "Epoch: 3/20... Step: 490... Loss: 1.6219... Val Loss: 1.5904\n",
      "Epoch: 3/20... Step: 500... Loss: 1.6438... Val Loss: 1.5796\n",
      "Epoch: 3/20... Step: 510... Loss: 1.5963... Val Loss: 1.5676\n",
      "Epoch: 3/20... Step: 520... Loss: 1.6361... Val Loss: 1.5596\n",
      "Epoch: 3/20... Step: 530... Loss: 1.6123... Val Loss: 1.5525\n",
      "Epoch: 4/20... Step: 540... Loss: 1.5736... Val Loss: 1.5462\n",
      "Epoch: 4/20... Step: 550... Loss: 1.5855... Val Loss: 1.5410\n",
      "Epoch: 4/20... Step: 560... Loss: 1.5544... Val Loss: 1.5301\n",
      "Epoch: 4/20... Step: 570... Loss: 1.5840... Val Loss: 1.5205\n",
      "Epoch: 4/20... Step: 580... Loss: 1.5618... Val Loss: 1.5159\n",
      "Epoch: 4/20... Step: 590... Loss: 1.5564... Val Loss: 1.5132\n",
      "Epoch: 4/20... Step: 600... Loss: 1.5267... Val Loss: 1.5000\n",
      "Epoch: 4/20... Step: 610... Loss: 1.5286... Val Loss: 1.4989\n",
      "Epoch: 4/20... Step: 620... Loss: 1.5184... Val Loss: 1.4913\n",
      "Epoch: 4/20... Step: 630... Loss: 1.5229... Val Loss: 1.4861\n",
      "Epoch: 4/20... Step: 640... Loss: 1.5164... Val Loss: 1.4791\n",
      "Epoch: 4/20... Step: 650... Loss: 1.5130... Val Loss: 1.4779\n",
      "Epoch: 4/20... Step: 660... Loss: 1.5191... Val Loss: 1.4660\n",
      "Epoch: 4/20... Step: 670... Loss: 1.5182... Val Loss: 1.4611\n",
      "Epoch: 4/20... Step: 680... Loss: 1.4725... Val Loss: 1.4542\n",
      "Epoch: 4/20... Step: 690... Loss: 1.5054... Val Loss: 1.4475\n",
      "Epoch: 4/20... Step: 700... Loss: 1.5114... Val Loss: 1.4419\n",
      "Epoch: 5/20... Step: 710... Loss: 1.4780... Val Loss: 1.4457\n",
      "Epoch: 5/20... Step: 720... Loss: 1.4610... Val Loss: 1.4351\n",
      "Epoch: 5/20... Step: 730... Loss: 1.4530... Val Loss: 1.4321\n",
      "Epoch: 5/20... Step: 740... Loss: 1.4422... Val Loss: 1.4246\n",
      "Epoch: 5/20... Step: 750... Loss: 1.4569... Val Loss: 1.4240\n",
      "Epoch: 5/20... Step: 760... Loss: 1.4505... Val Loss: 1.4206\n",
      "Epoch: 5/20... Step: 770... Loss: 1.4263... Val Loss: 1.4178\n",
      "Epoch: 5/20... Step: 780... Loss: 1.4363... Val Loss: 1.4153\n",
      "Epoch: 5/20... Step: 790... Loss: 1.4154... Val Loss: 1.4121\n",
      "Epoch: 5/20... Step: 800... Loss: 1.4371... Val Loss: 1.4099\n",
      "Epoch: 5/20... Step: 810... Loss: 1.4451... Val Loss: 1.4053\n",
      "Epoch: 5/20... Step: 820... Loss: 1.4434... Val Loss: 1.3986\n",
      "Epoch: 5/20... Step: 830... Loss: 1.4182... Val Loss: 1.3944\n",
      "Epoch: 5/20... Step: 840... Loss: 1.4322... Val Loss: 1.4003\n",
      "Epoch: 5/20... Step: 850... Loss: 1.4180... Val Loss: 1.3900\n",
      "Epoch: 5/20... Step: 860... Loss: 1.4026... Val Loss: 1.3819\n",
      "Epoch: 5/20... Step: 870... Loss: 1.4031... Val Loss: 1.3824\n",
      "Epoch: 5/20... Step: 880... Loss: 1.4155... Val Loss: 1.3783\n",
      "Epoch: 6/20... Step: 890... Loss: 1.4203... Val Loss: 1.3753\n",
      "Epoch: 6/20... Step: 900... Loss: 1.4082... Val Loss: 1.3757\n",
      "Epoch: 6/20... Step: 910... Loss: 1.3986... Val Loss: 1.3686\n",
      "Epoch: 6/20... Step: 920... Loss: 1.3968... Val Loss: 1.3644\n",
      "Epoch: 6/20... Step: 930... Loss: 1.3982... Val Loss: 1.3630\n",
      "Epoch: 6/20... Step: 940... Loss: 1.3848... Val Loss: 1.3633\n",
      "Epoch: 6/20... Step: 950... Loss: 1.3721... Val Loss: 1.3596\n",
      "Epoch: 6/20... Step: 960... Loss: 1.3756... Val Loss: 1.3590\n",
      "Epoch: 6/20... Step: 970... Loss: 1.3598... Val Loss: 1.3586\n",
      "Epoch: 6/20... Step: 980... Loss: 1.3804... Val Loss: 1.3594\n",
      "Epoch: 6/20... Step: 990... Loss: 1.3637... Val Loss: 1.3522\n",
      "Epoch: 6/20... Step: 1000... Loss: 1.3513... Val Loss: 1.3504\n",
      "Epoch: 6/20... Step: 1010... Loss: 1.3860... Val Loss: 1.3486\n",
      "Epoch: 6/20... Step: 1020... Loss: 1.3497... Val Loss: 1.3428\n",
      "Epoch: 6/20... Step: 1030... Loss: 1.3541... Val Loss: 1.3409\n",
      "Epoch: 6/20... Step: 1040... Loss: 1.3486... Val Loss: 1.3366\n",
      "Epoch: 6/20... Step: 1050... Loss: 1.3831... Val Loss: 1.3337\n",
      "Epoch: 6/20... Step: 1060... Loss: 1.3629... Val Loss: 1.3334\n",
      "Epoch: 7/20... Step: 1070... Loss: 1.3563... Val Loss: 1.3321\n",
      "Epoch: 7/20... Step: 1080... Loss: 1.3297... Val Loss: 1.3324\n",
      "Epoch: 7/20... Step: 1090... Loss: 1.3259... Val Loss: 1.3248\n",
      "Epoch: 7/20... Step: 1100... Loss: 1.3636... Val Loss: 1.3245\n",
      "Epoch: 7/20... Step: 1110... Loss: 1.3229... Val Loss: 1.3301\n",
      "Epoch: 7/20... Step: 1120... Loss: 1.3343... Val Loss: 1.3234\n",
      "Epoch: 7/20... Step: 1130... Loss: 1.3521... Val Loss: 1.3293\n",
      "Epoch: 7/20... Step: 1140... Loss: 1.3266... Val Loss: 1.3237\n",
      "Epoch: 7/20... Step: 1150... Loss: 1.3292... Val Loss: 1.3200\n",
      "Epoch: 7/20... Step: 1160... Loss: 1.3052... Val Loss: 1.3164\n",
      "Epoch: 7/20... Step: 1170... Loss: 1.3523... Val Loss: 1.3152\n",
      "Epoch: 7/20... Step: 1180... Loss: 1.3123... Val Loss: 1.3109\n",
      "Epoch: 7/20... Step: 1190... Loss: 1.3496... Val Loss: 1.3128\n",
      "Epoch: 7/20... Step: 1200... Loss: 1.3351... Val Loss: 1.3082\n",
      "Epoch: 7/20... Step: 1210... Loss: 1.3223... Val Loss: 1.3072\n",
      "Epoch: 7/20... Step: 1220... Loss: 1.3188... Val Loss: 1.3035\n",
      "Epoch: 7/20... Step: 1230... Loss: 1.3373... Val Loss: 1.3036\n",
      "Epoch: 8/20... Step: 1240... Loss: 1.3844... Val Loss: 1.3020\n",
      "Epoch: 8/20... Step: 1250... Loss: 1.2919... Val Loss: 1.3008\n",
      "Epoch: 8/20... Step: 1260... Loss: 1.3045... Val Loss: 1.2978\n",
      "Epoch: 8/20... Step: 1270... Loss: 1.2882... Val Loss: 1.2942\n",
      "Epoch: 8/20... Step: 1280... Loss: 1.3016... Val Loss: 1.2983\n",
      "Epoch: 8/20... Step: 1290... Loss: 1.3088... Val Loss: 1.2946\n",
      "Epoch: 8/20... Step: 1300... Loss: 1.2930... Val Loss: 1.2928\n",
      "Epoch: 8/20... Step: 1310... Loss: 1.2820... Val Loss: 1.2939\n",
      "Epoch: 8/20... Step: 1320... Loss: 1.2990... Val Loss: 1.2957\n",
      "Epoch: 8/20... Step: 1330... Loss: 1.2998... Val Loss: 1.2948\n",
      "Epoch: 8/20... Step: 1340... Loss: 1.3069... Val Loss: 1.2891\n",
      "Epoch: 8/20... Step: 1350... Loss: 1.2626... Val Loss: 1.2874\n",
      "Epoch: 8/20... Step: 1360... Loss: 1.2870... Val Loss: 1.2878\n",
      "Epoch: 8/20... Step: 1370... Loss: 1.2707... Val Loss: 1.2842\n",
      "Epoch: 8/20... Step: 1380... Loss: 1.3097... Val Loss: 1.2859\n",
      "Epoch: 8/20... Step: 1390... Loss: 1.2873... Val Loss: 1.2796\n",
      "Epoch: 8/20... Step: 1400... Loss: 1.3028... Val Loss: 1.2814\n",
      "Epoch: 8/20... Step: 1410... Loss: 1.2719... Val Loss: 1.2815\n",
      "Epoch: 9/20... Step: 1420... Loss: 1.2855... Val Loss: 1.2804\n",
      "Epoch: 9/20... Step: 1430... Loss: 1.2585... Val Loss: 1.2765\n",
      "Epoch: 9/20... Step: 1440... Loss: 1.2613... Val Loss: 1.2796\n",
      "Epoch: 9/20... Step: 1450... Loss: 1.3090... Val Loss: 1.2749\n",
      "Epoch: 9/20... Step: 1460... Loss: 1.2718... Val Loss: 1.2727\n",
      "Epoch: 9/20... Step: 1470... Loss: 1.2649... Val Loss: 1.2722\n",
      "Epoch: 9/20... Step: 1480... Loss: 1.2222... Val Loss: 1.2752\n",
      "Epoch: 9/20... Step: 1490... Loss: 1.2478... Val Loss: 1.2793\n",
      "Epoch: 9/20... Step: 1500... Loss: 1.2492... Val Loss: 1.2761\n",
      "Epoch: 9/20... Step: 1510... Loss: 1.2473... Val Loss: 1.2740\n",
      "Epoch: 9/20... Step: 1520... Loss: 1.2663... Val Loss: 1.2698\n",
      "Epoch: 9/20... Step: 1530... Loss: 1.2853... Val Loss: 1.2713\n",
      "Epoch: 9/20... Step: 1540... Loss: 1.2765... Val Loss: 1.2694\n",
      "Epoch: 9/20... Step: 1550... Loss: 1.2579... Val Loss: 1.2729\n",
      "Epoch: 9/20... Step: 1560... Loss: 1.2479... Val Loss: 1.2641\n",
      "Epoch: 9/20... Step: 1570... Loss: 1.2624... Val Loss: 1.2618\n",
      "Epoch: 9/20... Step: 1580... Loss: 1.2790... Val Loss: 1.2596\n",
      "Epoch: 9/20... Step: 1590... Loss: 1.2383... Val Loss: 1.2583\n",
      "Epoch: 10/20... Step: 1600... Loss: 1.2891... Val Loss: 1.2580\n",
      "Epoch: 10/20... Step: 1610... Loss: 1.2282... Val Loss: 1.2653\n",
      "Epoch: 10/20... Step: 1620... Loss: 1.2537... Val Loss: 1.2593\n",
      "Epoch: 10/20... Step: 1630... Loss: 1.2444... Val Loss: 1.2537\n",
      "Epoch: 10/20... Step: 1640... Loss: 1.2431... Val Loss: 1.2553\n",
      "Epoch: 10/20... Step: 1650... Loss: 1.2434... Val Loss: 1.2541\n",
      "Epoch: 10/20... Step: 1660... Loss: 1.2547... Val Loss: 1.2537\n",
      "Epoch: 10/20... Step: 1670... Loss: 1.2365... Val Loss: 1.2549\n",
      "Epoch: 10/20... Step: 1680... Loss: 1.2264... Val Loss: 1.2546\n",
      "Epoch: 10/20... Step: 1690... Loss: 1.2172... Val Loss: 1.2525\n",
      "Epoch: 10/20... Step: 1700... Loss: 1.2584... Val Loss: 1.2523\n",
      "Epoch: 10/20... Step: 1710... Loss: 1.2405... Val Loss: 1.2519\n",
      "Epoch: 10/20... Step: 1720... Loss: 1.2427... Val Loss: 1.2496\n",
      "Epoch: 10/20... Step: 1730... Loss: 1.2310... Val Loss: 1.2511\n",
      "Epoch: 10/20... Step: 1740... Loss: 1.2138... Val Loss: 1.2458\n",
      "Epoch: 10/20... Step: 1750... Loss: 1.2593... Val Loss: 1.2487\n",
      "Epoch: 10/20... Step: 1760... Loss: 1.2439... Val Loss: 1.2421\n",
      "Epoch: 10/20... Step: 1770... Loss: 1.2748... Val Loss: 1.2425\n",
      "Epoch: 11/20... Step: 1780... Loss: 1.2112... Val Loss: 1.2403\n",
      "Epoch: 11/20... Step: 1790... Loss: 1.2096... Val Loss: 1.2451\n",
      "Epoch: 11/20... Step: 1800... Loss: 1.1936... Val Loss: 1.2380\n",
      "Epoch: 11/20... Step: 1810... Loss: 1.2228... Val Loss: 1.2357\n",
      "Epoch: 11/20... Step: 1820... Loss: 1.2256... Val Loss: 1.2369\n",
      "Epoch: 11/20... Step: 1830... Loss: 1.2293... Val Loss: 1.2317\n",
      "Epoch: 11/20... Step: 1840... Loss: 1.2034... Val Loss: 1.2295\n",
      "Epoch: 11/20... Step: 1850... Loss: 1.2132... Val Loss: 1.2314\n",
      "Epoch: 11/20... Step: 1860... Loss: 1.1824... Val Loss: 1.2270\n",
      "Epoch: 11/20... Step: 1870... Loss: 1.2021... Val Loss: 1.2273\n",
      "Epoch: 11/20... Step: 1880... Loss: 1.1826... Val Loss: 1.2292\n",
      "Epoch: 11/20... Step: 1890... Loss: 1.1708... Val Loss: 1.2262\n",
      "Epoch: 11/20... Step: 1900... Loss: 1.2317... Val Loss: 1.2218\n",
      "Epoch: 11/20... Step: 1910... Loss: 1.2115... Val Loss: 1.2227\n",
      "Epoch: 11/20... Step: 1920... Loss: 1.1740... Val Loss: 1.2237\n",
      "Epoch: 11/20... Step: 1930... Loss: 1.2216... Val Loss: 1.2209\n",
      "Epoch: 11/20... Step: 1940... Loss: 1.2170... Val Loss: 1.2120\n",
      "Epoch: 12/20... Step: 1950... Loss: 1.1972... Val Loss: 1.2284\n",
      "Epoch: 12/20... Step: 1960... Loss: 1.1871... Val Loss: 1.2233\n",
      "Epoch: 12/20... Step: 1970... Loss: 1.2263... Val Loss: 1.2173\n",
      "Epoch: 12/20... Step: 1980... Loss: 1.1753... Val Loss: 1.2187\n",
      "Epoch: 12/20... Step: 1990... Loss: 1.2152... Val Loss: 1.2143\n",
      "Epoch: 12/20... Step: 2000... Loss: 1.1792... Val Loss: 1.2133\n",
      "Epoch: 12/20... Step: 2010... Loss: 1.1869... Val Loss: 1.2189\n",
      "Epoch: 12/20... Step: 2020... Loss: 1.1828... Val Loss: 1.2122\n",
      "Epoch: 12/20... Step: 2030... Loss: 1.1478... Val Loss: 1.2106\n",
      "Epoch: 12/20... Step: 2040... Loss: 1.1859... Val Loss: 1.2129\n",
      "Epoch: 12/20... Step: 2050... Loss: 1.1676... Val Loss: 1.2124\n",
      "Epoch: 12/20... Step: 2060... Loss: 1.2000... Val Loss: 1.2079\n",
      "Epoch: 12/20... Step: 2070... Loss: 1.1838... Val Loss: 1.2102\n",
      "Epoch: 12/20... Step: 2080... Loss: 1.1652... Val Loss: 1.2038\n",
      "Epoch: 12/20... Step: 2090... Loss: 1.1674... Val Loss: 1.2020\n",
      "Epoch: 12/20... Step: 2100... Loss: 1.1861... Val Loss: 1.2036\n",
      "Epoch: 12/20... Step: 2110... Loss: 1.1650... Val Loss: 1.2003\n",
      "Epoch: 12/20... Step: 2120... Loss: 1.1597... Val Loss: 1.2093\n",
      "Epoch: 13/20... Step: 2130... Loss: 1.1775... Val Loss: 1.2065\n",
      "Epoch: 13/20... Step: 2140... Loss: 1.1701... Val Loss: 1.2022\n",
      "Epoch: 13/20... Step: 2150... Loss: 1.1650... Val Loss: 1.2002\n",
      "Epoch: 13/20... Step: 2160... Loss: 1.1737... Val Loss: 1.1953\n",
      "Epoch: 13/20... Step: 2170... Loss: 1.1450... Val Loss: 1.1960\n",
      "Epoch: 13/20... Step: 2180... Loss: 1.1550... Val Loss: 1.1933\n",
      "Epoch: 13/20... Step: 2190... Loss: 1.1392... Val Loss: 1.1955\n",
      "Epoch: 13/20... Step: 2200... Loss: 1.1498... Val Loss: 1.1990\n",
      "Epoch: 13/20... Step: 2210... Loss: 1.1711... Val Loss: 1.2004\n",
      "Epoch: 13/20... Step: 2220... Loss: 1.1287... Val Loss: 1.2059\n",
      "Epoch: 13/20... Step: 2230... Loss: 1.1650... Val Loss: 1.1964\n",
      "Epoch: 13/20... Step: 2240... Loss: 1.1693... Val Loss: 1.1949\n",
      "Epoch: 13/20... Step: 2250... Loss: 1.1633... Val Loss: 1.1984\n",
      "Epoch: 13/20... Step: 2260... Loss: 1.1490... Val Loss: 1.2065\n",
      "Epoch: 13/20... Step: 2270... Loss: 1.1754... Val Loss: 1.1923\n",
      "Epoch: 13/20... Step: 2280... Loss: 1.1359... Val Loss: 1.1895\n",
      "Epoch: 13/20... Step: 2290... Loss: 1.1853... Val Loss: 1.1868\n",
      "Epoch: 13/20... Step: 2300... Loss: 1.1561... Val Loss: 1.1889\n",
      "Epoch: 14/20... Step: 2310... Loss: 1.1395... Val Loss: 1.1890\n",
      "Epoch: 14/20... Step: 2320... Loss: 1.1438... Val Loss: 1.1881\n",
      "Epoch: 14/20... Step: 2330... Loss: 1.1293... Val Loss: 1.1843\n",
      "Epoch: 14/20... Step: 2340... Loss: 1.1611... Val Loss: 1.1827\n",
      "Epoch: 14/20... Step: 2350... Loss: 1.1563... Val Loss: 1.1839\n",
      "Epoch: 14/20... Step: 2360... Loss: 1.1584... Val Loss: 1.1884\n",
      "Epoch: 14/20... Step: 2370... Loss: 1.1262... Val Loss: 1.1847\n",
      "Epoch: 14/20... Step: 2380... Loss: 1.1350... Val Loss: 1.1870\n",
      "Epoch: 14/20... Step: 2390... Loss: 1.1486... Val Loss: 1.1837\n",
      "Epoch: 14/20... Step: 2400... Loss: 1.1370... Val Loss: 1.1863\n",
      "Epoch: 14/20... Step: 2410... Loss: 1.1321... Val Loss: 1.1877\n",
      "Epoch: 14/20... Step: 2420... Loss: 1.1458... Val Loss: 1.1881\n",
      "Epoch: 14/20... Step: 2430... Loss: 1.1603... Val Loss: 1.1840\n",
      "Epoch: 14/20... Step: 2440... Loss: 1.1543... Val Loss: 1.1818\n",
      "Epoch: 14/20... Step: 2450... Loss: 1.1182... Val Loss: 1.1802\n",
      "Epoch: 14/20... Step: 2460... Loss: 1.1478... Val Loss: 1.1806\n",
      "Epoch: 14/20... Step: 2470... Loss: 1.1723... Val Loss: 1.1759\n",
      "Epoch: 15/20... Step: 2480... Loss: 1.1328... Val Loss: 1.1811\n",
      "Epoch: 15/20... Step: 2490... Loss: 1.1298... Val Loss: 1.1788\n",
      "Epoch: 15/20... Step: 2500... Loss: 1.1287... Val Loss: 1.1817\n",
      "Epoch: 15/20... Step: 2510... Loss: 1.1125... Val Loss: 1.1800\n",
      "Epoch: 15/20... Step: 2520... Loss: 1.1370... Val Loss: 1.1741\n",
      "Epoch: 15/20... Step: 2530... Loss: 1.1306... Val Loss: 1.1782\n",
      "Epoch: 15/20... Step: 2540... Loss: 1.1065... Val Loss: 1.1817\n",
      "Epoch: 15/20... Step: 2550... Loss: 1.1071... Val Loss: 1.1813\n",
      "Epoch: 15/20... Step: 2560... Loss: 1.1017... Val Loss: 1.1779\n",
      "Epoch: 15/20... Step: 2570... Loss: 1.1286... Val Loss: 1.1763\n",
      "Epoch: 15/20... Step: 2580... Loss: 1.1371... Val Loss: 1.1782\n",
      "Epoch: 15/20... Step: 2590... Loss: 1.1291... Val Loss: 1.1768\n",
      "Epoch: 15/20... Step: 2600... Loss: 1.1288... Val Loss: 1.1795\n",
      "Epoch: 15/20... Step: 2610... Loss: 1.1304... Val Loss: 1.1748\n",
      "Epoch: 15/20... Step: 2620... Loss: 1.1173... Val Loss: 1.1742\n",
      "Epoch: 15/20... Step: 2630... Loss: 1.1135... Val Loss: 1.1791\n",
      "Epoch: 15/20... Step: 2640... Loss: 1.1182... Val Loss: 1.1751\n",
      "Epoch: 15/20... Step: 2650... Loss: 1.1196... Val Loss: 1.1790\n",
      "Epoch: 16/20... Step: 2660... Loss: 1.1262... Val Loss: 1.1781\n",
      "Epoch: 16/20... Step: 2670... Loss: 1.1210... Val Loss: 1.1770\n",
      "Epoch: 16/20... Step: 2680... Loss: 1.1159... Val Loss: 1.1725\n",
      "Epoch: 16/20... Step: 2690... Loss: 1.1121... Val Loss: 1.1673\n",
      "Epoch: 16/20... Step: 2700... Loss: 1.1121... Val Loss: 1.1710\n",
      "Epoch: 16/20... Step: 2710... Loss: 1.1061... Val Loss: 1.1694\n",
      "Epoch: 16/20... Step: 2720... Loss: 1.1076... Val Loss: 1.1717\n",
      "Epoch: 16/20... Step: 2730... Loss: 1.0995... Val Loss: 1.1750\n",
      "Epoch: 16/20... Step: 2740... Loss: 1.0959... Val Loss: 1.1794\n",
      "Epoch: 16/20... Step: 2750... Loss: 1.1253... Val Loss: 1.1783\n",
      "Epoch: 16/20... Step: 2760... Loss: 1.1011... Val Loss: 1.1733\n",
      "Epoch: 16/20... Step: 2770... Loss: 1.1079... Val Loss: 1.1715\n",
      "Epoch: 16/20... Step: 2780... Loss: 1.1298... Val Loss: 1.1691\n",
      "Epoch: 16/20... Step: 2790... Loss: 1.1003... Val Loss: 1.1689\n",
      "Epoch: 16/20... Step: 2800... Loss: 1.0937... Val Loss: 1.1720\n",
      "Epoch: 16/20... Step: 2810... Loss: 1.1072... Val Loss: 1.1771\n",
      "Epoch: 16/20... Step: 2820... Loss: 1.1242... Val Loss: 1.1726\n",
      "Epoch: 16/20... Step: 2830... Loss: 1.1146... Val Loss: 1.1702\n",
      "Epoch: 17/20... Step: 2840... Loss: 1.1117... Val Loss: 1.1690\n",
      "Epoch: 17/20... Step: 2850... Loss: 1.0921... Val Loss: 1.1660\n",
      "Epoch: 17/20... Step: 2860... Loss: 1.0819... Val Loss: 1.1687\n",
      "Epoch: 17/20... Step: 2870... Loss: 1.1199... Val Loss: 1.1667\n",
      "Epoch: 17/20... Step: 2880... Loss: 1.0915... Val Loss: 1.1657\n",
      "Epoch: 17/20... Step: 2890... Loss: 1.0944... Val Loss: 1.1654\n",
      "Epoch: 17/20... Step: 2900... Loss: 1.1038... Val Loss: 1.1629\n",
      "Epoch: 17/20... Step: 2910... Loss: 1.0895... Val Loss: 1.1662\n",
      "Epoch: 17/20... Step: 2920... Loss: 1.0905... Val Loss: 1.1701\n",
      "Epoch: 17/20... Step: 2930... Loss: 1.0798... Val Loss: 1.1764\n",
      "Epoch: 17/20... Step: 2940... Loss: 1.1248... Val Loss: 1.1724\n",
      "Epoch: 17/20... Step: 2950... Loss: 1.0873... Val Loss: 1.1740\n",
      "Epoch: 17/20... Step: 2960... Loss: 1.1228... Val Loss: 1.1700\n",
      "Epoch: 17/20... Step: 2970... Loss: 1.1076... Val Loss: 1.1682\n",
      "Epoch: 17/20... Step: 2980... Loss: 1.1035... Val Loss: 1.1653\n",
      "Epoch: 17/20... Step: 2990... Loss: 1.1004... Val Loss: 1.1677\n",
      "Epoch: 17/20... Step: 3000... Loss: 1.1151... Val Loss: 1.1634\n",
      "Epoch: 18/20... Step: 3010... Loss: 1.2060... Val Loss: 1.1617\n",
      "Epoch: 18/20... Step: 3020... Loss: 1.0829... Val Loss: 1.1613\n",
      "Epoch: 18/20... Step: 3030... Loss: 1.0852... Val Loss: 1.1604\n",
      "Epoch: 18/20... Step: 3040... Loss: 1.0679... Val Loss: 1.1615\n",
      "Epoch: 18/20... Step: 3050... Loss: 1.0868... Val Loss: 1.1571\n",
      "Epoch: 18/20... Step: 3060... Loss: 1.0828... Val Loss: 1.1590\n",
      "Epoch: 18/20... Step: 3070... Loss: 1.0724... Val Loss: 1.1609\n",
      "Epoch: 18/20... Step: 3080... Loss: 1.0845... Val Loss: 1.1621\n",
      "Epoch: 18/20... Step: 3090... Loss: 1.0842... Val Loss: 1.1626\n",
      "Epoch: 18/20... Step: 3100... Loss: 1.0866... Val Loss: 1.1591\n",
      "Epoch: 18/20... Step: 3110... Loss: 1.0938... Val Loss: 1.1602\n",
      "Epoch: 18/20... Step: 3120... Loss: 1.0594... Val Loss: 1.1649\n",
      "Epoch: 18/20... Step: 3130... Loss: 1.0839... Val Loss: 1.1662\n",
      "Epoch: 18/20... Step: 3140... Loss: 1.0808... Val Loss: 1.1661\n",
      "Epoch: 18/20... Step: 3150... Loss: 1.0961... Val Loss: 1.1629\n",
      "Epoch: 18/20... Step: 3160... Loss: 1.0833... Val Loss: 1.1712\n",
      "Epoch: 18/20... Step: 3170... Loss: 1.0942... Val Loss: 1.1665\n",
      "Epoch: 18/20... Step: 3180... Loss: 1.0791... Val Loss: 1.1606\n",
      "Epoch: 19/20... Step: 3190... Loss: 1.0893... Val Loss: 1.1679\n",
      "Epoch: 19/20... Step: 3200... Loss: 1.0672... Val Loss: 1.1598\n",
      "Epoch: 19/20... Step: 3210... Loss: 1.0737... Val Loss: 1.1556\n",
      "Epoch: 19/20... Step: 3220... Loss: 1.1018... Val Loss: 1.1598\n",
      "Epoch: 19/20... Step: 3230... Loss: 1.0809... Val Loss: 1.1526\n",
      "Epoch: 19/20... Step: 3240... Loss: 1.0775... Val Loss: 1.1527\n",
      "Epoch: 19/20... Step: 3250... Loss: 1.0382... Val Loss: 1.1579\n",
      "Epoch: 19/20... Step: 3260... Loss: 1.0441... Val Loss: 1.1582\n",
      "Epoch: 19/20... Step: 3270... Loss: 1.0572... Val Loss: 1.1603\n",
      "Epoch: 19/20... Step: 3280... Loss: 1.0626... Val Loss: 1.1575\n",
      "Epoch: 19/20... Step: 3290... Loss: 1.0701... Val Loss: 1.1557\n",
      "Epoch: 19/20... Step: 3300... Loss: 1.0958... Val Loss: 1.1620\n",
      "Epoch: 19/20... Step: 3310... Loss: 1.0934... Val Loss: 1.1580\n",
      "Epoch: 19/20... Step: 3320... Loss: 1.0676... Val Loss: 1.1569\n",
      "Epoch: 19/20... Step: 3330... Loss: 1.0651... Val Loss: 1.1587\n",
      "Epoch: 19/20... Step: 3340... Loss: 1.0745... Val Loss: 1.1612\n",
      "Epoch: 19/20... Step: 3350... Loss: 1.0873... Val Loss: 1.1603\n",
      "Epoch: 19/20... Step: 3360... Loss: 1.0622... Val Loss: 1.1681\n",
      "Epoch: 20/20... Step: 3370... Loss: 1.0879... Val Loss: 1.1646\n",
      "Epoch: 20/20... Step: 3380... Loss: 1.0485... Val Loss: 1.1607\n",
      "Epoch: 20/20... Step: 3390... Loss: 1.0693... Val Loss: 1.1604\n",
      "Epoch: 20/20... Step: 3400... Loss: 1.0507... Val Loss: 1.1545\n",
      "Epoch: 20/20... Step: 3410... Loss: 1.0652... Val Loss: 1.1543\n",
      "Epoch: 20/20... Step: 3420... Loss: 1.0647... Val Loss: 1.1557\n",
      "Epoch: 20/20... Step: 3430... Loss: 1.0796... Val Loss: 1.1564\n",
      "Epoch: 20/20... Step: 3440... Loss: 1.0513... Val Loss: 1.1565\n",
      "Epoch: 20/20... Step: 3450... Loss: 1.0518... Val Loss: 1.1574\n",
      "Epoch: 20/20... Step: 3460... Loss: 1.0484... Val Loss: 1.1610\n",
      "Epoch: 20/20... Step: 3470... Loss: 1.0696... Val Loss: 1.1573\n",
      "Epoch: 20/20... Step: 3480... Loss: 1.0616... Val Loss: 1.1564\n",
      "Epoch: 20/20... Step: 3490... Loss: 1.0736... Val Loss: 1.1547\n",
      "Epoch: 20/20... Step: 3500... Loss: 1.0556... Val Loss: 1.1546\n",
      "Epoch: 20/20... Step: 3510... Loss: 1.0502... Val Loss: 1.1521\n",
      "Epoch: 20/20... Step: 3520... Loss: 1.0802... Val Loss: 1.1504\n",
      "Epoch: 20/20... Step: 3530... Loss: 1.0673... Val Loss: 1.1553\n",
      "Epoch: 20/20... Step: 3540... Loss: 1.1002... Val Loss: 1.1583\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "batch_size = 128\n",
    "seq_length = 128\n",
    "# start small if you are just testing initial behavior\n",
    "n_epochs = 20\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rt6EZ2fAFmId"
   },
   "source": [
    "#### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lE_4pTKgFOW7",
    "outputId": "a80befda-8997-4458-847a-7418e2824dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pávlovna’s handsome face, she was not at the same time, as a calam\n",
      "to back his hands and a shappy mood in his fach, without like him, and so often this\n",
      "and we are wanting for a moment in the marniage, but wished to say, and the\n",
      "capital of having seeing Bezúkhov to the story was standing on his sister’s\n",
      "hand.\n",
      "\n",
      "“Well, all will be done in your cousin.\n",
      "I will take the more than our little collasting,””\n",
      "rated Pierre also. “You know the Emperor alone should be settled\n",
      "in the same, only an officer with a men and such a special mising whell the\n",
      "countess wants to speak of all the conversation with the wind, and had\n",
      "to bran at all a military country, but they took them and who was\n",
      "not the prait and has no officer and we are with my family and that is the\n",
      "commin in the conversation was impossible to ask them all to be the whole monarch to\n",
      "the princess. Had he not to be sent for his friend’s for the former things\n",
      "to attack him, when he had to break it in at timide and directed the poor\n",
      "princess as\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RmctRtLFOao"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldb3f-EE5DAW"
   },
   "source": [
    "### Model_3: Three LSTM-layers\n",
    "n_hidden = 1024  \n",
    "n_layers = 3  \n",
    "drop_prob=0.4  \n",
    "batch_size = 254  \n",
    "seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0-PAnGosDki",
    "outputId": "c957ea80-1d09-499f-c6f0-ddee33169e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(112, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=1024, out_features=112, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set your model hyperparameters\n",
    "\n",
    "## YOUR CODE HERE\n",
    "n_hidden = 1024\n",
    "n_layers = 3\n",
    "drop_prob=0.4\n",
    "\n",
    "net = CharRNN(tuple(set(text)), n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCNDng5esDnf",
    "outputId": "2e32596c-d0a4-4c5b-a3bf-f5388789e793"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.1959... Val Loss: 3.1057\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1809... Val Loss: 3.0966\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1537... Val Loss: 3.0911\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1580... Val Loss: 3.0846\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1590... Val Loss: 3.0859\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1350... Val Loss: 3.0833\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1156... Val Loss: 3.0836\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1320... Val Loss: 3.0826\n",
      "Epoch: 2/20... Step: 90... Loss: 3.1468... Val Loss: 3.0829\n",
      "Epoch: 2/20... Step: 100... Loss: 3.1459... Val Loss: 3.0823\n",
      "Epoch: 2/20... Step: 110... Loss: 3.1247... Val Loss: 3.0604\n",
      "Epoch: 2/20... Step: 120... Loss: 3.0901... Val Loss: 3.0277\n",
      "Epoch: 2/20... Step: 130... Loss: 2.9768... Val Loss: 2.9134\n",
      "Epoch: 2/20... Step: 140... Loss: 2.9088... Val Loss: 2.8268\n",
      "Epoch: 2/20... Step: 150... Loss: 2.8409... Val Loss: 2.7761\n",
      "Epoch: 2/20... Step: 160... Loss: 2.8133... Val Loss: 2.7301\n",
      "Epoch: 2/20... Step: 170... Loss: 2.7726... Val Loss: 2.6958\n",
      "Epoch: 3/20... Step: 180... Loss: 2.7093... Val Loss: 2.6513\n",
      "Epoch: 3/20... Step: 190... Loss: 2.6748... Val Loss: 2.6070\n",
      "Epoch: 3/20... Step: 200... Loss: 2.6070... Val Loss: 2.5306\n",
      "Epoch: 3/20... Step: 210... Loss: 2.4919... Val Loss: 2.4400\n",
      "Epoch: 3/20... Step: 220... Loss: 2.3924... Val Loss: 2.3500\n",
      "Epoch: 3/20... Step: 230... Loss: 2.3597... Val Loss: 2.2799\n",
      "Epoch: 3/20... Step: 240... Loss: 2.3050... Val Loss: 2.2353\n",
      "Epoch: 3/20... Step: 250... Loss: 2.2694... Val Loss: 2.1970\n",
      "Epoch: 3/20... Step: 260... Loss: 2.2211... Val Loss: 2.1553\n",
      "Epoch: 4/20... Step: 270... Loss: 2.1829... Val Loss: 2.1148\n",
      "Epoch: 4/20... Step: 280... Loss: 2.1454... Val Loss: 2.0745\n",
      "Epoch: 4/20... Step: 290... Loss: 2.1131... Val Loss: 2.0405\n",
      "Epoch: 4/20... Step: 300... Loss: 2.0776... Val Loss: 1.9981\n",
      "Epoch: 4/20... Step: 310... Loss: 2.0436... Val Loss: 1.9771\n",
      "Epoch: 4/20... Step: 320... Loss: 2.0091... Val Loss: 1.9352\n",
      "Epoch: 4/20... Step: 330... Loss: 1.9999... Val Loss: 1.9026\n",
      "Epoch: 4/20... Step: 340... Loss: 1.9778... Val Loss: 1.8739\n",
      "Epoch: 4/20... Step: 350... Loss: 1.9551... Val Loss: 1.8435\n",
      "Epoch: 5/20... Step: 360... Loss: 1.9153... Val Loss: 1.8252\n",
      "Epoch: 5/20... Step: 370... Loss: 1.8696... Val Loss: 1.7980\n",
      "Epoch: 5/20... Step: 380... Loss: 1.8513... Val Loss: 1.7711\n",
      "Epoch: 5/20... Step: 390... Loss: 1.8283... Val Loss: 1.7486\n",
      "Epoch: 5/20... Step: 400... Loss: 1.8232... Val Loss: 1.7238\n",
      "Epoch: 5/20... Step: 410... Loss: 1.7844... Val Loss: 1.7172\n",
      "Epoch: 5/20... Step: 420... Loss: 1.7676... Val Loss: 1.6861\n",
      "Epoch: 5/20... Step: 430... Loss: 1.7417... Val Loss: 1.6638\n",
      "Epoch: 5/20... Step: 440... Loss: 1.7373... Val Loss: 1.6448\n",
      "Epoch: 6/20... Step: 450... Loss: 1.7437... Val Loss: 1.6223\n",
      "Epoch: 6/20... Step: 460... Loss: 1.6881... Val Loss: 1.6064\n",
      "Epoch: 6/20... Step: 470... Loss: 1.6777... Val Loss: 1.5913\n",
      "Epoch: 6/20... Step: 480... Loss: 1.6636... Val Loss: 1.5769\n",
      "Epoch: 6/20... Step: 490... Loss: 1.6219... Val Loss: 1.5652\n",
      "Epoch: 6/20... Step: 500... Loss: 1.6442... Val Loss: 1.5520\n",
      "Epoch: 6/20... Step: 510... Loss: 1.6102... Val Loss: 1.5368\n",
      "Epoch: 6/20... Step: 520... Loss: 1.5892... Val Loss: 1.5260\n",
      "Epoch: 6/20... Step: 530... Loss: 1.5932... Val Loss: 1.5095\n",
      "Epoch: 7/20... Step: 540... Loss: 1.5888... Val Loss: 1.4965\n",
      "Epoch: 7/20... Step: 550... Loss: 1.5688... Val Loss: 1.4839\n",
      "Epoch: 7/20... Step: 560... Loss: 1.5626... Val Loss: 1.4748\n",
      "Epoch: 7/20... Step: 570... Loss: 1.5455... Val Loss: 1.4632\n",
      "Epoch: 7/20... Step: 580... Loss: 1.5221... Val Loss: 1.4585\n",
      "Epoch: 7/20... Step: 590... Loss: 1.5315... Val Loss: 1.4482\n",
      "Epoch: 7/20... Step: 600... Loss: 1.5045... Val Loss: 1.4378\n",
      "Epoch: 7/20... Step: 610... Loss: 1.5086... Val Loss: 1.4297\n",
      "Epoch: 7/20... Step: 620... Loss: 1.4993... Val Loss: 1.4210\n",
      "Epoch: 8/20... Step: 630... Loss: 1.5187... Val Loss: 1.4139\n",
      "Epoch: 8/20... Step: 640... Loss: 1.4735... Val Loss: 1.4063\n",
      "Epoch: 8/20... Step: 650... Loss: 1.4792... Val Loss: 1.3969\n",
      "Epoch: 8/20... Step: 660... Loss: 1.4488... Val Loss: 1.3909\n",
      "Epoch: 8/20... Step: 670... Loss: 1.4471... Val Loss: 1.3853\n",
      "Epoch: 8/20... Step: 680... Loss: 1.4345... Val Loss: 1.3814\n",
      "Epoch: 8/20... Step: 690... Loss: 1.4392... Val Loss: 1.3808\n",
      "Epoch: 8/20... Step: 700... Loss: 1.4424... Val Loss: 1.3686\n",
      "Epoch: 8/20... Step: 710... Loss: 1.4358... Val Loss: 1.3639\n",
      "Epoch: 9/20... Step: 720... Loss: 1.4389... Val Loss: 1.3620\n",
      "Epoch: 9/20... Step: 730... Loss: 1.4232... Val Loss: 1.3514\n",
      "Epoch: 9/20... Step: 740... Loss: 1.4129... Val Loss: 1.3456\n",
      "Epoch: 9/20... Step: 750... Loss: 1.4153... Val Loss: 1.3431\n",
      "Epoch: 9/20... Step: 760... Loss: 1.3958... Val Loss: 1.3375\n",
      "Epoch: 9/20... Step: 770... Loss: 1.3863... Val Loss: 1.3361\n",
      "Epoch: 9/20... Step: 780... Loss: 1.3892... Val Loss: 1.3358\n",
      "Epoch: 9/20... Step: 790... Loss: 1.3923... Val Loss: 1.3281\n",
      "Epoch: 9/20... Step: 800... Loss: 1.3596... Val Loss: 1.3224\n",
      "Epoch: 10/20... Step: 810... Loss: 1.3862... Val Loss: 1.3246\n",
      "Epoch: 10/20... Step: 820... Loss: 1.3865... Val Loss: 1.3146\n",
      "Epoch: 10/20... Step: 830... Loss: 1.3597... Val Loss: 1.3079\n",
      "Epoch: 10/20... Step: 840... Loss: 1.3523... Val Loss: 1.3064\n",
      "Epoch: 10/20... Step: 850... Loss: 1.3529... Val Loss: 1.3045\n",
      "Epoch: 10/20... Step: 860... Loss: 1.3415... Val Loss: 1.3036\n",
      "Epoch: 10/20... Step: 870... Loss: 1.3317... Val Loss: 1.3004\n",
      "Epoch: 10/20... Step: 880... Loss: 1.3389... Val Loss: 1.2968\n",
      "Epoch: 10/20... Step: 890... Loss: 1.3755... Val Loss: 1.2902\n",
      "Epoch: 11/20... Step: 900... Loss: 1.3329... Val Loss: 1.2925\n",
      "Epoch: 11/20... Step: 910... Loss: 1.3311... Val Loss: 1.2905\n",
      "Epoch: 11/20... Step: 920... Loss: 1.3390... Val Loss: 1.2812\n",
      "Epoch: 11/20... Step: 930... Loss: 1.3129... Val Loss: 1.2796\n",
      "Epoch: 11/20... Step: 940... Loss: 1.3332... Val Loss: 1.2789\n",
      "Epoch: 11/20... Step: 950... Loss: 1.3050... Val Loss: 1.2783\n",
      "Epoch: 11/20... Step: 960... Loss: 1.3109... Val Loss: 1.2734\n",
      "Epoch: 11/20... Step: 970... Loss: 1.3130... Val Loss: 1.2711\n",
      "Epoch: 12/20... Step: 980... Loss: 1.3515... Val Loss: 1.2705\n",
      "Epoch: 12/20... Step: 990... Loss: 1.3220... Val Loss: 1.2713\n",
      "Epoch: 12/20... Step: 1000... Loss: 1.3073... Val Loss: 1.2665\n",
      "Epoch: 12/20... Step: 1010... Loss: 1.2924... Val Loss: 1.2590\n",
      "Epoch: 12/20... Step: 1020... Loss: 1.2869... Val Loss: 1.2565\n",
      "Epoch: 12/20... Step: 1030... Loss: 1.2833... Val Loss: 1.2556\n",
      "Epoch: 12/20... Step: 1040... Loss: 1.2881... Val Loss: 1.2566\n",
      "Epoch: 12/20... Step: 1050... Loss: 1.2850... Val Loss: 1.2515\n",
      "Epoch: 12/20... Step: 1060... Loss: 1.3012... Val Loss: 1.2498\n",
      "Epoch: 13/20... Step: 1070... Loss: 1.2866... Val Loss: 1.2485\n",
      "Epoch: 13/20... Step: 1080... Loss: 1.2694... Val Loss: 1.2483\n",
      "Epoch: 13/20... Step: 1090... Loss: 1.2926... Val Loss: 1.2438\n",
      "Epoch: 13/20... Step: 1100... Loss: 1.2731... Val Loss: 1.2445\n",
      "Epoch: 13/20... Step: 1110... Loss: 1.2502... Val Loss: 1.2450\n",
      "Epoch: 13/20... Step: 1120... Loss: 1.2706... Val Loss: 1.2420\n",
      "Epoch: 13/20... Step: 1130... Loss: 1.2735... Val Loss: 1.2396\n",
      "Epoch: 13/20... Step: 1140... Loss: 1.2748... Val Loss: 1.2366\n",
      "Epoch: 13/20... Step: 1150... Loss: 1.2749... Val Loss: 1.2354\n",
      "Epoch: 14/20... Step: 1160... Loss: 1.2646... Val Loss: 1.2345\n",
      "Epoch: 14/20... Step: 1170... Loss: 1.2561... Val Loss: 1.2463\n",
      "Epoch: 14/20... Step: 1180... Loss: 1.2505... Val Loss: 1.2340\n",
      "Epoch: 14/20... Step: 1190... Loss: 1.2532... Val Loss: 1.2298\n",
      "Epoch: 14/20... Step: 1200... Loss: 1.2279... Val Loss: 1.2289\n",
      "Epoch: 14/20... Step: 1210... Loss: 1.2358... Val Loss: 1.2280\n",
      "Epoch: 14/20... Step: 1220... Loss: 1.2425... Val Loss: 1.2262\n",
      "Epoch: 14/20... Step: 1230... Loss: 1.2517... Val Loss: 1.2261\n",
      "Epoch: 14/20... Step: 1240... Loss: 1.2421... Val Loss: 1.2221\n",
      "Epoch: 15/20... Step: 1250... Loss: 1.2453... Val Loss: 1.2189\n",
      "Epoch: 15/20... Step: 1260... Loss: 1.2347... Val Loss: 1.2185\n",
      "Epoch: 15/20... Step: 1270... Loss: 1.2247... Val Loss: 1.2148\n",
      "Epoch: 15/20... Step: 1280... Loss: 1.2264... Val Loss: 1.2153\n",
      "Epoch: 15/20... Step: 1290... Loss: 1.2260... Val Loss: 1.2166\n",
      "Epoch: 15/20... Step: 1300... Loss: 1.2159... Val Loss: 1.2187\n",
      "Epoch: 15/20... Step: 1310... Loss: 1.2170... Val Loss: 1.2139\n",
      "Epoch: 15/20... Step: 1320... Loss: 1.2159... Val Loss: 1.2158\n",
      "Epoch: 15/20... Step: 1330... Loss: 1.2358... Val Loss: 1.2098\n",
      "Epoch: 16/20... Step: 1340... Loss: 1.2390... Val Loss: 1.2106\n",
      "Epoch: 16/20... Step: 1350... Loss: 1.2135... Val Loss: 1.2090\n",
      "Epoch: 16/20... Step: 1360... Loss: 1.2097... Val Loss: 1.2044\n",
      "Epoch: 16/20... Step: 1370... Loss: 1.2085... Val Loss: 1.2091\n",
      "Epoch: 16/20... Step: 1380... Loss: 1.1750... Val Loss: 1.2029\n",
      "Epoch: 16/20... Step: 1390... Loss: 1.2116... Val Loss: 1.2050\n",
      "Epoch: 16/20... Step: 1400... Loss: 1.2018... Val Loss: 1.2057\n",
      "Epoch: 16/20... Step: 1410... Loss: 1.1940... Val Loss: 1.2035\n",
      "Epoch: 16/20... Step: 1420... Loss: 1.2046... Val Loss: 1.2005\n",
      "Epoch: 17/20... Step: 1430... Loss: 1.2117... Val Loss: 1.1977\n",
      "Epoch: 17/20... Step: 1440... Loss: 1.1941... Val Loss: 1.2006\n",
      "Epoch: 17/20... Step: 1450... Loss: 1.1947... Val Loss: 1.2003\n",
      "Epoch: 17/20... Step: 1460... Loss: 1.1931... Val Loss: 1.2001\n",
      "Epoch: 17/20... Step: 1470... Loss: 1.1736... Val Loss: 1.1981\n",
      "Epoch: 17/20... Step: 1480... Loss: 1.1862... Val Loss: 1.1947\n",
      "Epoch: 17/20... Step: 1490... Loss: 1.1795... Val Loss: 1.1948\n",
      "Epoch: 17/20... Step: 1500... Loss: 1.1891... Val Loss: 1.2006\n",
      "Epoch: 17/20... Step: 1510... Loss: 1.1823... Val Loss: 1.1985\n",
      "Epoch: 18/20... Step: 1520... Loss: 1.2183... Val Loss: 1.1891\n",
      "Epoch: 18/20... Step: 1530... Loss: 1.1765... Val Loss: 1.1911\n",
      "Epoch: 18/20... Step: 1540... Loss: 1.1887... Val Loss: 1.1895\n",
      "Epoch: 18/20... Step: 1550... Loss: 1.1691... Val Loss: 1.1950\n",
      "Epoch: 18/20... Step: 1560... Loss: 1.1598... Val Loss: 1.1878\n",
      "Epoch: 18/20... Step: 1570... Loss: 1.1610... Val Loss: 1.1871\n",
      "Epoch: 18/20... Step: 1580... Loss: 1.1668... Val Loss: 1.1874\n",
      "Epoch: 18/20... Step: 1590... Loss: 1.1791... Val Loss: 1.1861\n",
      "Epoch: 18/20... Step: 1600... Loss: 1.1709... Val Loss: 1.1902\n",
      "Epoch: 19/20... Step: 1610... Loss: 1.1877... Val Loss: 1.1894\n",
      "Epoch: 19/20... Step: 1620... Loss: 1.1700... Val Loss: 1.1837\n",
      "Epoch: 19/20... Step: 1630... Loss: 1.1618... Val Loss: 1.1865\n",
      "Epoch: 19/20... Step: 1640... Loss: 1.1756... Val Loss: 1.1858\n",
      "Epoch: 19/20... Step: 1650... Loss: 1.1634... Val Loss: 1.1816\n",
      "Epoch: 19/20... Step: 1660... Loss: 1.1528... Val Loss: 1.1814\n",
      "Epoch: 19/20... Step: 1670... Loss: 1.1574... Val Loss: 1.1835\n",
      "Epoch: 19/20... Step: 1680... Loss: 1.1523... Val Loss: 1.1791\n",
      "Epoch: 19/20... Step: 1690... Loss: 1.1409... Val Loss: 1.1814\n",
      "Epoch: 20/20... Step: 1700... Loss: 1.1676... Val Loss: 1.1794\n",
      "Epoch: 20/20... Step: 1710... Loss: 1.1722... Val Loss: 1.1845\n",
      "Epoch: 20/20... Step: 1720... Loss: 1.1461... Val Loss: 1.1810\n",
      "Epoch: 20/20... Step: 1730... Loss: 1.1433... Val Loss: 1.1808\n",
      "Epoch: 20/20... Step: 1740... Loss: 1.1495... Val Loss: 1.1774\n",
      "Epoch: 20/20... Step: 1750... Loss: 1.1412... Val Loss: 1.1757\n",
      "Epoch: 20/20... Step: 1760... Loss: 1.1299... Val Loss: 1.1782\n",
      "Epoch: 20/20... Step: 1770... Loss: 1.1311... Val Loss: 1.1764\n",
      "Epoch: 20/20... Step: 1780... Loss: 1.1835... Val Loss: 1.1755\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "batch_size = 254\n",
    "seq_length = 128\n",
    "# start small if you are just testing initial behavior\n",
    "n_epochs = 20\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEF3_nBJ9cQg"
   },
   "source": [
    "#### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBBZsnQGsDsr",
    "outputId": "9c7920e7-0b86-4dea-a2b6-c7bab0e85c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Pávlovna was already contented.\n",
      "\n",
      "“You are gloamy and save your ball, in the same time if you have not?” his face\n",
      "added: “Well.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER XIV\n",
      "\n",
      "On the first time he sat down on a small position of her state, and a cheerful proper firilla and at the count had at once lasted his country with a commander who had been absent. The state of the count, who were carry in with her as if to talk in the fact as\n",
      "it\n",
      "was sitting on the firm. The cannon, were trying to repore to himself: “I have seen\n",
      "those more steps off! How\n",
      "did you want to,” said Prince Andrew, said something: “I shall not hear of this son. We shall not see you. If they are to serve.” She\n",
      "recalled the soldiers who,\n",
      "were at last as a conversation and tormented him. He was a single ance of water which had been straightened, and the face, but he did not rose and wait in the\n",
      "door.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER XX\n",
      "\n",
      "In the stafes, and the people at the campaign were to be an excited\n",
      "cap and the pleasure, which had so properly, but she was, as a man wer\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Prepared by Sergey Afanasiev*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SGA2_Text Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
