{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice assignment: Advanced ensembling techniques\n",
    "This assignment is graded by your `submission.json`.\n",
    "\n",
    "The cell below creates a valid `submission.json` file, fill your answers in there. \n",
    "\n",
    "You can press \"Submit Assignment\" at any time to submit partial progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submission.json\n"
     ]
    }
   ],
   "source": [
    "%%file submission.json\n",
    "{\n",
    "    \"q1\": 0,\n",
    "    \"q2\": 843300,\n",
    "    \"q3\": 1400,\n",
    "    \"q4\": 3349.74057,\n",
    "    \"q5\": 10329.20768,\n",
    "    \"q6\": 8890.23464,\n",
    "    \"q7\": 8692.77417,\n",
    "    \"q8\": \"data_channel_is_bus\",\n",
    "    \"q9\": 8451.2859,\n",
    "    \"q10\": 8421.6219,\n",
    "    \"q11\": \"self_reference_min_shares\",\n",
    "    \"q12\": 8527.92717,\n",
    "    \"q13\": 8485.01086,\n",
    "    \"q14\": 8465.44037,\n",
    "    \"q15\": \"kw_avg_avg\",\n",
    "    \"q16\": 8426.97894,\n",
    "    \"q17\": 2,\n",
    "    \"q18\": 2880.08592,\n",
    "    \"q19\": 0.84346,\n",
    "    \"q20\": 8438.77113,\n",
    "    \"q21\": 8411.04015,\n",
    "    \"q22\": 8441.65406\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XgQKq6Ldl-g"
   },
   "source": [
    "In this programming assignment, you are going to work with a dataset based on the following data:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n",
    "\n",
    "_Citation:_\n",
    "\n",
    "* _K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal._\n",
    "\n",
    "The dataset contains the information about the internet news articles. In this assignment, you are going to predict a number of shares of the news article (target column: `shares`). The information about the features is available through the link above. You are going to construct several machine learning algorithms (XGBoost, LightGBM, CatBoost and Lasso) and blend them into the final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qx7Y3W-_3LPT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ROnt-v3A5Rzy"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-95PM9m56JZ"
   },
   "source": [
    "## 1\n",
    "\n",
    "**q1:** How many missing values are there in the data? Provide the number of cells in the dataframe that contain NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AuyhBV3ULyIj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "nans = df.isna().sum().sum()\n",
    "print(nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-PSn5w66Ylo"
   },
   "source": [
    "## 2\n",
    "\n",
    "**q2:** What is the maximum number of shares among all the news articles presented in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tA6IUIgZL0JP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843300\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "maxn = df.shares.max()\n",
    "print(maxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7SEP8pj9VQf"
   },
   "source": [
    "## 3\n",
    "\n",
    "**q3:** What is the median number of shares for the articles published on Monday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E8acRylcL4eB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "median_monday = df[df.weekday_is_monday == 1].shares.median()\n",
    "print(median_monday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJRiUrSt9gWS"
   },
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36BGDOx19peU"
   },
   "source": [
    "First, we separate the target from the dataframe with features (`df` -> `X`, `y`).\n",
    "\n",
    "Next, let's split the data into train/val/test sets in the ratio 60:20:20. The idea is that we will use train set to train our models, val set to validate them and test set to calculate the final error of the blend. So, test set will be a completely unseen data.\n",
    "\n",
    "To do this, use a regular `train_test_split` from `sklearn` to split `X` and `y` into train and val/test parts in the ratio 60:40. Then use `train_test_split` again, but to split the obtain val/test part into validation and test in the ratio 50:50. In each `train_test_split` application, use `random_state=13` and other default parameter values.\n",
    "\n",
    "In the end, you should obtain `X_train`, `X_val`, `X_test` with the following shapes, respectively: (23786, 58), (7929, 58), (7929, 58). The same logic is with `y_train`, `y_val`, `y_test`.\n",
    "\n",
    "**q4:** What is the mean value of target in the test part (`X_test`)? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x18Nt2Ln9hAQ"
   },
   "outputs": [],
   "source": [
    "X = df.drop('shares', axis=1)\n",
    "y = df['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JBpG87eiL7DY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3349.74057\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, random_state=13, test_size=0.4)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, random_state=13, test_size=0.5)\n",
    "\n",
    "mean_tt = round(y_test.mean(), 5)\n",
    "print(mean_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zijl0TGp_8P2"
   },
   "source": [
    "## 5\n",
    "\n",
    "Now let's train our first model - XGBoost. A link to the documentation: https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "We will use Scikit-Learn Wrapper interface for XGBoost (and the same logic applies to the following LightGBM and CatBoost models). Here, we work on the regression task - hence we will use `XGBRegressor`. Read about the parameters of `XGBRegressor`: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "\n",
    "The main list of XGBoost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html# Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `XGBRegressor` with MSE objective (`objective='reg:squarederror'`), 200 trees (`n_estimators=200`), `learning_rate=0.01`, `max_depth=5`, `random_state=13` and all other default parameter values. Train it on the train set (`fit` function). \n",
    "\n",
    "**q5:** Calculate Root Mean Squared Error (RMSE) on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bsf1mDiPL-dX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10329.20768\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01, objective=\"reg:squarederror\", random_state=13)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_val = model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9YFI8rhBTmN"
   },
   "source": [
    "## 6\n",
    "\n",
    "In the task 5, we have decided to build 200 trees in our model. However, it is hard to understand whether it is a good decision - maybe it is too much? Maybe 150 is a better number? Or 100? Or 50 is enough?\n",
    "\n",
    "During the training process, it is possible to stop constructing the ensemble if we see that the validation error does not decrease anymore. Using the same XGBoost model, call `fit` function (to train it) with `eval_set=[(X_val, y_val)]` (to evaluate the boosting model after building a new tree) and `early_stopping_rounds=50` (and other default parameter values). This `early_stopping_rounds` says that if the validation metric does not increase on 50 consequent iterations, the training stops.\n",
    "\n",
    "**q6:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WmX6FqvoMA4L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8890.23464\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01, objective=\"reg:squarederror\", random_state=13)\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "y_pred_val = model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_jy-BqrCaci"
   },
   "source": [
    "## 7\n",
    "\n",
    "Notes on parameter tuning: https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `XGBRegressor` with the following parameters:\n",
    "\n",
    "* `objective='reg:squarederror'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=4`\n",
    "* `gamma=1`\n",
    "* `subsample=0.5`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 6, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q7:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "E4nDYySWMC_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8692.77417\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "xgb_model = XGBRegressor(n_estimators=5000, max_depth=4, learning_rate=0.001, \n",
    "                         objective=\"reg:squarederror\", gamma=1, subsample=0.5, random_state=13)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=500, verbose=False)\n",
    "y_pred_val = xgb_model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57TAxOUfC_-9"
   },
   "source": [
    "## 8\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 7. \n",
    "\n",
    "**q8:** What is the name of the most important feature? Provide it as the answer. Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that by default, `XGBRegressor` calculates feature importance considering gain (`importance_type` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uyWWYYvKMEwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_channel_is_bus\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "feature_importances = dict(zip(X_train.columns, xgb_model.feature_importances_))\n",
    "max_fi = max(feature_importances, key=feature_importances.get)\n",
    "\n",
    "print(max_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4EWKwRkEJGO"
   },
   "source": [
    "## 9\n",
    "\n",
    "Let's move to LightGBM. We will work with `LGBMRegressor`.\n",
    "\n",
    "LGBMRegressor parameters: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor\n",
    "\n",
    "The main list of LightGBM parameters: https://lightgbm.readthedocs.io/en/latest/Parameters.html Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `LGBMRegressor` with the following parameters, similar to the previous `XGBoost` model:\n",
    "\n",
    "* `objective='regression'`\n",
    "* `n_estimators=200`\n",
    "* `learning_rate=0.01`\n",
    "* `max_depth=5`\n",
    "* `random_state=13`\n",
    "* other default parameter values\n",
    "\n",
    "Train it on the training data with `eval_set=[(X_val, y_val)]`, `eval_metric='rmse'`, `early_stopping_rounds=50` and all other default parameter values. \n",
    "\n",
    "**q9:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm and compare it to the speed of XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4Y-dWz4SMHTB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "8451.2859\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model = LGBMRegressor(max_depth=5, learning_rate=0.01, n_estimators=200, objective=\"regression\", random_state=13)\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=\"rmse\", early_stopping_rounds=50, verbose=False)\n",
    "y_pred_val = model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9CrcUB3GZg9"
   },
   "source": [
    "## 10\n",
    "\n",
    "Notes on parameter tuning: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `LGBMRegressor` with the following parameters:\n",
    "\n",
    "* `objective='regression'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=3`\n",
    "* `lambda_l2=1.0`\n",
    "* `boosting_type='goss'`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 9, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q10:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ybE86ky2MJLx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "8421.39852\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model = LGBMRegressor(boosting_type=\"goss\", max_depth=3, learning_rate=0.001, n_estimators=5000, \n",
    "                      objective=\"regression\", random_state=13, lambda_l2=1.0)\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=\"rmse\", early_stopping_rounds=500, verbose=False)\n",
    "y_pred_val = model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRMPMVMqJEJu"
   },
   "source": [
    "## 11\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 10. \n",
    "\n",
    "**q11:** What is the name of the most important feature? Provide it as the answer. \n",
    "\n",
    "Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that by default, `LGBMRegressor` calculates feature importance considering number of times the feature is used in the model (`importance_type` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FYrvkH1rMK6t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_reference_min_shares\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "feature_importances = dict(zip(X_train.columns, model.feature_importances_))\n",
    "max_fi = max(feature_importances, key=feature_importances.get)\n",
    "print(max_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjzix3YhSy5x"
   },
   "source": [
    "## 12\n",
    "\n",
    "Since some features are not important for the model, we can drop them in order to try to construct a better model which does not consider them at all.\n",
    "\n",
    "Obtain new train and validation sets without the features with LightGBM importance less than 10 (the importances were computed in the task 11). Train the same model as in the task 10 on the new train set in the same manner. \n",
    "\n",
    "**q12:** Calculate RMSE on the new validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice that the new versions of train and validation sets are used only in this task and in blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7FEgaXIXMNkh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n",
      "8422.73009\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "new_features = filter(lambda x: feature_importances[x] >= 10, feature_importances)\n",
    "X_train_new = X_train[new_features]\n",
    "X_val_new = X_val[X_train_new.columns]\n",
    "\n",
    "lgb_model = LGBMRegressor(boosting_type=\"goss\", max_depth=3, learning_rate=0.001, \n",
    "                          n_estimators=5000, objective=\"regression\", random_state=13, lambda_l2=1.0)\n",
    "lgb_model.fit(X_train_new, y_train, eval_set=[(X_val_new, y_val)], eval_metric=\"rmse\", \n",
    "              early_stopping_rounds=500, verbose=False)\n",
    "y_pred_val = lgb_model.predict(X_val_new)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pIu6_n3JvGI"
   },
   "source": [
    "## 13\n",
    "\n",
    "Let's move to CatBoost. We will work with `CatBoostRegressor`.\n",
    "\n",
    "Info about `CatBoostRegressor`: https://catboost.ai/docs/concepts/python-reference_catboostregressor.html\n",
    "\n",
    "CatBoost parameters: https://catboost.ai/docs/concepts/python-reference_parameters-list.html Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `CatBoostRegressor` with the following parameters, similar to the previous models:\n",
    "\n",
    "* `loss_function='RMSE'`\n",
    "* `iterations=200`\n",
    "* `learning_rate=0.01`\n",
    "* `max_depth=5`\n",
    "* `random_state=13`\n",
    "* other default parameter values\n",
    "\n",
    "Train it on the training data with `eval_set=[(X_val, y_val)]`, `early_stopping_rounds=50` and all other default parameter values. \n",
    "\n",
    "**q13:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm and compare it to the speed of XGBoost and LightGBM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "R-rFegFdMPx6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8485.01086\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "model = CatBoostRegressor(iterations=200, learning_rate=0.01, loss_function=\"RMSE\", max_depth=5, random_state=13)\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "y_pred_val = model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keSiZN2DNYbn"
   },
   "source": [
    "## 14\n",
    "\n",
    "Notes on parameter tuning: https://catboost.ai/docs/concepts/parameter-tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `CatBoostRegressor` with the following parameters:\n",
    "\n",
    "* `loss_function='RMSE'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=9`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 13, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q14:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "t-1QF1nKMRye"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8465.44037\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "cb_model = CatBoostRegressor(n_estimators=5000, learning_rate=0.001, loss_function=\"RMSE\", max_depth=9, random_state=13)\n",
    "cb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=500, verbose=False)\n",
    "y_pred_val = cb_model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d0kLl7HS3lG"
   },
   "source": [
    "## 15\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 14. \n",
    "\n",
    "**q15:** What is the name of the most important feature? Provide it as the answer. \n",
    "\n",
    "Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that in case of regression, `CatBoostRegressor` calculates feature importance considering PredictionValuesChange: https://catboost.ai/docs/concepts/fstr.html#fstr__regular-feature-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Z0-Bw-IbMVW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kw_avg_avg\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "feature_importances = dict(zip(X_train.columns, cb_model.feature_importances_))\n",
    "max_fi = max(feature_importances, key=feature_importances.get)\n",
    "print(max_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMdjrX7TdSx"
   },
   "source": [
    "## 16\n",
    "\n",
    "Finally, take a `Lasso` model from `sklearn` with `alpha=10.0`, `random_state=13` and all other default parameter values. Train it on the train set. \n",
    "\n",
    "**q16:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "D-3Ie1OmMXbj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8426.97894\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "lasso_model = Lasso(alpha=10.0, random_state=13)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "y_pred_val = lasso_model.predict(X_val)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6COwSmbBt1m8"
   },
   "source": [
    "## 17\n",
    "\n",
    "Compare the results on the validation set of the trained models:\n",
    "\n",
    "* XGBoost (task 7)\n",
    "* LightGBM (task 12)\n",
    "* CatBoost (task 14)\n",
    "* Lasso (task 16)\n",
    "\n",
    "**q17:** Which model has the best RMSE value on the validation set? For the answer, provide the following:\n",
    "\n",
    "* 1 (if XGBoost was the best)\n",
    "* 2 (if LightGBM was the best)\n",
    "* 3 (if CatBoost was the best)\n",
    "* 4 (if Lasso was the best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aEMdDB0cY01"
   },
   "source": [
    "## 18\n",
    "\n",
    "Finally, let's move to blending the models that we obtained. First, calculate the predictions for the trained models on the validation set. Remember that LightGBM model used slightly different set of columns in the data.\n",
    "\n",
    "After getting the predictions for the validation set, concatenate them into a single dataframe `X_val_blend`. The dataframe should look like this:\n",
    "\n",
    "||xgb|lgb|cb|lasso|\n",
    "|-|-|-|-|-|\n",
    "|0|2298.947754|3728.088336|3680.924182|4270.039931|\n",
    "|1|3208.189209|5243.744431|4487.549790|6755.853939|\n",
    "|...|...|...|...|...|\n",
    "\n",
    "Here, `xgb` column represents XGBoost predictions, `lgb` - LightGBM predictions, `cb` - CatBoost predictions, `lasso` - lasso predictions.\n",
    "\n",
    "**q18:** For the answer, calculate the mean value of all model predictions in the last row of this column (`X_val_blend.iloc[-1]`). Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pLLOMwNSUG9k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2881.74948\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "X_val_blend = pd.concat([pd.Series(xgb_model.predict(X_val), name=\"xgb\"),\n",
    "                         pd.Series(lgb_model.predict(X_val_new), name=\"lgb\"),\n",
    "                         pd.Series(cb_model.predict(X_val), name=\"cb\"),\n",
    "                         pd.Series(lasso_model.predict(X_val), name=\"lasso\")],\n",
    "                        axis=1)\n",
    "\n",
    "print(round(X_val_blend.iloc[-1].mean(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RikEkjthVKX-"
   },
   "source": [
    "## 19\n",
    "\n",
    "Obtain a matrix of pairwise Pearson Correlation Coefficient (PCC) values for the column of the dataframe `X_val_blend`. Find a pair of model predictions with the highest PCC value (don't consider 1.0 values of correlations with themselves). \n",
    "\n",
    "**q19:** What is this value equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e7wJfNuTMh4A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84455\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "corr_matrix = X_val_blend.corr()\n",
    "max_corr = corr_matrix[corr_matrix < 1.0].max().max()\n",
    "\n",
    "print(round(max_corr, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKrXVxTGVaZa"
   },
   "source": [
    "## 20\n",
    "\n",
    "Blend models into the ensemble with the weights 0.25, 0.25, 0.25 and 0.25 (just mean value of the predictions). \n",
    "\n",
    "**q20:** Calculate RMSE of such ensemble on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Compare it with RMSE of each model and think whether this is a good ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NoD2EsNRMjvQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8439.26508\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "y_pred_val = X_val_blend.mean(axis=1)\n",
    "rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)\n",
    "\n",
    "print(round(rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBPbgfyadERE"
   },
   "source": [
    "## 21\n",
    "\n",
    "Tune the weights of the ensemble. Run each model weight through `np.linspace(0, 1, 101)`, so that all possible values of each weight will be [0.0, 0.01, 0.02, ..., 0.99, 1.0]. Skip each combinations of weights, if their sum is not equal to 1.0. If the sum of the weights in the combination is equal to 1.0, though, get ensemble prediction on the validation set using these weights and calculate RMSE value.\n",
    "\n",
    "In the end, select a combination of weights with the best RMSE value - these are the best weights for the ensemble. \n",
    "\n",
    "**q21:** What is their corresponding RMSE value equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Compare RMSE value of the ensemble with RMSE values of the models in it. Is the ensemble better?\n",
    "\n",
    "_Hint. You probably want to save RMSE with the corresponding weights for each valid combination into some array. Also this weight tuning might be implemented as quadriple nested loop, or you may think about other ways of implementing it. You can track tuning progress using `tqdm` module._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "H9Tud8e-Mp0C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8411.46345\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from itertools import combinations_with_replacement\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_weights = None\n",
    "best_rmse_val = float(\"inf\")\n",
    "\n",
    "possible_weights = combinations_with_replacement(np.linspace(0, 1, 101), 4)\n",
    "possible_weights = filter(lambda x: sum(x) == 1.0, possible_weights)\n",
    "possible_weights = np.array(list(possible_weights))\n",
    "\n",
    "y_pred_val = X_val_blend.to_numpy() @ possible_weights.T\n",
    "\n",
    "for i in range(y_pred_val.shape[1]):\n",
    "    rmse_val = mean_squared_error(y_val, y_pred_val[:, i], squared=False)\n",
    "    if rmse_val < best_rmse_val:\n",
    "        best_rmse_val = rmse_val\n",
    "        best_weights = possible_weights[i]\n",
    "\n",
    "print(round(best_rmse_val, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFqDAt-EdKxS"
   },
   "source": [
    "## 22\n",
    "\n",
    "Using the best weights obtained in the task 21, run the best ensemble on the test set. To do this, obtain model predictions on the test set (you can write them to the similar table to the one for the validation set in the task 18). Remember that LightGBM model uses slightly different set of columns.\n",
    "\n",
    "**q22:** Calculate RMSE of the final ensemble on the test set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dEW1M-s6MtOx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8441.68222\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "X_test_blend = pd.concat([pd.Series(xgb_model.predict(X_test), name=\"xgb\"),\n",
    "                          pd.Series(lgb_model.predict(X_test[X_train_new.columns]), name=\"lgb\"),\n",
    "                          pd.Series(cb_model.predict(X_test), name=\"cb\"),\n",
    "                          pd.Series(lasso_model.predict(X_test), name=\"lasso\")],\n",
    "                         axis=1)\n",
    "\n",
    "y_pred_test = X_test_blend.apply(lambda x: np.average(x, weights=best_weights), axis=1)\n",
    "rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "\n",
    "print(round(rmse_test, 5))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8xzctGVVD8ZayBsxVM3cd",
   "collapsed_sections": [],
   "name": "Week2_practice.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "ensembling-techniques-task-week-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
